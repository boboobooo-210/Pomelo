# NTU RGB+D 时空图卷积(GCN)骨架Tokenizer训练配置 - Gumbel-Softmax版本

optimizer: {
  type: AdamW,
  kwargs: {
    lr: 0.0003,  # 降低学习率，防止 Gumbel-Softmax 训练初期爆炸
    weight_decay: 0.0001
  }
}

scheduler: {
  type: CosLR,
  kwargs: {
    epochs: 100,   # 与原版相同
    initial_epochs: 10,  # 增加 warmup epochs，从5→10
    warming_up_init_lr: 0.00005  # 更低的初始学习率
  }
}

# KL散度权重配置（Gumbel-Softmax版本中是真正的KL散度损失）
# 用于鼓励码本均匀使用，防止mode collapse
kldweight: {
  start: 0.2,   # Gumbel版本从0.2开始（比原版VQ更重视分布均匀性）
  target: 0.8,  # 最终权重0.8（强制均匀使用码本）
  ntime: 8000   # 8000步内达到目标权重（更缓慢的warmup，让模型适应）
}

# 数据集配置 - 与原版相同
dataset: {
  train: { 
    _base_: cfgs/dataset_configs/NTU_skeleton_raw.yaml, 
    others: {
      subset: 'train',
      npoints: 25,
      normalize: true,
      bs: 4  # 保持相同批大小
    }
  },
  val: { 
    _base_: cfgs/dataset_configs/NTU_skeleton_raw.yaml, 
    others: {
      subset: 'val',
      npoints: 25,
      normalize: true,
      bs: 4
    }
  },
  test: {
    _base_: cfgs/dataset_configs/NTU_skeleton_raw.yaml,
    others: {
      subset: 'test',
      npoints: 25,
      normalize: true,
      bs: 4
    }
  }
}

# GCN骨架Tokenizer配置 - Gumbel-Softmax版本
model: {
  NAME: GCNSkeletonTokenizer_Gumbel,
  num_joints: 25,
  num_tokens: 256,        # 5个语义组 × 48码字/组 = 240个码字（更匹配数据复杂度）
  token_dim: 256,
  temporal_length: 1,
  tokens_per_group: 48,   # 每组48个token（针对MARS数据优化，避免过大码本）
  temperature_init: 1.0,  # Gumbel-Softmax初始温度（软量化，充分探索）
  temperature_min: 0.5,   # 最小温度0.5（保持足够软性，支持姿态插值）
  temperature_decay: 0.99995,  # 温度衰减率（更缓慢，让模型充分学习软组合）
  kl_weight: 0.2,         # KL散度损失权重0.2（鼓励均匀使用码本）
  residual_reg_weight: 2.0  # 残差正则化权重2.0（更强约束，迫使依赖码本）
}

# 训练设置 - 与原版相同
total_bs: 8         # 通过梯度累积达到等效批大小
step_per_update: 2  # 梯度累积2步 (4*2=8)
max_epoch: 100       # 与原版相同
use_gpu: true
val_freq: 10        # 与原版相同
num_workers: 2      # 与原版相同
seed: 2021
consider_metric: CDL1
