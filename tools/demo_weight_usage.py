#!/usr/bin/env python3
"""
GCNå¯è§†åŒ–å™¨æƒé‡ä½¿ç”¨æµç¨‹æ¼”ç¤ºè„šæœ¬
å±•ç¤ºæƒé‡ä»åŠ è½½åˆ°æ¨ç†çš„å®Œæ•´è¿‡ç¨‹
"""

import os
import sys
import torch
import numpy as np
from pathlib import Path

# æ·»åŠ é¡¹ç›®è·¯å¾„
project_root = Path(__file__).parent.parent
sys.path.append(str(project_root))

def demonstrate_weight_loading_process():
    """æ¼”ç¤ºæƒé‡åŠ è½½è¿‡ç¨‹"""
    print("=" * 80)
    print("ğŸ”„ GCNéª¨æ¶å¯è§†åŒ–å™¨æƒé‡ä½¿ç”¨æµç¨‹æ¼”ç¤º")
    print("=" * 80)
    
    # ç¬¬1æ­¥ï¼šæ£€æŸ¥æ–‡ä»¶è·¯å¾„
    print("\nğŸ“ ç¬¬1æ­¥ï¼šæ£€æŸ¥è®­ç»ƒæƒé‡å’Œé…ç½®æ–‡ä»¶")
    model_path = "experiments/gcn_skeleton_memory_optimized/checkpoints/ckpt-best.pth"
    config_path = "cfgs/NTU_models/gcn_skeleton_memory_optimized.yaml"
    
    print(f"  æƒé‡æ–‡ä»¶: {model_path}")
    print(f"  é…ç½®æ–‡ä»¶: {config_path}")
    
    if os.path.exists(model_path):
        print("  âœ… æƒé‡æ–‡ä»¶å­˜åœ¨")
    else:
        print("  âŒ æƒé‡æ–‡ä»¶ä¸å­˜åœ¨")
        
    if os.path.exists(config_path):
        print("  âœ… é…ç½®æ–‡ä»¶å­˜åœ¨")
    else:
        print("  âŒ é…ç½®æ–‡ä»¶ä¸å­˜åœ¨")
    
    # ç¬¬2æ­¥ï¼šæ¨¡æ‹Ÿæƒé‡åŠ è½½
    print("\nğŸ—ï¸ ç¬¬2æ­¥ï¼šæ¨¡æ‹Ÿæƒé‡åŠ è½½è¿‡ç¨‹")
    print("  2.1 åŠ è½½é…ç½®æ–‡ä»¶...")
    try:
        from utils.config import cfg_from_yaml_file
        config = cfg_from_yaml_file(config_path)
        print("      âœ… é…ç½®åŠ è½½æˆåŠŸ")
        print(f"      æ¨¡å‹ç±»å‹: {config.model.get('NAME', 'Unknown')}")
        print(f"      å…³èŠ‚æ•°é‡: {config.model.get('num_joints', 'Unknown')}")
        print(f"      Tokenç»´åº¦: {config.model.get('token_dim', 'Unknown')}")
    except Exception as e:
        print(f"      âŒ é…ç½®åŠ è½½å¤±è´¥: {e}")
        return
    
    print("  2.2 åˆ›å»ºæ¨¡å‹æ¶æ„...")
    try:
        from models.GCNSkeletonTokenizer import GCNSkeletonTokenizer
        model = GCNSkeletonTokenizer(config.model)
        print("      âœ… æ¨¡å‹æ¶æ„åˆ›å»ºæˆåŠŸ")
        
        # ç»Ÿè®¡æ¨¡å‹å‚æ•°
        total_params = sum(p.numel() for p in model.parameters())
        trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
        print(f"      æ€»å‚æ•°é‡: {total_params:,}")
        print(f"      å¯è®­ç»ƒå‚æ•°: {trainable_params:,}")
    except Exception as e:
        print(f"      âŒ æ¨¡å‹åˆ›å»ºå¤±è´¥: {e}")
        return
    
    print("  2.3 åŠ è½½è®­ç»ƒæƒé‡...")
    if os.path.exists(model_path):
        try:
            checkpoint = torch.load(model_path, map_location='cpu')
            print("      âœ… æ£€æŸ¥ç‚¹æ–‡ä»¶åŠ è½½æˆåŠŸ")
            
            # åˆ†ææ£€æŸ¥ç‚¹å†…å®¹
            print(f"      æ£€æŸ¥ç‚¹é”®: {list(checkpoint.keys())}")
            
            if 'base_model' in checkpoint:
                state_dict = checkpoint['base_model']
                print("      ä½¿ç”¨ 'base_model' æƒé‡")
            else:
                state_dict = checkpoint
                print("      ä½¿ç”¨æ ¹çº§æƒé‡")
            
            # å¤„ç†åˆ†å¸ƒå¼è®­ç»ƒæƒé‡
            new_state_dict = {}
            module_count = 0
            for k, v in state_dict.items():
                if k.startswith('module.'):
                    new_state_dict[k[7:]] = v
                    module_count += 1
                else:
                    new_state_dict[k] = v
            
            if module_count > 0:
                print(f"      ç§»é™¤äº† {module_count} ä¸ª 'module.' å‰ç¼€")
            
            # åŠ è½½æƒé‡åˆ°æ¨¡å‹
            model.load_state_dict(new_state_dict)
            model.eval()
            print("      âœ… æƒé‡åŠ è½½åˆ°æ¨¡å‹æˆåŠŸ")
            
            # åˆ†ææƒé‡ç»“æ„
            print("      æƒé‡ç»“æ„åˆ†æ:")
            weight_groups = {}
            for name, param in model.named_parameters():
                group = name.split('.')[0]
                if group not in weight_groups:
                    weight_groups[group] = []
                weight_groups[group].append((name, param.shape))
            
            for group, params in weight_groups.items():
                group_params = sum(p.numel() for _, p in params)
                print(f"        {group}: {len(params)} å±‚, {group_params:,} å‚æ•°")
                
        except Exception as e:
            print(f"      âŒ æƒé‡åŠ è½½å¤±è´¥: {e}")
            return
    else:
        print("      âŒ æƒé‡æ–‡ä»¶ä¸å­˜åœ¨ï¼Œè·³è¿‡åŠ è½½")
        return

def demonstrate_inference_process():
    """æ¼”ç¤ºæ¨ç†è¿‡ç¨‹"""
    print("\nğŸ¯ ç¬¬3æ­¥ï¼šæ¨¡æ‹Ÿæ¨ç†è¿‡ç¨‹")
    
    # åˆ›å»ºæ¨¡æ‹Ÿéª¨æ¶æ•°æ®
    print("  3.1 åˆ›å»ºæ¨¡æ‹Ÿéª¨æ¶æ•°æ®...")
    # æ¨¡æ‹ŸNTU RGB+D 25å…³èŠ‚ç‚¹æ•°æ®
    skeleton = np.random.randn(25, 3).astype(np.float32) * 0.5
    print(f"      è¾“å…¥éª¨æ¶å½¢çŠ¶: {skeleton.shape}")
    print(f"      éª¨æ¶æ•°æ®èŒƒå›´: [{skeleton.min():.3f}, {skeleton.max():.3f}]")
    
    print("  3.2 æ•°æ®é¢„å¤„ç†...")
    # æ ‡å‡†åŒ–å¤„ç†ï¼ˆä¸è®­ç»ƒæ—¶ä¸€è‡´ï¼‰
    centroid = np.mean(skeleton, axis=0)
    centered = skeleton - centroid
    distances = np.sqrt(np.sum(centered**2, axis=1))
    max_distance = np.max(distances)
    
    if max_distance > 0:
        normalized = centered / max_distance
    else:
        normalized = centered
    
    print(f"      æ ‡å‡†åŒ–åå½¢çŠ¶: {normalized.shape}")
    print(f"      æ ‡å‡†åŒ–åèŒƒå›´: [{normalized.min():.3f}, {normalized.max():.3f}]")
    
    print("  3.3 å¼ é‡è½¬æ¢...")
    skeleton_tensor = torch.from_numpy(normalized).unsqueeze(0)  # æ·»åŠ batchç»´åº¦
    print(f"      å¼ é‡å½¢çŠ¶: {skeleton_tensor.shape}")
    print(f"      å¼ é‡æ•°æ®ç±»å‹: {skeleton_tensor.dtype}")
    
    print("  3.4 æ¨¡æ‹Ÿå‰å‘ä¼ æ’­...")
    print("      âš¡ è¾“å…¥åµŒå…¥å±‚ (3Dåæ ‡ â†’ 64ç»´ç‰¹å¾)")
    print("      ğŸ§  è¯­ä¹‰åˆ†ç»„å¤„ç†:")
    print("         - head_spine: 5ä¸ªå…³èŠ‚ â†’ 64ç»´ç‰¹å¾")
    print("         - left_arm: 6ä¸ªå…³èŠ‚ â†’ 64ç»´ç‰¹å¾")  
    print("         - right_arm: 6ä¸ªå…³èŠ‚ â†’ 64ç»´ç‰¹å¾")
    print("         - left_leg: 4ä¸ªå…³èŠ‚ â†’ 64ç»´ç‰¹å¾")
    print("         - right_leg: 4ä¸ªå…³èŠ‚ â†’ 64ç»´ç‰¹å¾")
    print("      ğŸ­ å‘é‡é‡åŒ–ç æœ¬:")
    print("         - æ¯ä¸ªè¯­ä¹‰ç»„128ä¸ªç å­—")
    print("         - æœ€è¿‘é‚»åŒ¹é… â†’ token ID")
    print("      ğŸ”„ ç‰¹å¾èåˆ â†’ å…¨å±€ç‰¹å¾")
    print("      ğŸ—ï¸ é‡å»ºç½‘ç»œ â†’ 25Ã—3 éª¨æ¶åæ ‡")

def demonstrate_visualization_output():
    """æ¼”ç¤ºå¯è§†åŒ–è¾“å‡º"""
    print("\nğŸ¨ ç¬¬4æ­¥ï¼šå¯è§†åŒ–è¾“å‡ºè¿‡ç¨‹")
    
    print("  4.1 é‡å»ºè´¨é‡è¯„ä¼°...")
    # æ¨¡æ‹Ÿé‡å»ºè¯¯å·®è®¡ç®—
    original = np.random.randn(25, 3) * 0.5
    reconstructed = original + np.random.randn(25, 3) * 0.1  # æ·»åŠ å°‘é‡å™ªå£°æ¨¡æ‹Ÿé‡å»º
    
    mse_error = np.mean((original - reconstructed) ** 2)
    print(f"      MSEé‡å»ºè¯¯å·®: {mse_error:.6f}")
    
    if mse_error < 0.01:
        print("      âœ… é‡å»ºè´¨é‡: ä¼˜ç§€")
    elif mse_error < 0.05:
        print("      âš ï¸ é‡å»ºè´¨é‡: è‰¯å¥½")
    else:
        print("      âŒ é‡å»ºè´¨é‡: éœ€è¦æ”¹è¿›")
    
    print("  4.2 3Då¯è§†åŒ–ç”Ÿæˆ...")
    print("      ğŸ“Š åˆ›å»ºmatplotlib 3Då›¾å½¢")
    print("      ğŸ¦´ ç»˜åˆ¶éª¨æ¶è¿æ¥å…³ç³» (25ä¸ªå…³èŠ‚ç‚¹)")
    print("      ğŸ¨ é¢œè‰²ç¼–ç : è“è‰²(åŸå§‹) vs çº¢è‰²(é‡å»º)")
    print("      ğŸ“ è§†è§’è°ƒæ•´: elev=15Â°, azim=45Â°")
    
    print("  4.3 è¾“å‡ºæ–‡ä»¶ä¿å­˜...")
    print("      ğŸ“ ä¿å­˜è·¯å¾„: visualizations/0_gcn/results_*/")
    print("      ğŸ–¼ï¸ æ–‡ä»¶æ ¼å¼: PNG, DPI=300")
    print("      ğŸ“ æ–‡ä»¶å: gcn_reconstruction_sample_{i}_{name}.png")

def analyze_weight_importance():
    """åˆ†ææƒé‡é‡è¦æ€§"""
    print("\nğŸ” ç¬¬5æ­¥ï¼šæƒé‡é‡è¦æ€§åˆ†æ")
    
    print("  5.1 å…³é”®æƒé‡ç»„ä»¶:")
    weight_components = {
        "input_embedding": "3Dåæ ‡åˆ°64ç»´ç‰¹å¾çš„çº¿æ€§å˜æ¢",
        "st_gcn_layers": "æ—¶ç©ºå›¾å·ç§¯æ ¸å¿ƒæƒé‡",
        "group_processors": "è¯­ä¹‰åˆ†ç»„çš„ç‹¬ç«‹å¤„ç†å™¨",
        "semantic_codebooks": "å‘é‡é‡åŒ–çš„å¯å­¦ä¹ ç æœ¬",
        "global_fusion": "å¤šç»„ç‰¹å¾èåˆæƒé‡",
        "reconstruction_head": "ç‰¹å¾åˆ°éª¨æ¶åæ ‡çš„é‡å»ºæƒé‡"
    }
    
    for component, description in weight_components.items():
        print(f"      ğŸ”§ {component}: {description}")
    
    print("  5.2 æƒé‡è®­ç»ƒè¿‡ç¨‹:")
    print("      ğŸ“ˆ é‡å»ºæŸå¤±: MSE(åŸå§‹, é‡å»º)")
    print("      ğŸ¯ VQæŸå¤±: Commitment loss + ç æœ¬æ›´æ–°")
    print("      âš–ï¸ æŸå¤±å¹³è¡¡: reconstruction_weight + kld_weight * vq_loss")
    print("      ğŸ² ä¼˜åŒ–å™¨: AdamW, lr=0.001, weight_decay=0.0001")
    
    print("  5.3 æƒé‡è´¨é‡æŒ‡æ ‡:")
    print("      âœ… æ”¶æ•›æ€§: æŸå¤±æ›²çº¿å¹³æ»‘ä¸‹é™")
    print("      ğŸ¯ é‡å»ºç²¾åº¦: MSE < 0.01 (ä¼˜ç§€)")
    print("      ğŸ”„ Tokenä½¿ç”¨: å„ç»„ç æœ¬å‡åŒ€ä½¿ç”¨")
    print("      ğŸƒ æ³›åŒ–èƒ½åŠ›: æµ‹è¯•é›†æ€§èƒ½æ¥è¿‘è®­ç»ƒé›†")

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸš€ GCNéª¨æ¶å¯è§†åŒ–å™¨æƒé‡ä½¿ç”¨å®Œæ•´æµç¨‹æ¼”ç¤º")
    
    # æ¼”ç¤ºå„ä¸ªæ­¥éª¤
    demonstrate_weight_loading_process()
    demonstrate_inference_process()  
    demonstrate_visualization_output()
    analyze_weight_importance()
    
    print("\n" + "=" * 80)
    print("ğŸ“‹ æ€»ç»“ï¼šæƒé‡ä½¿ç”¨æµç¨‹")
    print("=" * 80)
    
    summary_steps = [
        "1ï¸âƒ£ åŠ è½½è®­ç»ƒé…ç½® â†’ åˆ›å»ºæ¨¡å‹æ¶æ„",
        "2ï¸âƒ£ åŠ è½½æ£€æŸ¥ç‚¹æ–‡ä»¶ â†’ æå–è®­ç»ƒæƒé‡", 
        "3ï¸âƒ£ å¤„ç†æƒé‡æ ¼å¼ â†’ åŠ è½½åˆ°æ¨¡å‹",
        "4ï¸âƒ£ è®¾ç½®è¯„ä¼°æ¨¡å¼ â†’ ç¦ç”¨æ¢¯åº¦è®¡ç®—",
        "5ï¸âƒ£ è¾“å…¥æ•°æ®é¢„å¤„ç† â†’ æ ‡å‡†åŒ–å’Œå¼ é‡è½¬æ¢",
        "6ï¸âƒ£ å‰å‘ä¼ æ’­æ¨ç† â†’ åˆ©ç”¨è®­ç»ƒæƒé‡é‡å»º",
        "7ï¸âƒ£ åå¤„ç†è¾“å‡º â†’ åæ ‡è½¬æ¢å’Œè¯¯å·®è®¡ç®—",
        "8ï¸âƒ£ 3Då¯è§†åŒ–ç”Ÿæˆ â†’ å¯¹æ¯”åŸå§‹å’Œé‡å»ºéª¨æ¶"
    ]
    
    for step in summary_steps:
        print(f"  {step}")
    
    print("\nğŸ’¡ å…³é”®è¦ç‚¹:")
    print("  â€¢ æƒé‡æ–‡ä»¶åŒ…å«æ‰€æœ‰è®­ç»ƒå¥½çš„å‚æ•°ï¼ˆGCNå±‚ã€ç æœ¬ã€é‡å»ºç½‘ç»œç­‰ï¼‰")
    print("  â€¢ æ¨ç†è¿‡ç¨‹å¿…é¡»ä¸è®­ç»ƒæ—¶ä¿æŒæ•°æ®é¢„å¤„ç†çš„ä¸€è‡´æ€§")
    print("  â€¢ MSEè¯¯å·®ç›´æ¥åæ˜ è®­ç»ƒæƒé‡çš„é‡å»ºè´¨é‡")
    print("  â€¢ å¯è§†åŒ–ç»“æœæ˜¯è¯„ä¼°æ¨¡å‹è®­ç»ƒæ•ˆæœçš„é‡è¦å·¥å…·")
    
    print("\nğŸ¯ ä½¿ç”¨å»ºè®®:")
    print("  â€¢ ç¡®ä¿ä½¿ç”¨æœ€ä½³æ£€æŸ¥ç‚¹ (ckpt-best.pth) è¿›è¡Œå¯è§†åŒ–")
    print("  â€¢ ä»…åœ¨æµ‹è¯•é›†ä¸Šè¯„ä¼°ï¼Œé¿å…è¿‡æ‹Ÿåˆè¯„ä¼°")
    print("  â€¢ å…³æ³¨MSEæ•°å€¼å’Œè§†è§‰æ•ˆæœçš„ä¸€è‡´æ€§") 
    print("  â€¢ å¯¹æ¯”ä¸åŒè®­ç»ƒé˜¶æ®µçš„æƒé‡æ•ˆæœ")

if __name__ == "__main__":
    main()