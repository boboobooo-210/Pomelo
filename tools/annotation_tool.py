#!/usr/bin/env python3
"""
ç æœ¬åŠ¨ä½œæ ‡æ³¨å·¥å…·
ç”¨äºäººå·¥æ ‡æ³¨Tokenå¯¹åº”çš„åŠ¨ä½œè¯­ä¹‰ï¼Œæ„å»ºç æœ¬-åŠ¨ä½œæ˜ å°„è¡¨
"""

import json
import os
import sys
import numpy as np
import re
from datetime import datetime
from typing import Dict, List, Optional

# LLM å‹å¥½æ ¼å¼å¯¼å‡ºå™¨
try:
    from llm_annotation_exporter import LLMAnnotationExporter
    LLM_EXPORTER_AVAILABLE = True
except ImportError:
    LLM_EXPORTER_AVAILABLE = False
    print("âš ï¸ LLM å¯¼å‡ºå™¨ä¸å¯ç”¨ï¼Œå°†åªæ”¯æŒæ ‡å‡†æ ¼å¼å¯¼å‡º")

try:
    import h5py
except ImportError:
    h5py = None
    print("âš ï¸ h5py æœªå®‰è£…ï¼Œå°†ä½¿ç”¨æ¨¡æ‹Ÿæ•°æ®")

# å¯è§†åŒ–åº“ (ç”¨äºGIFç”Ÿæˆ)
try:
    import matplotlib
    matplotlib.use('Agg')  # ä½¿ç”¨éäº¤äº’å¼åç«¯
    import matplotlib.pyplot as plt
    from mpl_toolkits.mplot3d import Axes3D
    import matplotlib.animation as animation
    GIF_AVAILABLE = True
except ImportError:
    GIF_AVAILABLE = False
    print("âš ï¸ matplotlib ä¸å¯ç”¨ï¼ŒGIFç”ŸæˆåŠŸèƒ½å°†è¢«ç¦ç”¨")

try:
    import pandas as pd
    PANDAS_AVAILABLE = True
except ImportError:
    PANDAS_AVAILABLE = False
    print("âš ï¸ pandas ä¸å¯ç”¨ï¼ŒMARS Tokenæ•°æ®é›†åŠ è½½å°†è¢«ç¦ç”¨")

def safe_input(prompt, default="", timeout=None):
    """å®‰å…¨çš„è¾“å…¥å‡½æ•°ï¼Œå¤„ç†EOFç­‰å¼‚å¸¸"""
    try:
        result = input(prompt).strip()
        return result if result else default
    except (EOFError, KeyboardInterrupt):
        print(f"\nâš ï¸ è¾“å…¥ä¸­æ–­ï¼Œä½¿ç”¨é»˜è®¤å€¼: {default}")
        return default

# å…¨å±€æ ‡å¿—ï¼šæ˜¯å¦è¿è¡Œåœ¨æ‰¹å¤„ç†æ¨¡å¼
BATCH_MODE = False

def set_batch_mode(enabled=True):
    """è®¾ç½®æ‰¹å¤„ç†æ¨¡å¼"""
    global BATCH_MODE
    BATCH_MODE = enabled

try:
    import tkinter as tk
    from tkinter import ttk, messagebox, filedialog
    GUI_AVAILABLE = True
except ImportError:
    GUI_AVAILABLE = False
    print("âš ï¸ GUIç»„ä»¶ä¸å¯ç”¨ï¼Œå°†ä½¿ç”¨å‘½ä»¤è¡Œç•Œé¢")

try:
    import matplotlib.pyplot as plt
    from matplotlib.backends.backend_tkagg import FigureCanvasTkAgg
    from matplotlib.figure import Figure
    MATPLOTLIB_AVAILABLE = True
except ImportError:
    MATPLOTLIB_AVAILABLE = False
    print("âš ï¸ matplotlibä¸å¯ç”¨ï¼Œå°†ä½¿ç”¨ç®€åŒ–å¯è§†åŒ–")

# å¯¼å…¥å¯è§†åŒ–çª—å£æ¨¡å—
try:
    from visualization_window import show_sample_visualization, close_visualization_window
    VISUALIZATION_AVAILABLE = True
except ImportError:
    try:
        import sys
        sys.path.append(os.path.dirname(__file__))
        from visualization_window import show_sample_visualization, close_visualization_window
        VISUALIZATION_AVAILABLE = True
    except ImportError:
        VISUALIZATION_AVAILABLE = False
        print("âš ï¸ å¯è§†åŒ–çª—å£æ¨¡å—ä¸å¯ç”¨ï¼Œå°†ä½¿ç”¨æ–‡æœ¬æ˜¾ç¤º")

class SkeletonAnnotationTool:
    """éª¨æ¶ç æœ¬æ ‡æ³¨å·¥å…·"""
    
    def __init__(self):
        self.annotation_data = {}
        self.samples_to_annotate = []
        self.current_sample_id = 0
        self.save_dir = "annotations"  # æ ‡æ³¨ç»“æœä¿å­˜ç›®å½•
        self.session_id = datetime.now().strftime("%Y%m%d_%H%M%S")  # ä¼šè¯ID
        
        # åˆ›å»ºä¿å­˜ç›®å½•
        os.makedirs(self.save_dir, exist_ok=True)
        os.makedirs(os.path.join(self.save_dir, "sessions"), exist_ok=True)
        os.makedirs(os.path.join(self.save_dir, "exports"), exist_ok=True)
        
        # åŠ¨ä½œæ ‡ç­¾æ¨¡æ¿ - æ”¯æŒNTUå’ŒMARSæ•°æ®é›†
        # NTUæ•°æ®é›†åŠ¨ä½œæ¨¡æ¿ï¼ˆåŸå§‹å®Œæ•´ç‰ˆæœ¬ï¼‰
        self.ntu_action_templates = {
            'head_spine': [
                "ä¸­æ€§å§¿æ€", "æŠ¬å¤´å‘ä¸Š", "ä½å¤´å‘ä¸‹", "å·¦è½¬å¤´éƒ¨", "å³è½¬å¤´éƒ¨",
                "æŒºç›´è„ŠæŸ±", "å‰å€¾èº«ä½“", "åä»°èº«ä½“", "å·¦ä¾§å¼¯æ›²", "å³ä¾§å¼¯æ›²",
                "ç‚¹å¤´åŠ¨ä½œ", "æ‘‡å¤´åŠ¨ä½œ", "ä¾§å€¾å¤´éƒ¨", "æ—‹è½¬èº¯å¹²", "å¼“èƒŒå§¿æ€"
            ],
            'left_arm': [
                "è‡ªç„¶ä¸‹å‚", "ä¸Šä¸¾è¿‡å¤´", "å‰ä¼¸æŒ‡å‘", "ä¾§å¹³ä¸¾", "å¼¯æ›²æ’‘è…°",
                "äº¤å‰èƒ¸å‰", "æŒ¥æ‰‹åŠ¨ä½œ", "èƒŒåä¼¸å±•", "æ¡æ‹³å‡†å¤‡", "æ”¾æ¾æ‘†åŠ¨",
                "æ¨æ‹‰åŠ¨ä½œ", "æŠ±æŠ±å§¿åŠ¿", "æ•¬ç¤¼åŠ¨ä½œ", "é®æŒ¡é¢éƒ¨", "æ”¯æ’‘èº«ä½“"
            ],
            'right_arm': [
                "è‡ªç„¶ä¸‹å‚", "ä¸Šä¸¾è¿‡å¤´", "å‰ä¼¸æŒ‡å‘", "ä¾§å¹³ä¸¾", "å¼¯æ›²æ’‘è…°",
                "äº¤å‰èƒ¸å‰", "æŒ¥æ‰‹åŠ¨ä½œ", "èƒŒåä¼¸å±•", "æ¡æ‹³å‡†å¤‡", "æ”¾æ¾æ‘†åŠ¨",
                "æ¨æ‹‰åŠ¨ä½œ", "æŠ±æŠ±å§¿åŠ¿", "æ•¬ç¤¼åŠ¨ä½œ", "é®æŒ¡é¢éƒ¨", "æ”¯æ’‘èº«ä½“"
            ],
            'left_leg': [
                "ç›´ç«‹æ”¯æ’‘", "å¾®å¼¯å‡†å¤‡", "æŠ¬èµ·å‰è¸", "ä¾§å‘è¿ˆæ­¥", "è¹²å§¿å¼¯æ›²",
                "åé€€å‡†å¤‡", "è¸¢è…¿åŠ¨ä½œ", "ç«™ç«‹å¹³è¡¡", "äº¤å‰ç«™ç«‹", "è·³è·ƒå‡†å¤‡",
                "å•è…¿æ”¯æ’‘", "è†ç›–å¼¯æ›²", "è„šå°–ç€åœ°", "æŠ¬è†åŠ¨ä½œ", "ä¾§è¸¢å‡†å¤‡"
            ],
            'right_leg': [
                "ç›´ç«‹æ”¯æ’‘", "å¾®å¼¯å‡†å¤‡", "æŠ¬èµ·å‰è¸", "ä¾§å‘è¿ˆæ­¥", "è¹²å§¿å¼¯æ›²",
                "åé€€å‡†å¤‡", "è¸¢è…¿åŠ¨ä½œ", "ç«™ç«‹å¹³è¡¡", "äº¤å‰ç«™ç«‹", "è·³è·ƒå‡†å¤‡",
                "å•è…¿æ”¯æ’‘", "è†ç›–å¼¯æ›²", "è„šå°–ç€åœ°", "æŠ¬è†åŠ¨ä½œ", "ä¾§è¸¢å‡†å¤‡"
            ]
        }
        
        # MARSæ•°æ®é›†ç®€åŒ–åŠ¨ä½œæ¨¡æ¿ï¼ˆç§»é™¤æ‰‹æŒ‡ä¾èµ–åŠ¨ä½œï¼‰
        self.mars_action_templates = {
            'head_spine': [
                "æ­£å¸¸å§¿æ€", "æŠ¬å¤´", "ä½å¤´çœ‹", "å·¦ä¾§è½¬", "å³ä¾§è½¬",
                "æŒºç›´ç«™ç«‹", "å‰å€¾", "åä»°", "å·¦å€¾æ–œ", "å³å€¾æ–œ"
            ],
            'left_arm': [
                "è‡ªç„¶å‚è½", "ä¸Šä¸¾", "å‰ä¼¸", "ä¾§ä¸¾", "å‰è…°",
                "å‘å†…å¼¯æ›²", "è‡ªç„¶å¼¯æ›²", "åä¼¸", "å·¦ä¾§æŠ¬èµ·", "å‘ä¸Šå¼¯æ›²"
            ],
            'right_arm': [
                "è‡ªç„¶å‚è½", "ä¸Šä¸¾", "å‰ä¼¸", "ä¾§ä¸¾", "å‰è…°",
                "å‘å†…å¼¯æ›²", "è‡ªç„¶å¼¯æ›²", "åä¼¸", "å³ä¾§æŠ¬èµ·", "å‘ä¸Šå¼¯æ›²"
            ],
            'left_leg': [
                "ç«™ç«‹", "å¼¯æ›²", "å‰æŠ¬", "ä¾§æŠ¬", "è¹²ä¸‹",
                "åé€€", "è¸¢å‡º", "å‘å‰è·¨æ­¥", "å‘å·¦è·¨æ­¥", "è·³è·ƒ"
            ],
            'right_leg': [
                "ç«™ç«‹", "å¼¯æ›²", "å‰æŠ¬", "ä¾§æŠ¬", "è¹²ä¸‹",
                "åé€€", "è¸¢å‡º", "å‘å‰è·¨æ­¥", "å‘å³è·¨æ­¥", "è·³è·ƒ"
            ]
        }
        
        # æ ¹æ®æ•°æ®é›†ç±»å‹é€‰æ‹©åŠ¨ä½œæ¨¡æ¿
        self.action_templates = self.ntu_action_templates  # é»˜è®¤ä½¿ç”¨NTUæ¨¡æ¿
        
        self.part_names = ['head_spine', 'left_arm', 'right_arm', 'left_leg', 'right_leg']
        self.part_display_names = ['å¤´éƒ¨è„ŠæŸ±', 'å·¦è‡‚', 'å³è‡‚', 'å·¦è…¿', 'å³è…¿']
        
        # ä¸ºå…¼å®¹æ€§æ·»åŠ body_partsåˆ«å
        self.body_parts = self.part_names
        
        # æ ¹æ®å¯ç”¨æ€§é€‰æ‹©ç•Œé¢æ¨¡å¼
        self.use_gui = GUI_AVAILABLE
        
    def generate_sample_data(self, num_samples: int = 50):
        """ç”Ÿæˆç¤ºä¾‹æ ‡æ³¨æ•°æ®"""
        print(f"ğŸ“Š ç”Ÿæˆ {num_samples} ä¸ªç¤ºä¾‹æ ·æœ¬...")
        
        self.samples_to_annotate = []
        
        for i in range(num_samples):
            # æ¨¡æ‹Ÿéª¨æ¶æ•°æ® (25å…³èŠ‚ç‚¹ x 3åæ ‡)
            skeleton = np.random.randn(25, 3) * 0.5
            
            # æ¨¡æ‹Ÿtokenåºåˆ— - ä½¿ç”¨ä¸€äº›é¢„è®¾çš„ç»„åˆ
            if i % 10 == 0:
                tokens = [28, 58, 65, 18, 23]  # åº†ç¥åŠ¨ä½œ
            elif i % 10 == 1:
                tokens = [15, 76, 119, 72, 23]  # é—®å€™åŠ¨ä½œ
            elif i % 10 == 2:
                tokens = [45, 32, 41, 113, 126]  # æ£€æŸ¥åŠ¨ä½œ
            else:
                tokens = [
                    np.random.randint(0, 128),
                    np.random.randint(0, 128),
                    np.random.randint(0, 128),
                    np.random.randint(0, 128),
                    np.random.randint(0, 128)
                ]
            
            self.samples_to_annotate.append({
                'id': i,
                'skeleton': skeleton,
                'tokens': tokens,
                'annotated': False,
                'source': 'generated'
            })
            
        print(f"âœ… ç”Ÿæˆå®Œæˆï¼Œå…± {len(self.samples_to_annotate)} ä¸ªæ ·æœ¬")
        
    def load_real_data(self, data_source: str = "ntu"):
        """åŠ è½½çœŸå®æ•°æ®
        
        Args:
            data_source: "ntu" æˆ– "radar_gt" æˆ– "both" æˆ– "mars_tokens"
        """
        print(f"ğŸ“¥ åŠ è½½çœŸå®æ•°æ®æº: {data_source}")
        
        if data_source == "ntu":
            self._load_ntu_dataset()
        elif data_source == "radar_gt":
            self._load_radar_ground_truth()
        elif data_source == "mars_tokens":
            self._load_mars_token_dataset()
        elif data_source == "both":
            self._load_ntu_dataset()
            self._load_radar_ground_truth()
        else:
            print("âŒ æ— æ•ˆçš„æ•°æ®æºï¼Œä½¿ç”¨ç¤ºä¾‹æ•°æ®")
            self.generate_sample_data()
            
    def _load_ntu_dataset(self):
        """åŠ è½½NTU RGB+Dæ•°æ®é›†"""
        print("ğŸ“Š åŠ è½½NTU RGB+Dæ•°æ®é›†...")
        
        try:
            # NTUæ•°æ®é›†è·¯å¾„ - ä¼˜å…ˆä½¿ç”¨éª¨æ¶æ•°æ®
            ntu_data_paths = [
                "/home/uo/myProject/HumanPoint-BERT/data/NTU-RGB+D",  # ä¼˜å…ˆï¼šåŸå§‹éª¨æ¶æ•°æ®è·¯å¾„
                "/home/uo/myProject/HumanPoint-BERT/data/NTU-Pred",  # å¤‡ç”¨ï¼šé¢„å¤„ç†ç‚¹äº‘æ•°æ®è·¯å¾„
                "/home/uo/myProject/CRSkeleton/data/ntu"  # å¤‡ç”¨æœ¬åœ°è·¯å¾„
            ]
            
            # æŸ¥æ‰¾å¯ç”¨çš„NTUæ•°æ®è·¯å¾„
            ntu_path = None
            for path in ntu_data_paths:
                if os.path.exists(path):
                    ntu_path = path
                    break
                    
            if ntu_path is None:
                print("âš ï¸ æœªæ‰¾åˆ°NTUæ•°æ®é›†ï¼Œç”Ÿæˆæ¨¡æ‹ŸNTUæ•°æ®...")
                self._generate_simulated_ntu_data()
                return
                
            print(f"âœ… æ‰¾åˆ°NTUæ•°æ®é›†: {ntu_path}")
            
            # åŠ è½½NTUåŠ¨ä½œæ ‡ç­¾æ˜ å°„
            ntu_action_labels = {
                1: "drink water", 2: "eat meal/snack", 3: "brushing teeth", 4: "brushing hair",
                5: "drop", 6: "pickup", 7: "throw", 8: "sitting down", 9: "standing up (from sitting position)",
                10: "clapping", 11: "reading", 12: "writing", 13: "tear up paper", 14: "wear jacket",
                15: "take off jacket", 16: "wear a shoe", 17: "take off a shoe", 18: "wear on glasses",
                19: "take off glasses", 20: "put on a hat/cap", 21: "take off a hat/cap", 22: "cheer up",
                23: "hand waving", 24: "kicking something", 25: "reach into pocket", 26: "hopping (one foot jumping)",
                27: "jump up", 28: "make a phone call/answer phone", 29: "playing with phone/tablet", 30: "typing on a keyboard",
                31: "pointing to something with finger", 32: "taking a selfie", 33: "check time (from watch)",
                34: "rub two hands together", 35: "nod head/bow", 36: "shake head", 37: "wipe face",
                38: "salute", 39: "put the palms together", 40: "cross hands in front (say stop)",
                41: "sneeze/cough", 42: "staggering", 43: "falling", 44: "touch head (headache)",
                45: "touch chest (stomachache/heart pain)", 46: "touch back (backache)", 47: "touch neck (neckache)",
                48: "nausea or vomiting condition", 49: "use a fan (with hand or paper)/feeling warm",
                50: "punching/slapping other person", 51: "kicking other person", 52: "pushing other person",
                53: "pat on back of other person", 54: "point finger at the other person", 55: "hugging other person",
                56: "giving something to other person", 57: "touch other person's pocket", 58: "handshaking",
                59: "walking towards each other", 60: "walking apart from each other"
            }
            
            # åŠ è½½å®é™…NTUæ•°æ®æ ·æœ¬
            self._load_ntu_samples(ntu_path, ntu_action_labels)
            
        except Exception as e:
            print(f"âŒ åŠ è½½NTUæ•°æ®å¤±è´¥: {e}")
            self._generate_simulated_ntu_data()
            
    def _load_radar_ground_truth(self):
        """åŠ è½½é›·è¾¾æ•°æ®é›†çš„Ground Truth"""
        print("ğŸ“¡ åŠ è½½é›·è¾¾æ•°æ®é›†Ground Truth...")
        
        try:
            # é›·è¾¾æ•°æ®é›†è·¯å¾„ - ä½¿ç”¨å®é™…MARSé¡¹ç›®è·¯å¾„
            radar_gt_paths = [
                "/home/uo/myProject/HumanPoint-BERT/data/MARS",  # MARSé›·è¾¾æ•°æ®è·¯å¾„
                "/home/uo/myProject/CRSkeleton/data/radar_gt",  # å¤‡ç”¨æœ¬åœ°è·¯å¾„
                "./data/radar_gt"  # ç›¸å¯¹è·¯å¾„å¤‡ç”¨
            ]
            
            # æŸ¥æ‰¾å¯ç”¨çš„é›·è¾¾GTæ•°æ®è·¯å¾„
            radar_path = None
            for path in radar_gt_paths:
                if os.path.exists(path):
                    radar_path = path
                    break
                    
            if radar_path is None:
                print("âš ï¸ æœªæ‰¾åˆ°é›·è¾¾Ground Truthæ•°æ®ï¼Œç”Ÿæˆæ¨¡æ‹Ÿæ•°æ®...")
                self._generate_simulated_radar_data()
                return
                
            print(f"âœ… æ‰¾åˆ°é›·è¾¾Ground Truth: {radar_path}")
            # åŠ è½½å®é™…MARSé›·è¾¾æ•°æ®
            self._load_mars_samples(radar_path)
            
        except Exception as e:
            print(f"âŒ åŠ è½½é›·è¾¾Ground Truthå¤±è´¥: {e}")
            self._generate_simulated_radar_data()
    
    def _load_ntu_samples(self, ntu_path, action_labels, num_samples=10):
        """åŠ è½½å®é™…NTUæ•°æ®æ ·æœ¬"""
        try:
            print(f"ğŸ”„ ä»NTUæ•°æ®é›†åŠ è½½æ ·æœ¬: {ntu_path}")
            
            # é¦–å…ˆå°è¯•åŠ è½½.skeletonæ–‡ä»¶ï¼ˆçœŸæ­£çš„éª¨æ¶æ•°æ®ï¼‰
            if os.path.exists(ntu_path):
                skeleton_files = [f for f in os.listdir(ntu_path) if f.endswith('.skeleton')]
                if skeleton_files:
                    print(f"ğŸ“‹ æ‰¾åˆ°{len(skeleton_files)}ä¸ª.skeletonæ–‡ä»¶ï¼ŒåŠ è½½éª¨æ¶æ•°æ®")
                    return self._load_ntu_skeleton_files(ntu_path, action_labels, num_samples)
                
                # å¦‚æœæ²¡æœ‰.skeletonæ–‡ä»¶ï¼Œå†å°è¯•H5æ–‡ä»¶
                h5_files = [f for f in os.listdir(ntu_path) if f.endswith('.h5')]
                if h5_files and h5py is not None:
                    print(f"ğŸ“‹ æ‰¾åˆ°{len(h5_files)}ä¸ª.h5æ–‡ä»¶ï¼ŒåŠ è½½ç‚¹äº‘æ•°æ®")
                    return self._load_ntu_h5_files(ntu_path, action_labels, num_samples)
                elif h5_files:
                    print("âŒ h5py æœªå®‰è£…ï¼Œæ— æ³•åŠ è½½HDF5æ•°æ®")
                    return False
            
            print("âš ï¸ æœªæ‰¾åˆ°NTUæ•°æ®æ–‡ä»¶ï¼Œä½¿ç”¨æ¨¡æ‹Ÿæ•°æ®")
            return False
                
        except Exception as e:
            print(f"âŒ åŠ è½½NTUçœŸå®æ•°æ®å¤±è´¥: {e}")
            return False
    
    def _load_ntu_skeleton_files(self, ntu_path, action_labels, num_samples=10):
        """åŠ è½½NTU .skeletonæ–‡ä»¶ï¼ˆçœŸæ­£çš„25å…³èŠ‚éª¨æ¶æ•°æ®ï¼‰"""
        try:
            skeleton_files = [f for f in os.listdir(ntu_path) if f.endswith('.skeleton')]
            selected_files = skeleton_files[:min(num_samples, len(skeleton_files))]
            
            sample_id_offset = len(self.samples_to_annotate)
            loaded_count = 0
            
            for filename in selected_files:
                try:
                    filepath = os.path.join(ntu_path, filename)
                    skeleton_data = self._read_ntu_skeleton_file(filepath)
                    
                    if skeleton_data is not None and len(skeleton_data) > 0:
                        # å–ç¬¬ä¸€å¸§çš„éª¨æ¶æ•°æ®ï¼ˆ25ä¸ªå…³èŠ‚ç‚¹ï¼‰
                        sample_frame = skeleton_data[0]  # (25, 3)
                        
                        # ä»æ–‡ä»¶åæå–åŠ¨ä½œID
                        action_id = self._extract_action_id_from_filename(filename)
                        action_name = action_labels.get(action_id, f"action_{action_id}")
                        
                        # åˆ›å»ºæ ·æœ¬ - æ³¨æ„è¿™é‡Œç”¨skeleton_dataè€Œä¸æ˜¯point_cloud_data
                        sample = {
                            'id': sample_id_offset + loaded_count,
                            'tokens': self._skeleton_to_mock_tokens(sample_frame),
                            'source': 'ntu_real',
                            'filename': filename,
                            'ground_truth_action': action_name,
                            'skeleton_data': sample_frame,  # å…³é”®ï¼š25ä¸ªå…³èŠ‚ç‚¹
                            'total_frames': len(skeleton_data)
                        }
                        
                        self.samples_to_annotate.append(sample)
                        loaded_count += 1
                        print(f"âœ… åŠ è½½NTUéª¨æ¶æ ·æœ¬ {loaded_count}: {filename} -> {action_name} (å…³èŠ‚æ•°: {sample_frame.shape[0]}, å¸§æ•°: {len(skeleton_data)})")
                        
                except Exception as e:
                    print(f"âš ï¸ è·³è¿‡éª¨æ¶æ–‡ä»¶ {filename}: {e}")
                    continue
                    
            print(f"ğŸ“Š æˆåŠŸåŠ è½½ {loaded_count} ä¸ªçœŸå®NTUéª¨æ¶æ ·æœ¬")
            return loaded_count > 0
            
        except Exception as e:
            print(f"âŒ åŠ è½½NTUéª¨æ¶æ–‡ä»¶å¤±è´¥: {e}")
            return False
    
    def _load_ntu_h5_files(self, ntu_path, action_labels, num_samples=10):
        """åŠ è½½NTU H5æ–‡ä»¶ï¼ˆç‚¹äº‘æ•°æ®ï¼‰"""
        try:
            h5_files = [f for f in os.listdir(ntu_path) if f.endswith('.h5')]
            selected_files = h5_files[:min(num_samples, len(h5_files))]
            
            sample_id_offset = len(self.samples_to_annotate)
            loaded_count = 0
            
            for i, filename in enumerate(selected_files):
                try:
                    filepath = os.path.join(ntu_path, filename)
                    with h5py.File(filepath, 'r') as f:
                        # æ£€æŸ¥å¯ç”¨çš„é”®
                        available_keys = list(f.keys())
                        print(f"ğŸ“‹ æ–‡ä»¶ {filename} å¯ç”¨é”®: {available_keys}")
                        
                        # å°è¯•å¸¸è§çš„éª¨æ¶æ•°æ®é”®
                        skeleton_key = None
                        for key in ['enhanced_data', 'skeleton', 'data', 'joints', 'keypoints']:
                            if key in f:
                                skeleton_key = key
                                break
                        
                        if skeleton_key:
                            point_cloud_data = f[skeleton_key][:]
                            # NTU-Predæ•°æ®æ ¼å¼: [frames, points, coords] = (103, 720, 3)
                            if len(point_cloud_data.shape) == 3 and point_cloud_data.shape[0] > 0:
                                # å–ç¬¬ä¸€å¸§ç‚¹äº‘æ•°æ®
                                sample_frame = point_cloud_data[0]  # (720, 3)
                                
                                # ä»æ–‡ä»¶åæå–åŠ¨ä½œIDï¼ˆNTUæ ¼å¼ï¼š...A[action_id]...ï¼‰
                                action_id = self._extract_action_id_from_filename(filename)
                                action_name = action_labels.get(action_id, f"action_{action_id}")
                                
                                # åˆ›å»ºæ ·æœ¬ - è¿™æ˜¯ç‚¹äº‘æ•°æ®
                                sample = {
                                    'id': sample_id_offset + loaded_count,
                                    'tokens': self._pointcloud_to_mock_tokens(sample_frame),
                                    'source': 'ntu_real',
                                    'filename': filename,
                                    'ground_truth_action': action_name,
                                    'point_cloud_data': sample_frame,  # 720ä¸ªç‚¹çš„ç‚¹äº‘
                                    'total_frames': point_cloud_data.shape[0]
                                }
                                
                                self.samples_to_annotate.append(sample)
                                loaded_count += 1
                                print(f"âœ… åŠ è½½NTUç‚¹äº‘æ ·æœ¬ {loaded_count}: {filename} -> {action_name} (ç‚¹æ•°: {sample_frame.shape[0]}, å¸§æ•°: {point_cloud_data.shape[0]})")
                except Exception as e:
                    print(f"âš ï¸ è·³è¿‡æ–‡ä»¶ {filename}: {e}")
                    continue
            
            print(f"ğŸ“Š æˆåŠŸåŠ è½½ {loaded_count} ä¸ªçœŸå®NTUç‚¹äº‘æ ·æœ¬")
            return loaded_count > 0
            
        except Exception as e:
            print(f"âŒ åŠ è½½NTU H5æ–‡ä»¶å¤±è´¥: {e}")
            return False
    
    def _load_mars_samples(self, mars_path, num_samples=8):
        """åŠ è½½å®é™…MARSéª¨æ¶ground truthæ•°æ®æ ·æœ¬"""
        try:
            print(f"ğŸ”„ ä»MARSæ•°æ®é›†åŠ è½½éª¨æ¶ground truth: {mars_path}")
            
            # ç›´æ¥ä½¿ç”¨éª¨æ¶æ ‡ç­¾æ–‡ä»¶ï¼ˆground truthï¼‰
            skeleton_files = [
                'labels_test.npy',
                'labels_train.npy',
                'labels_validate.npy'
            ]
            
            sample_id_offset = len(self.samples_to_annotate)
            loaded_count = 0
            
            for skeleton_file in skeleton_files:
                skeleton_path = os.path.join(mars_path, skeleton_file)
                
                if os.path.exists(skeleton_path):
                    try:
                        # åŠ è½½éª¨æ¶ground truthæ•°æ®
                        skeleton_labels = np.load(skeleton_path)
                        
                        print(f"ğŸ“Š {skeleton_file} éª¨æ¶æ•°æ®å½¢çŠ¶: {skeleton_labels.shape}")
                        
                        # é€‰æ‹©æ ·æœ¬
                        total_samples = skeleton_labels.shape[0]
                        selected_indices = np.random.choice(total_samples, min(num_samples//3, total_samples), replace=False)
                        
                        for idx in selected_indices:
                            # è§£æ57ç»´æ•°æ®ä¸º19Ã—3éª¨æ¶æ ¼å¼
                            # å‚è€ƒvis_gif_skeleton_extractor.pyçš„parse_jointså‡½æ•°
                            # æ ¼å¼: (x1...x19, y1...y19, z1...z19)
                            skeleton_57d = skeleton_labels[idx]
                            skeleton_19x3 = self._parse_mars_joints(skeleton_57d)
                            
                            # åˆ›å»ºæ ·æœ¬ - æ³¨æ„è¿™é‡Œç”¨skeleton_dataè€Œä¸æ˜¯radar_data
                            sample = {
                                'id': sample_id_offset + loaded_count,
                                'tokens': self._skeleton_to_mock_tokens(skeleton_19x3),
                                'source': 'mars_real',
                                'filename': f"{skeleton_file}_{idx}",
                                'ground_truth_action': f"mars_skeleton_{idx}",
                                'skeleton_data': skeleton_19x3  # 19Ã—3éª¨æ¶æ•°æ®
                            }
                            
                            self.samples_to_annotate.append(sample)
                            loaded_count += 1
                            print(f"âœ… åŠ è½½MARSéª¨æ¶æ ·æœ¬ {loaded_count}: {skeleton_file}[{idx}] (å…³èŠ‚æ•°: {skeleton_19x3.shape[0]})")
                            
                            if loaded_count >= num_samples:
                                break
                                
                    except Exception as e:
                        print(f"âš ï¸ åŠ è½½ {skeleton_file} å¤±è´¥: {e}")
                        continue
                
                if loaded_count >= num_samples:
                    break
            
            print(f"ğŸ“Š æˆåŠŸåŠ è½½ {loaded_count} ä¸ªçœŸå®MARSéª¨æ¶æ ·æœ¬")
            return loaded_count > 0
            
        except Exception as e:
            print(f"âŒ åŠ è½½MARSçœŸå®æ•°æ®å¤±è´¥: {e}")
            return False
    
    def _load_mars_token_dataset(self, num_samples=None):
        """åŠ è½½MARS_recon_tokensæ•°æ®é›†(å¸¦tokenåºåˆ—)
        
        è¿™æ˜¯æ ‡æ³¨çš„ä¸»è¦æ•°æ®æº,åŒ…å«:
        - æå–çš„éª¨æ¶ (extracted)
        - é‡å»ºçš„éª¨æ¶ (reconstructed) 
        - Tokenåºåˆ— (5ä¸ªéƒ¨ä½token)
        - VQæŸå¤±
        
        Args:
            num_samples: åŠ è½½æ ·æœ¬æ•°é‡,Noneè¡¨ç¤ºå…¨éƒ¨åŠ è½½
        """
        print("\nğŸ¯ åŠ è½½ MARS Token æ•°æ®é›† (ç”¨äºæ ‡æ³¨)")
        print("=" * 70)
        
        try:
            # MARS_recon_tokens ç›®å½•
            token_data_dir = 'data/MARS_recon_tokens'
            
            if not os.path.exists(token_data_dir):
                print(f"âŒ æœªæ‰¾åˆ°ç›®å½•: {token_data_dir}")
                print("ğŸ’¡ è¯·å…ˆè¿è¡Œ skeleton_extraction_reconstruction_saver.py ç”Ÿæˆæ•°æ®")
                return False
            
            # è¯»å– CSV ç´¢å¼•æ–‡ä»¶
            csv_path = os.path.join(token_data_dir, 'index.csv')
            if not os.path.exists(csv_path):
                print(f"âŒ æœªæ‰¾åˆ°ç´¢å¼•æ–‡ä»¶: {csv_path}")
                return False
            
            import pandas as pd
            index_df = pd.read_csv(csv_path)
            
            print(f"âœ… æ‰¾åˆ° {len(index_df)} ä¸ªæ ·æœ¬")
            print(f"   æ•°æ®åˆ—: {list(index_df.columns)}")
            
            # å†³å®šåŠ è½½å¤šå°‘æ ·æœ¬
            if num_samples is None:
                samples_to_load = len(index_df)
                print(f"ğŸ“Š åŠ è½½å…¨éƒ¨ {samples_to_load} ä¸ªæ ·æœ¬")
            else:
                samples_to_load = min(num_samples, len(index_df))
                print(f"ğŸ“Š åŠ è½½å‰ {samples_to_load} ä¸ªæ ·æœ¬")
            
            # åŠ è½½æ ·æœ¬
            sample_id_offset = len(self.samples_to_annotate)
            loaded_count = 0
            
            for idx in range(samples_to_load):
                row = index_df.iloc[idx]
                
                try:
                    # è¯»å–å•ä¸ªæ ·æœ¬æ–‡ä»¶
                    sample_file = row['file_path']
                    if not os.path.exists(sample_file):
                        print(f"âš ï¸ æ ·æœ¬æ–‡ä»¶ä¸å­˜åœ¨: {sample_file}")
                        continue
                    
                    data = np.load(sample_file)
                    
                    # è§£æ tokens å­—ç¬¦ä¸² "[1, 2, 3, 4, 5]" -> [1, 2, 3, 4, 5]
                    tokens_str = row['tokens_str']
                    tokens = eval(tokens_str) if isinstance(tokens_str, str) else tokens_str
                    
                    # åˆ›å»ºæ ·æœ¬
                    sample = {
                        'id': sample_id_offset + loaded_count,
                        'tokens': tokens,  # 5ä¸ªéƒ¨ä½token
                        'source': 'mars_tokens',
                        'filename': os.path.basename(sample_file),
                        'file_path': sample_file,
                        'split': row['split'],
                        'vq_loss': float(row['vq_loss']),
                        'token_first': int(row['token_first']),
                        # éª¨æ¶æ•°æ®
                        'extracted': data['extracted'],  # (25, 3)
                        'reconstructed': data['reconstructed'],  # (25, 3)
                        'annotated': False
                    }
                    
                    self.samples_to_annotate.append(sample)
                    loaded_count += 1
                    
                    if loaded_count % 1000 == 0:
                        print(f"   å·²åŠ è½½ {loaded_count}/{samples_to_load} ä¸ªæ ·æœ¬...")
                    
                except Exception as e:
                    print(f"âš ï¸ åŠ è½½æ ·æœ¬ {idx} å¤±è´¥: {e}")
                    continue
            
            print(f"\nâœ… æˆåŠŸåŠ è½½ {loaded_count} ä¸ª MARS Token æ ·æœ¬")
            print(f"   - æ¯ä¸ªæ ·æœ¬åŒ…å«: 5ä¸ªtoken + æå–éª¨æ¶ + é‡å»ºéª¨æ¶")
            print(f"   - æ•°æ®æ¥æº: {token_data_dir}")
            
            # è®¾ç½®ä¸ºMARSåŠ¨ä½œæ¨¡æ¿ï¼ˆæ›´é€‚åˆéª¨æ¶æ•°æ®æ ‡æ³¨ï¼‰
            self.action_templates = self.mars_action_templates
            print(f"   - ä½¿ç”¨ MARS åŠ¨ä½œæ¨¡æ¿è¿›è¡Œæ ‡æ³¨")
            
            return loaded_count > 0
            
        except Exception as e:
            print(f"âŒ åŠ è½½ MARS Token æ•°æ®é›†å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            return False
    
    def _extract_action_id_from_filename(self, filename):
        """ä»NTUæ–‡ä»¶åæå–åŠ¨ä½œID"""
        import re
        # NTUæ–‡ä»¶åæ ¼å¼ï¼šS001C001P001R001A001.h5 æˆ– S001C001P001R001A001.skeleton
        match = re.search(r'A(\d+)', filename)
        if match:
            return int(match.group(1))
        return 1  # é»˜è®¤åŠ¨ä½œID
    
    def _read_ntu_skeleton_file(self, file_path):
        """è¯»å–NTU .skeletonæ–‡ä»¶ï¼Œè¿”å›25å…³èŠ‚ç‚¹éª¨æ¶æ•°æ®ï¼ˆæ–‡æœ¬æ ¼å¼ï¼‰"""
        try:
            with open(file_path, 'r') as f:
                # è¯»å–å¸§æ•°
                frame_count = int(f.readline().strip())
                
                frames_data = []
                for frame_idx in range(min(frame_count, 5)):  # æœ€å¤šè¯»å–5å¸§
                    # è¯»å–äººä½“æ•°é‡
                    body_count = int(f.readline().strip())
                    
                    frame_skeletons = []
                    for body_idx in range(body_count):
                        # è¯»å–äººä½“ä¿¡æ¯è¡Œï¼ˆè·³è¿‡ï¼‰
                        body_info = f.readline().strip()
                        
                        # è¯»å–å…³èŠ‚æ•°é‡
                        joint_count = int(f.readline().strip())
                        
                        # è¯»å–å…³èŠ‚æ•°æ®
                        joints = []
                        for joint_idx in range(joint_count):
                            joint_line = f.readline().strip().split()
                            if len(joint_line) >= 3:
                                # NTU RGB+Dåæ ‡è½¬æ¢: (x,z,y) -> (x,y,z) ä»…ç”¨äºå¯è§†åŒ–
                                # å‚è€ƒgcn_skeleton_gif_visualizer.pyçš„å¤„ç†æ–¹å¼
                                x, z, y = float(joint_line[0]), float(joint_line[1]), float(joint_line[2])
                                joints.append([x, y, z])
                        
                        if len(joints) == 25:  # NTUæœ‰25ä¸ªå…³èŠ‚
                            skeleton = np.array(joints)
                            # æ•°æ®é¢„å¤„ç†ï¼šæ ‡å‡†åŒ–åæ ‡ç³»
                            skeleton = self._normalize_ntu_skeleton(skeleton)
                            frame_skeletons.append(skeleton)
                    
                    if frame_skeletons:
                        # å¦‚æœæœ‰å¤šä¸ªäººä½“ï¼Œé€‰æ‹©ç¬¬ä¸€ä¸ª
                        frames_data.append(frame_skeletons[0])
                
                return frames_data if frames_data else None
                
        except Exception as e:
            print(f"è¯»å–éª¨æ¶æ–‡ä»¶å¤±è´¥: {e}")
            return None
    
    def _parse_mars_joints(self, joints_data):
        """è§£æMARSå…³èŠ‚æ•°æ®æ ¼å¼: (x1...x19, y1...y19, z1...z19)
        å‚è€ƒvis_gif_skeleton_extractor.pyçš„parse_jointså‡½æ•°
        """
        if joints_data.shape == (57,):
            x_coords = joints_data[0:19]
            y_coords = joints_data[19:38]  
            z_coords = joints_data[38:57]
            return np.column_stack((x_coords, y_coords, z_coords))
        else:
            raise ValueError(f"æ— æ•ˆçš„MARSå…³èŠ‚æ•°æ®å½¢çŠ¶: {joints_data.shape}")
    
    def _normalize_ntu_skeleton(self, skeleton):
        """æ ‡å‡†åŒ–NTUéª¨æ¶æ•°æ®ï¼Œä½¿å…¶é€‚åˆå¯è§†åŒ–"""
        # 1. ä»¥è„Šæ¤ä¸­å¿ƒï¼ˆå…³èŠ‚1ï¼‰ä¸ºåŸç‚¹
        spine_center = skeleton[1].copy()  # è„Šæ¤ä¸­å¿ƒ
        skeleton_centered = skeleton - spine_center
        
        # 2. è°ƒæ•´åæ ‡ç³»ï¼šåŸå§‹NTUæ•°æ®Zæ˜¯æ·±åº¦ï¼Œæˆ‘ä»¬éœ€è¦Zä¸ºç«–ç›´æ–¹å‘
        # NTUåæ ‡ç³»: X-å·¦å³, Y-ä¸Šä¸‹, Z-å‰å(æ·±åº¦)
        # ç›®æ ‡åæ ‡ç³»: X-å·¦å³, Y-å‰å, Z-ä¸Šä¸‹
        skeleton_reoriented = skeleton_centered.copy()
        skeleton_reoriented[:, 1] = skeleton_centered[:, 2]  # Z(æ·±åº¦) -> Y(å‰å)
        skeleton_reoriented[:, 2] = skeleton_centered[:, 1]  # Y(ä¸Šä¸‹) -> Z(ç«–ç›´)
        
        # 3. ç¼©æ”¾åˆ°åˆé€‚çš„èŒƒå›´
        max_range = np.abs(skeleton_reoriented).max()
        if max_range > 0:
            skeleton_reoriented = skeleton_reoriented / max_range
        
        return skeleton_reoriented
    
    def _pointcloud_to_mock_tokens(self, point_cloud_data):
        """å°†ç‚¹äº‘æ•°æ®è½¬æ¢ä¸ºæ¨¡æ‹Ÿtoken (NTU-Predæ ¼å¼)"""
        # ç‚¹äº‘æ•°æ®æ ¼å¼: (720, 3) - 720ä¸ªç‚¹ï¼Œæ¯ä¸ªç‚¹xyzåæ ‡
        if point_cloud_data.shape[0] >= 720 and point_cloud_data.shape[1] == 3:
            tokens = []
            # å°†720ä¸ªç‚¹åˆ†ä¸º5ä¸ªåŒºåŸŸ (å¯¹åº”5ä¸ªèº«ä½“éƒ¨ä½)
            points_per_part = 144  # 720 / 5 = 144
            
            for i in range(5):
                start_idx = i * points_per_part
                end_idx = start_idx + points_per_part if i < 4 else point_cloud_data.shape[0]
                part_points = point_cloud_data[start_idx:end_idx]
                
                if len(part_points) > 0:
                    # è®¡ç®—è¯¥éƒ¨åˆ†ç‚¹äº‘çš„ç‰¹å¾
                    centroid = np.mean(part_points, axis=0)
                    variance = np.var(part_points, axis=0)
                    feature_sum = np.sum(np.abs(centroid)) + np.sum(variance)
                    token = int(feature_sum * 50) % 128  # æ˜ å°„åˆ°0-127
                    tokens.append(token)
                else:
                    tokens.append(np.random.randint(0, 128))
            
            return tokens
        else:
            return [np.random.randint(0, 128) for _ in range(5)]
    
    def _skeleton_to_mock_tokens(self, skeleton_data):
        """å°†éª¨æ¶æ•°æ®è½¬æ¢ä¸ºæ¨¡æ‹Ÿtoken"""
        # ç®€å•çš„éª¨æ¶->tokenæ˜ å°„
        if skeleton_data.shape[0] >= 25:  # NTU 25å…³èŠ‚
            # åŸºäºå…³èŠ‚ä½ç½®ç”Ÿæˆtoken
            tokens = []
            for part_name in self.body_parts:
                part_joints = self._get_part_joints(part_name, skeleton_data)
                if len(part_joints) > 0:
                    # åŸºäºå…³èŠ‚ä½ç½®è®¡ç®—token
                    avg_pos = np.mean(part_joints, axis=0)
                    token = int(np.sum(np.abs(avg_pos)) * 100) % 128  # ç®€å•æ˜ å°„åˆ°0-127
                    tokens.append(token)
                else:
                    tokens.append(np.random.randint(0, 128))
            return tokens[:5]  # è¿”å›5ä¸ªéƒ¨ä½çš„token
        else:
            return [np.random.randint(0, 128) for _ in range(5)]
    
    def _radar_to_mock_tokens(self, radar_data):
        """å°†é›·è¾¾æ•°æ®è½¬æ¢ä¸ºæ¨¡æ‹Ÿtoken"""
        # ç®€å•çš„é›·è¾¾->tokenæ˜ å°„
        if len(radar_data.shape) >= 1:
            tokens = []
            # å°†é›·è¾¾æ•°æ®åˆ†ä¸º5ä¸ªåŒºåŸŸ
            data_flat = radar_data.flatten()
            chunk_size = len(data_flat) // 5
            for i in range(5):
                start_idx = i * chunk_size
                end_idx = start_idx + chunk_size if i < 4 else len(data_flat)
                chunk = data_flat[start_idx:end_idx]
                token = int(np.mean(np.abs(chunk)) * 1000) % 128 if len(chunk) > 0 else 0
                tokens.append(token)
            return tokens
        else:
            return [np.random.randint(0, 128) for _ in range(5)]
    
    def _get_part_joints(self, part_name, skeleton_data):
        """è·å–èº«ä½“éƒ¨ä½å¯¹åº”çš„å…³èŠ‚ç‚¹"""
        # æ ¹æ®å…³èŠ‚æ•°é‡è‡ªåŠ¨åˆ¤æ–­æ•°æ®é›†ç±»å‹
        num_joints = skeleton_data.shape[0]
        
        if num_joints == 25:  # NTU RGB+D 25å…³èŠ‚ç‚¹æ˜ å°„
            # åˆ‡æ¢åˆ°NTUåŠ¨ä½œæ¨¡æ¿
            self.action_templates = self.ntu_action_templates
            joint_mapping = {
                'head_spine': [0, 1, 2, 3, 20],  # å¤´éƒ¨å’Œè„Šæ¤
                'left_arm': [4, 5, 6, 7, 21],    # å·¦è‡‚
                'right_arm': [8, 9, 10, 11, 22], # å³è‡‚
                'left_leg': [12, 13, 14, 15, 23], # å·¦è…¿
                'right_leg': [16, 17, 18, 19, 24] # å³è…¿
            }
        elif num_joints == 19:  # MARS Kinect 19å…³èŠ‚ç‚¹æ˜ å°„
            # åˆ‡æ¢åˆ°MARSç®€åŒ–åŠ¨ä½œæ¨¡æ¿
            self.action_templates = self.mars_action_templates
            joint_mapping = {
                'head_spine': [0, 1, 2, 3, 18],  # å¤´éƒ¨å’Œè„Šæ¤ (spinebase, spinemid, head, neck, spineshoulder)
                'left_arm': [4, 5, 6],           # å·¦è‡‚ (leftshoulder, leftelbow, leftwrist)
                'right_arm': [7, 8, 9],          # å³è‡‚ (rightshoulder, rightelbow, rightwrist)
                'left_leg': [10, 11, 12, 13],    # å·¦è…¿ (hipleft, kneeleft, ankleleft, footleft)
                'right_leg': [14, 15, 16, 17]    # å³è…¿ (hipright, kneeright, ankleright, footright)
            }
        else:
            # æœªçŸ¥æ ¼å¼ï¼Œä½¿ç”¨NTUé»˜è®¤æ˜ å°„
            self.action_templates = self.ntu_action_templates
            joint_mapping = {
                'head_spine': list(range(min(5, num_joints))),
                'left_arm': [],
                'right_arm': [],
                'left_leg': [],
                'right_leg': []
            }
        
        joint_indices = joint_mapping.get(part_name, [])
        valid_joints = []
        for idx in joint_indices:
            if idx < skeleton_data.shape[0]:
                valid_joints.append(skeleton_data[idx])
        return np.array(valid_joints) if len(valid_joints) > 0 else np.array([])
            
    def _generate_simulated_ntu_data(self):
        """ç”ŸæˆåŸºäºNTUåŠ¨ä½œç±»åˆ«çš„æ¨¡æ‹Ÿæ•°æ®"""
        print("ğŸ­ ç”ŸæˆNTUé£æ ¼çš„æ¨¡æ‹Ÿæ•°æ®...")
        
        # NTUå¸¸è§åŠ¨ä½œçš„Tokenæ¨¡å¼
        ntu_token_patterns = {
            "drink water": [15, 76, 41, 18, 23],        # ä¸­æ€§å¤´éƒ¨ + æ‰‹è‡‚åŠ¨ä½œ + ç«™ç«‹
            "clapping": [15, 58, 65, 18, 23],           # ä¸­æ€§å¤´éƒ¨ + åŒæ‰‹æ‹å‡» + ç«™ç«‹
            "hand waving": [15, 32, 119, 18, 23],       # ä¸­æ€§å¤´éƒ¨ + æŒ¥æ‰‹ + ç«™ç«‹
            "sitting down": [45, 32, 41, 113, 126],     # ä½å¤´ + ä¸‹å‚æ‰‹è‡‚ + è¹²å§¿
            "standing up": [28, 58, 65, 72, 78],        # æŠ¬å¤´ + ä¸Šä¸¾æ‰‹è‡‚ + è¿ˆæ­¥
            "jump up": [28, 103, 107, 95, 101],         # æŠ¬å¤´ + ä¾§ä¸¾æ‰‹è‡‚ + è·³è·ƒå‡†å¤‡
            "reading": [45, 76, 82, 18, 23],            # ä½å¤´ + å‰ä¼¸æ‰‹è‡‚ + ç«™ç«‹
            "phone call": [67, 76, 41, 18, 23],         # è½¬å¤´ + å•æ‰‹ä¸¾èµ· + ç«™ç«‹
            "check time": [45, 76, 41, 18, 23],         # ä½å¤´ + çœ‹æ‰‹è…• + ç«™ç«‹
            "cross hands": [15, 124, 119, 18, 23],      # ä¸­æ€§å¤´éƒ¨ + äº¤å‰æ‰‹è‡‚ + ç«™ç«‹
        }
        
        sample_id_offset = len(self.samples_to_annotate)
        
        for action_name, token_pattern in ntu_token_patterns.items():
            # ä¸ºæ¯ä¸ªåŠ¨ä½œç”Ÿæˆ2-3ä¸ªå˜ä½“
            for variant in range(2):
                # æ·»åŠ è½»å¾®éšæœºå˜åŒ–
                varied_tokens = []
                for token in token_pattern:
                    # åœ¨åŸTokenåŸºç¡€ä¸Šæ·»åŠ Â±5çš„éšæœºå˜åŒ–
                    varied_token = token + np.random.randint(-5, 6)
                    varied_token = max(0, min(127, varied_token))  # ç¡®ä¿åœ¨0-127èŒƒå›´å†…
                    varied_tokens.append(varied_token)
                
                # ç”Ÿæˆå¯¹åº”çš„éª¨æ¶æ•°æ®ï¼ˆæ¨¡æ‹Ÿï¼‰
                skeleton = self._generate_skeleton_for_action(action_name)
                
                self.samples_to_annotate.append({
                    'id': sample_id_offset + len(self.samples_to_annotate),
                    'skeleton': skeleton,
                    'tokens': varied_tokens,
                    'annotated': False,
                    'source': 'ntu_simulated',
                    'action_hint': action_name,
                    'original_ntu_action': action_name
                })
                
        print(f"âœ… ç”Ÿæˆäº† {len(ntu_token_patterns) * 2} ä¸ªNTUé£æ ¼æ ·æœ¬")
        
    def _generate_simulated_radar_data(self):
        """ç”Ÿæˆé›·è¾¾æ•°æ®é£æ ¼çš„æ¨¡æ‹Ÿæ•°æ®"""
        print("ğŸ“¡ ç”Ÿæˆé›·è¾¾é£æ ¼çš„æ¨¡æ‹Ÿæ•°æ®...")
        
        # é›·è¾¾æ•°æ®å¯èƒ½æ›´å¤šæ˜¯åŸºç¡€åŠ¨ä½œ
        radar_actions = [
            "walking", "standing", "sitting", "raising_hand", 
            "bending", "turning", "reaching", "pointing"
        ]
        
        sample_id_offset = len(self.samples_to_annotate)
        
        for action in radar_actions:
            for variant in range(3):  # æ¯ä¸ªåŠ¨ä½œ3ä¸ªå˜ä½“
                tokens = [np.random.randint(0, 128) for _ in range(5)]
                skeleton = np.random.randn(25, 3) * 0.3  # æ›´å°çš„å˜åŒ–èŒƒå›´
                
                self.samples_to_annotate.append({
                    'id': sample_id_offset + len(self.samples_to_annotate),
                    'skeleton': skeleton,
                    'tokens': tokens,
                    'annotated': False,
                    'source': 'radar_simulated',
                    'action_hint': action,
                    'radar_action': action
                })
                
        print(f"âœ… ç”Ÿæˆäº† {len(radar_actions) * 3} ä¸ªé›·è¾¾é£æ ¼æ ·æœ¬")
        
    def _generate_ntu_based_samples(self, ntu_labels):
        """åŸºäºçœŸå®NTUæ ‡ç­¾ç”Ÿæˆæ ·æœ¬"""
        print("ğŸ“Š åŸºäºNTUæ ‡ç­¾ç”Ÿæˆæ ·æœ¬...")
        
        # é€‰æ‹©ä¸€äº›ä»£è¡¨æ€§çš„åŠ¨ä½œè¿›è¡Œæ ‡æ³¨
        priority_actions = [1, 2, 8, 9, 10, 23, 27, 28, 31, 35, 36, 40]  # ä¼˜å…ˆæ ‡æ³¨çš„åŠ¨ä½œ
        
        sample_id_offset = len(self.samples_to_annotate)
        
        for action_id in priority_actions:
            if action_id in ntu_labels:
                action_name = ntu_labels[action_id]
                
                # ä¸ºæ¯ä¸ªåŠ¨ä½œç”Ÿæˆæ ·æœ¬
                tokens = self._generate_tokens_for_ntu_action(action_id, action_name)
                skeleton = self._generate_skeleton_for_action(action_name)
                
                self.samples_to_annotate.append({
                    'id': sample_id_offset + len(self.samples_to_annotate),
                    'skeleton': skeleton,
                    'tokens': tokens,
                    'annotated': False,
                    'source': 'ntu_real',
                    'ntu_action_id': action_id,
                    'ntu_action_name': action_name,
                    'priority': True
                })
                
        print(f"âœ… åŸºäºNTUæ ‡ç­¾ç”Ÿæˆäº† {len(priority_actions)} ä¸ªä¼˜å…ˆæ ·æœ¬")
        
    def _generate_tokens_for_ntu_action(self, action_id: int, action_name: str) -> List[int]:
        """æ ¹æ®NTUåŠ¨ä½œç”Ÿæˆåˆç†çš„Tokenåºåˆ—"""
        
        # åŸºäºåŠ¨ä½œè¯­ä¹‰ç”ŸæˆTokenæ¨¡å¼
        token_patterns = {
            1: [15, 76, 41, 18, 23],    # drink water: ä¸­æ€§å¤´éƒ¨ + ä¸¾æ‰‹åˆ°å˜´è¾¹
            2: [45, 76, 82, 18, 23],    # eat meal: ä½å¤´ + åŒæ‰‹è¿›é£ŸåŠ¨ä½œ
            8: [45, 32, 41, 113, 126],  # sitting down: ä½å¤´ + ä¸‹å‚æ‰‹è‡‚ + è¹²å§¿
            9: [28, 58, 65, 72, 78],    # standing up: æŠ¬å¤´ + ä¸Šä¸¾æ‰‹è‡‚ + èµ·ç«‹
            10: [15, 58, 65, 18, 23],   # clapping: ä¸­æ€§å¤´éƒ¨ + åŒæ‰‹æ‹å‡»
            23: [15, 32, 119, 18, 23],  # hand waving: ä¸­æ€§å¤´éƒ¨ + æŒ¥æ‰‹
            27: [28, 103, 107, 95, 101], # jump up: æŠ¬å¤´ + ä¾§ä¸¾ + è·³è·ƒ
            28: [67, 76, 41, 18, 23],   # phone call: è½¬å¤´ + å•æ‰‹ä¸¾èµ·
            31: [15, 76, 82, 18, 23],   # pointing: ä¸­æ€§å¤´éƒ¨ + æŒ‡å‘
            35: [89, 32, 41, 18, 23],   # nod head: ç‚¹å¤´ + è‡ªç„¶æ‰‹è‡‚
            36: [67, 32, 41, 18, 23],   # shake head: æ‘‡å¤´ + è‡ªç„¶æ‰‹è‡‚
            40: [15, 124, 119, 18, 23], # cross hands: ä¸­æ€§å¤´éƒ¨ + äº¤å‰æ‰‹è‡‚
        }
        
        if action_id in token_patterns:
            base_tokens = token_patterns[action_id]
            # æ·»åŠ å°‘é‡éšæœºå˜åŒ–
            varied_tokens = []
            for token in base_tokens:
                variation = np.random.randint(-3, 4)
                varied_token = max(0, min(127, token + variation))
                varied_tokens.append(varied_token)
            return varied_tokens
        else:
            # å¯¹äºæœªé¢„è®¾çš„åŠ¨ä½œï¼Œç”Ÿæˆéšæœºä½†åˆç†çš„Token
            return [np.random.randint(0, 128) for _ in range(5)]
            
    def _generate_skeleton_for_action(self, action_name: str) -> np.ndarray:
        """æ ¹æ®åŠ¨ä½œåç§°ç”Ÿæˆå¯¹åº”çš„éª¨æ¶æ•°æ®"""
        
        # ç”ŸæˆåŸºç¡€éª¨æ¶ (25å…³èŠ‚ç‚¹)
        base_skeleton = np.random.randn(25, 3) * 0.2
        
        # æ ¹æ®åŠ¨ä½œè°ƒæ•´éª¨æ¶å§¿æ€
        if "sitting" in action_name or "down" in action_name:
            # åä¸‹åŠ¨ä½œï¼šé™ä½é«˜åº¦ï¼Œè…¿éƒ¨å¼¯æ›²
            base_skeleton[:, 1] -= 0.3  # é™ä½Yåæ ‡
            base_skeleton[12:20, :] *= 0.7  # è…¿éƒ¨æ”¶ç¼©
            
        elif "jump" in action_name or "up" in action_name:
            # è·³è·ƒåŠ¨ä½œï¼šæŠ¬é«˜ï¼Œæ‰‹è‡‚ä¸Šä¸¾
            base_skeleton[:, 1] += 0.2  # æŠ¬é«˜Yåæ ‡
            base_skeleton[4:12, 1] += 0.3  # æ‰‹è‡‚ä¸Šä¸¾
            
        elif "clapping" in action_name:
            # æ‹æ‰‹ï¼šåŒæ‰‹æ¥è¿‘
            base_skeleton[7, 0] = -0.1   # å·¦æ‰‹å‘ä¸­å¿ƒ
            base_skeleton[11, 0] = 0.1   # å³æ‰‹å‘ä¸­å¿ƒ
            
        elif "waving" in action_name:
            # æŒ¥æ‰‹ï¼šä¸€åªæ‰‹è‡‚æŠ¬èµ·
            base_skeleton[7:9, 1] += 0.4  # æŠ¬èµ·ä¸€åªæ‰‹è‡‚
            
        return base_skeleton
            
    def run_cli_annotation(self):
        """è¿è¡Œå‘½ä»¤è¡Œæ ‡æ³¨ç•Œé¢"""
        print("\nğŸ·ï¸ ç æœ¬åŠ¨ä½œæ ‡æ³¨å·¥å…· (å‘½ä»¤è¡Œæ¨¡å¼)")
        print("=" * 60)
        
        while True:
            self.show_cli_menu()
            choice = safe_input("è¯·é€‰æ‹©æ“ä½œ: ", "0")
            
            if choice == '1':
                self.generate_sample_data()
            elif choice == '2':
                self.load_real_data("ntu")
            elif choice == '3':
                self.load_real_data("radar_gt")
            elif choice == '4':
                self.load_real_data("both")
            elif choice == '5':
                self.load_real_data("mars_tokens")
            elif choice == '6':
                self.annotate_samples_cli()
            elif choice == '7':
                self.batch_annotate_cli()
            elif choice == '8':
                self.show_progress_cli()
            elif choice == '9':
                self.export_annotations()
            elif choice == '10':
                self.load_previous_session()
            elif choice == 'a' or choice == 'A':
                self.token_analysis_cli()
            elif choice == 's' or choice == 'S':
                self.sequence_frame_annotation()
            elif choice == 'v' or choice == 'V':
                self.open_visualization_window()
            elif choice == '0':
                break
            else:
                print("âŒ æ— æ•ˆé€‰æ‹©ï¼Œè¯·é‡è¯•")
                
        print("ğŸ‘‹ æ ‡æ³¨å·¥å…·é€€å‡º")
        
    def show_cli_menu(self):
        """æ˜¾ç¤ºå‘½ä»¤è¡Œèœå•"""
        print("\nğŸ“‹ æ“ä½œèœå•:")
        print("1. ç”Ÿæˆç¤ºä¾‹æ•°æ®")
        print("2. åŠ è½½NTUæ•°æ®é›†") 
        print("3. åŠ è½½é›·è¾¾Ground Truthæ•°æ®")
        print("4. åŠ è½½æ··åˆæ•°æ® (NTU + é›·è¾¾GT)")
        print("5. åŠ è½½MARS Tokenæ•°æ®é›† (æ¨èï¼ŒåŒ…å«Tokenåºåˆ—)")
        print("6. å¼€å§‹æ ‡æ³¨æ ·æœ¬")
        print("7. æ™ºèƒ½æ‰¹é‡æ ‡æ³¨ (ğŸ”¥æ¨è)")
        print("8. æŸ¥çœ‹æ ‡æ³¨è¿›åº¦")
        print("9. å¯¼å‡ºæ ‡æ³¨ç»“æœ")
        print("10. åŠ è½½ä¹‹å‰çš„ä¼šè¯")
        print("a. Tokenåˆ†æä¸é‡‡æ ·ç­–ç•¥")
        print("s. åºåˆ—å¸§æ‰¹æ³¨ (MARSæ¨è)")
        print("v. æ‰“å¼€3Då¯è§†åŒ–çª—å£")
        print("0. é€€å‡º")
        
    def annotate_samples_cli(self):
        """å‘½ä»¤è¡Œæ¨¡å¼æ ‡æ³¨æ ·æœ¬ - åŒ…å«å¯è§†åŒ–"""
        if not self.samples_to_annotate:
            print("âŒ æš‚æ— æ ·æœ¬æ•°æ®ï¼Œè¯·å…ˆç”Ÿæˆæˆ–åŠ è½½æ•°æ®")
            return
            
        print(f"\nğŸ·ï¸ å¼€å§‹æ ‡æ³¨ ({len(self.samples_to_annotate)} ä¸ªæ ·æœ¬)")
        
        for i, sample in enumerate(self.samples_to_annotate):
            if sample.get('annotated', False):
                continue
                
            print(f"\n" + "="*80)
            print(f"ğŸ“‹ æ ·æœ¬ {i+1}/{len(self.samples_to_annotate)}")
            print(f"ğŸ“ æ–‡ä»¶: {sample.get('filename', 'æœªçŸ¥')}")
            print(f"ğŸ¯ Tokenåºåˆ—: {sample['tokens']}")
            
            # æ˜¾ç¤ºGround Truthä¿¡æ¯
            if 'ground_truth_action' in sample:
                print(f"ğŸ­ Ground Truth: {sample['ground_truth_action']}")
            
            # å¯è§†åŒ–æ ·æœ¬æ•°æ®
            print(f"\nğŸ“Š æ•°æ®å¯è§†åŒ–:")
            self._visualize_sample_data_cli(sample)
            
            # é€‰æ‹©æ ‡æ³¨æ¨¡å¼æˆ–æ“ä½œ
            while True:
                print(f"\nğŸ·ï¸ æ“ä½œé€‰æ‹©:")
                print("1. è¯¦ç»†åˆ†éƒ¨ä½æ ‡æ³¨ (æ¨èï¼Œå‡†ç¡®æ€§é«˜)")
                print("2. å¿«é€Ÿæ•´ä½“æ ‡æ³¨ (é€Ÿåº¦å¿«)")
                if GIF_AVAILABLE:
                    print("g. æŸ¥çœ‹ç›¸é‚»å¸§GIFåŠ¨ç”» (æ—¶åºä¸Šä¸‹æ–‡)")
                print("3. è·³è¿‡æ­¤æ ·æœ¬")
                print("4. é€€å‡ºæ ‡æ³¨")
                
                mode_choice = safe_input("é€‰æ‹©æ“ä½œ (1-4/g): ", "4")
                
                if mode_choice.lower() == 'g' and GIF_AVAILABLE:
                    # ç”Ÿæˆå¹¶æ˜¾ç¤ºGIF
                    print(f"\nğŸ¬ ç”Ÿæˆç›¸é‚»å¸§GIFåŠ¨ç”»...")
                    num_frames = safe_input("åŒ…å«å¤šå°‘å¸§? (é»˜è®¤5): ", "5")
                    try:
                        num_frames = int(num_frames)
                        num_frames = max(3, min(num_frames, 11))  # é™åˆ¶åœ¨3-11å¸§
                    except:
                        num_frames = 5
                    
                    gif_path = self._generate_adjacent_frames_gif(i, num_frames=num_frames)
                    if gif_path:
                        print(f"âœ… GIFå·²ä¿å­˜åˆ°: {gif_path}")
                        print(f"ğŸ’¡ æç¤º: å¯ä»¥ç”¨å›¾ç‰‡æŸ¥çœ‹å™¨æ‰“å¼€æŸ¥çœ‹åŠ¨ç”»")
                        input("æŒ‰å›è½¦ç»§ç»­...")
                    continue  # è¿”å›èœå•
                    
                elif mode_choice == '1':
                    success = self._detailed_part_annotation_cli(sample)
                    break
                elif mode_choice == '2':
                    success = self._quick_overall_annotation_cli(sample)
                    break
                elif mode_choice == '3':
                    print("â­ï¸ è·³è¿‡æ ·æœ¬")
                    success = False
                    break
                elif mode_choice == '4' or mode_choice == "":
                    print("ğŸšª é€€å‡ºæ ‡æ³¨")
                    return
                else:
                    print("âŒ æ— æ•ˆé€‰æ‹©ï¼Œè¯·é‡æ–°è¾“å…¥")
                    # é˜²æ­¢æ— é™å¾ªç¯
                    if mode_choice == "":
                        print("ğŸšª è¾“å…¥ä¸ºç©ºï¼Œé€€å‡ºæ ‡æ³¨")
                        return
                    continue
            
            if success:
                sample['annotated'] = True
                print(f"âœ… æ ·æœ¬ {i+1} æ ‡æ³¨å®Œæˆ")
            
            # è¯¢é—®æ˜¯å¦ç»§ç»­
            if i < len(self.samples_to_annotate) - 1:
                continue_choice = safe_input(f"\nç»§ç»­æ ‡æ³¨ä¸‹ä¸€ä¸ªæ ·æœ¬? (y/n, é»˜è®¤y): ", "y").lower()
                if continue_choice == 'n':
                    break
        
        print(f"\nğŸ‰ æ ‡æ³¨ä¼šè¯ç»“æŸ")
    
    def _visualize_sample_data_cli(self, sample):
        """CLIæ¨¡å¼ä¸‹å¯è§†åŒ–æ ·æœ¬æ•°æ®"""
        try:
            # é¦–å…ˆå°è¯•æ‰“å¼€ç‹¬ç«‹å¯è§†åŒ–çª—å£
            if VISUALIZATION_AVAILABLE:
                print("ğŸ–¼ï¸ æ‰“å¼€3Då¯è§†åŒ–çª—å£...")
                success = show_sample_visualization(sample, sample)
                if success:
                    print("âœ… å¯è§†åŒ–çª—å£å·²æ‰“å¼€ï¼Œè¯·æŸ¥çœ‹ç‹¬ç«‹çª—å£")
                    # ç­‰å¾…ç”¨æˆ·ç¡®è®¤çœ‹åˆ°äº†å¯è§†åŒ–
                    input("ğŸ‘€ è¯·æŸ¥çœ‹å¯è§†åŒ–çª—å£ï¼Œç¡®è®¤åæŒ‰å›è½¦ç»§ç»­...")
                else:
                    print("âš ï¸ å¯è§†åŒ–çª—å£æ‰“å¼€å¤±è´¥ï¼Œä½¿ç”¨æ–‡æœ¬æè¿°")
                    self._show_text_visualization(sample)
            else:
                print("ğŸ“Š ä½¿ç”¨æ–‡æœ¬æ¨¡å¼æ˜¾ç¤ºæ•°æ®ä¿¡æ¯:")
                self._show_text_visualization(sample)
        except Exception as e:
            print(f"âš ï¸ å¯è§†åŒ–å¤±è´¥: {e}")
            self._show_text_visualization(sample)
    
    def _show_text_visualization(self, sample):
        """æ˜¾ç¤ºæ–‡æœ¬æ¨¡å¼çš„æ•°æ®ä¿¡æ¯"""
        if 'point_cloud_data' in sample:
            self._show_point_cloud_info(sample['point_cloud_data'], sample)
        elif 'radar_data' in sample:
            self._show_radar_info(sample['radar_data'], sample)
        elif 'skeleton_data' in sample:
            self._show_skeleton_info(sample['skeleton_data'], sample)
        elif 'extracted' in sample or 'reconstructed' in sample:
            # MARS Token æ•°æ®é›†æ ¼å¼: extracted/reconstructed éª¨æ¶
            skeleton = sample.get('reconstructed', sample.get('extracted'))
            self._show_skeleton_info(skeleton, sample)
        else:
            self._show_basic_info(sample)
    
    def _show_point_cloud_info(self, point_cloud_data, sample):
        """æ˜¾ç¤ºç‚¹äº‘æ•°æ®ä¿¡æ¯"""
        print(f"â˜ï¸ ç‚¹äº‘æ•°æ® (å½¢çŠ¶: {point_cloud_data.shape})")
        
        # è®¡ç®—æ•´ä½“ç»Ÿè®¡
        min_coords = np.min(point_cloud_data, axis=0)
        max_coords = np.max(point_cloud_data, axis=0)
        center = np.mean(point_cloud_data, axis=0)
        
        print(f"   ğŸ“ è¾¹ç•Œ: X[{min_coords[0]:.2f}~{max_coords[0]:.2f}] Y[{min_coords[1]:.2f}~{max_coords[1]:.2f}] Z[{min_coords[2]:.2f}~{max_coords[2]:.2f}]")
        print(f"   ğŸ“ ä¸­å¿ƒ: ({center[0]:.2f}, {center[1]:.2f}, {center[2]:.2f})")
        
        # æ˜¾ç¤º5ä¸ªèº«ä½“éƒ¨ä½çš„ç‰¹å¾
        points_per_part = len(point_cloud_data) // 5
        print(f"   ğŸ¦´ å„éƒ¨ä½åˆ†æ:")
        
        for i, part_name in enumerate(['å¤´é¢ˆ', 'å·¦è‡‚', 'å³è‡‚', 'å·¦è…¿', 'å³è…¿']):
            start_idx = i * points_per_part
            end_idx = start_idx + points_per_part if i < 4 else len(point_cloud_data)
            part_points = point_cloud_data[start_idx:end_idx]
            
            if len(part_points) > 0:
                part_center = np.mean(part_points, axis=0)
                part_spread = np.std(part_points, axis=0)
                print(f"     {part_name}: ä¸­å¿ƒ({part_center[0]:.1f},{part_center[1]:.1f},{part_center[2]:.1f}) "
                      f"åˆ†å¸ƒ({part_spread[0]:.1f},{part_spread[1]:.1f},{part_spread[2]:.1f})")
    
    def _show_radar_info(self, radar_data, sample):
        """æ˜¾ç¤ºé›·è¾¾æ•°æ®ä¿¡æ¯"""
        print(f"ğŸ“¡ é›·è¾¾æ•°æ® (å½¢çŠ¶: {radar_data.shape})")
        print(f"   ğŸ“Š å€¼åŸŸ: [{np.min(radar_data):.3f} ~ {np.max(radar_data):.3f}]")
        print(f"   ğŸ“ˆ å‡å€¼: {np.mean(radar_data):.3f}, æ ‡å‡†å·®: {np.std(radar_data):.3f}")
        
        # å¦‚æœæ˜¯ç‰¹å¾å›¾ï¼Œåˆ†æå„é€šé“
        if len(radar_data.shape) == 3 and radar_data.shape[2] == 5:
            print(f"   ğŸ”¬ é€šé“åˆ†æ:")
            for ch in range(5):
                ch_data = radar_data[:, :, ch]
                print(f"     é€šé“{ch}: [{np.min(ch_data):.2f}~{np.max(ch_data):.2f}] å‡å€¼{np.mean(ch_data):.2f}")
    
    def _show_skeleton_info(self, skeleton_data, sample):
        """æ˜¾ç¤ºéª¨æ¶æ•°æ®ä¿¡æ¯"""
        print(f"ğŸ¦´ éª¨æ¶æ•°æ® (å½¢çŠ¶: {skeleton_data.shape})")
        if len(skeleton_data.shape) == 2 and skeleton_data.shape[1] == 3:
            print(f"   ğŸ”— å…³èŠ‚ç‚¹æ•°: {skeleton_data.shape[0]}")
            
            # åˆ†æå„èº«ä½“éƒ¨ä½ (åŸºäºNTU 25å…³èŠ‚æ ‡å‡†)
            joint_groups = {
                'å¤´é¢ˆ': [0, 1, 2, 3, 20],
                'å·¦è‡‚': [4, 5, 6, 7, 21], 
                'å³è‡‚': [8, 9, 10, 11, 22],
                'å·¦è…¿': [12, 13, 14, 15, 23],
                'å³è…¿': [16, 17, 18, 19, 24]
            }
            
            for part_name, joint_indices in joint_groups.items():
                valid_joints = [idx for idx in joint_indices if idx < skeleton_data.shape[0]]
                if valid_joints:
                    part_joints = skeleton_data[valid_joints]
                    center = np.mean(part_joints, axis=0)
                    print(f"     {part_name}: ä¸­å¿ƒ({center[0]:.1f},{center[1]:.1f},{center[2]:.1f})")
    
    def _show_basic_info(self, sample):
        """æ˜¾ç¤ºåŸºç¡€æ ·æœ¬ä¿¡æ¯"""
        print(f"ğŸ“‹ åŸºç¡€ä¿¡æ¯:")
        for key, value in sample.items():
            if key not in ['point_cloud_data', 'radar_data', 'skeleton_data', 'tokens', 'extracted', 'reconstructed']:
                if isinstance(value, (int, float, str, bool)):
                    print(f"   {key}: {value}")
    
    # ==================== GIF å¯è§†åŒ–åŠŸèƒ½ ====================
    
    def _generate_adjacent_frames_gif(self, sample_idx, num_frames=5, output_dir='temp_gifs'):
        """ç”Ÿæˆç›¸é‚»å‡ å¸§çš„GIFåŠ¨ç”»
        
        Args:
            sample_idx: å½“å‰æ ·æœ¬ç´¢å¼•
            num_frames: GIFä¸­åŒ…å«çš„æ€»å¸§æ•° (å»ºè®®å¥‡æ•°,ä»¥å½“å‰æ ·æœ¬ä¸ºä¸­å¿ƒ)
            output_dir: GIFä¿å­˜ç›®å½•
            
        Returns:
            GIFæ–‡ä»¶è·¯å¾„ï¼Œå¦‚æœå¤±è´¥è¿”å›None
        """
        if not GIF_AVAILABLE:
            print("âŒ matplotlibä¸å¯ç”¨ï¼Œæ— æ³•ç”ŸæˆGIF")
            return None
        
        try:
            # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
            os.makedirs(output_dir, exist_ok=True)
            
            # è®¡ç®—ç›¸é‚»æ ·æœ¬ç´¢å¼•èŒƒå›´
            half = num_frames // 2
            start_idx = max(0, sample_idx - half)
            end_idx = min(len(self.samples_to_annotate), sample_idx + half + 1)
            
            # æ”¶é›†éª¨æ¶å¸§æ•°æ®
            skeleton_frames = []
            frame_labels = []
            
            for idx in range(start_idx, end_idx):
                sample = self.samples_to_annotate[idx]
                
                # ä¼˜å…ˆä½¿ç”¨ reconstructedï¼Œå…¶æ¬¡ extractedï¼Œæœ€å skeleton_data
                skeleton = None
                data_source = ""
                
                if 'reconstructed' in sample:
                    skeleton = sample['reconstructed']
                    data_source = "reconstructed"
                elif 'extracted' in sample:
                    skeleton = sample['extracted']
                    data_source = "extracted"
                elif 'skeleton_data' in sample:
                    skeleton = sample['skeleton_data']
                    data_source = "skeleton_data"
                
                if skeleton is not None:
                    # è½¬æ¢ä¸º (num_joints, 3) æ ¼å¼
                    if len(skeleton.shape) == 2 and skeleton.shape[1] == 3:
                        skeleton_frames.append(skeleton)
                        is_current = "â˜…" if idx == sample_idx else ""
                        frame_labels.append(f"Sample {idx} {is_current} ({data_source})")
                    else:
                        print(f"âš ï¸ æ ·æœ¬ {idx} éª¨æ¶æ ¼å¼å¼‚å¸¸: {skeleton.shape}")
            
            if len(skeleton_frames) == 0:
                print("âŒ æ²¡æœ‰æ‰¾åˆ°å¯ç”¨çš„éª¨æ¶æ•°æ®")
                return None
            
            # ç”ŸæˆGIFæ–‡ä»¶å
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            gif_path = os.path.join(output_dir, f"sample_{sample_idx:05d}_{timestamp}.gif")
            
            # è°ƒç”¨GIFç”Ÿæˆå‡½æ•°
            success = self._create_skeleton_sequence_gif(
                skeleton_frames, 
                frame_labels, 
                gif_path,
                title=f"Adjacent Frames Around Sample {sample_idx}"
            )
            
            if success:
                print(f"âœ… GIFå·²ç”Ÿæˆ: {gif_path}")
                return gif_path
            else:
                print(f"âŒ GIFç”Ÿæˆå¤±è´¥")
                return None
                
        except Exception as e:
            print(f"âŒ ç”ŸæˆGIFæ—¶å‡ºé”™: {e}")
            import traceback
            traceback.print_exc()
            return None
    
    def _create_skeleton_sequence_gif(self, skeleton_frames, frame_labels, output_path, 
                                     title="Skeleton Sequence", fps=2):
        """åˆ›å»ºéª¨æ¶åºåˆ—GIFåŠ¨ç”»
        
        Args:
            skeleton_frames: List of (num_joints, 3) numpy arrays
            frame_labels: List of frame label strings
            output_path: GIFè¾“å‡ºè·¯å¾„
            title: GIFæ ‡é¢˜
            fps: å¸§ç‡
            
        Returns:
            æˆåŠŸè¿”å›Trueï¼Œå¤±è´¥è¿”å›False
        """
        if not GIF_AVAILABLE:
            return False
        
        try:
            # æ£€æµ‹éª¨æ¶ç±»å‹å¹¶ä½¿ç”¨å¯¹åº”çš„è¿æ¥
            num_joints = skeleton_frames[0].shape[0]
            
            if num_joints == 19:
                # MARS 19å…³èŠ‚éª¨æ¶è¿æ¥å®šä¹‰ (0-basedç´¢å¼•)
                skeleton_connections = [
                    (2, 3),   # head-neck
                    (2, 18),  # neck-spineshoulder
                    (18, 4),  # spineshoulder-leftshoulder
                    (4, 5),   # leftshoulder-leftelbow
                    (5, 6),   # leftelbow-leftwrist
                    (18, 7),  # spineshoulder-rightshoulder
                    (7, 8),   # rightshoulder-rightelbow
                    (8, 9),   # rightelbow-rightwrist
                    (18, 1),  # spineshoulder-spinemid
                    (1, 0),   # spinemid-spinebase
                    (0, 10),  # spinebase-hipleft
                    (10, 11), # hipleft-kneeleft
                    (11, 12), # kneeleft-ankleleft
                    (12, 13), # ankleleft-footleft
                    (0, 14),  # spinebase-hipright
                    (14, 15), # hipright-kneeright
                    (15, 16), # kneeright-ankleright
                    (16, 17)  # ankleright-footright
                ]
                skeleton_type = "MARS (19 joints)"
            elif num_joints == 25:
                # NTU RGB+D 25å…³èŠ‚éª¨æ¶è¿æ¥å®šä¹‰
                # å‚è€ƒ tools/analyze_ntu_skeleton.py çš„æ ‡å‡†å®šä¹‰
                skeleton_connections = [
                    # èº¯å¹²å’Œå¤´éƒ¨
                    (3, 2),   # å¤´é¡¶ - é¢ˆéƒ¨
                    (2, 20),  # é¢ˆéƒ¨ - ä¸Šèº¯å¹²
                    (20, 1),  # ä¸Šèº¯å¹² - èº¯å¹²ä¸­
                    (1, 0),   # èº¯å¹²ä¸­ - èº¯å¹²ä¸‹
                    
                    # å·¦ä¸Šè‚¢
                    (20, 4),  # ä¸Šèº¯å¹² - å·¦è‚©
                    (4, 5),   # å·¦è‚© - å·¦è‚˜
                    (5, 6),   # å·¦è‚˜ - å·¦è…•
                    (6, 22),  # å·¦è…• - å·¦æ‰‹æŒ‡1
                    (6, 7),   # å·¦è…• - å·¦æ‰‹
                    (7, 21),  # å·¦æ‰‹ - å·¦æ‰‹æŒ‡2
                    
                    # å³ä¸Šè‚¢
                    (20, 8),  # ä¸Šèº¯å¹² - å³è‚©
                    (8, 9),   # å³è‚© - å³è‚˜
                    (9, 10),  # å³è‚˜ - å³è…•
                    (10, 24), # å³è…• - å³æ‰‹æŒ‡1
                    (10, 11), # å³è…• - å³æ‰‹
                    (11, 23), # å³æ‰‹ - å³æ‰‹æŒ‡2
                    
                    # å·¦ä¸‹è‚¢
                    (0, 12),  # èº¯å¹²ä¸‹ - å·¦é«‹
                    (12, 13), # å·¦é«‹ - å·¦è†
                    (13, 14), # å·¦è† - å·¦è¸
                    (14, 15), # å·¦è¸ - å·¦è„š
                    
                    # å³ä¸‹è‚¢
                    (0, 16),  # èº¯å¹²ä¸‹ - å³é«‹
                    (16, 17), # å³é«‹ - å³è†
                    (17, 18), # å³è† - å³è¸
                    (18, 19), # å³è¸ - å³è„š
                ]
                skeleton_type = "NTU RGB+D (25 joints)"
            else:
                print(f"âš ï¸ æœªçŸ¥éª¨æ¶ç±»å‹: {num_joints} å…³èŠ‚ï¼Œä½¿ç”¨ç®€å•è¿æ¥")
                # åˆ›å»ºç®€å•çš„é¡ºåºè¿æ¥
                skeleton_connections = [(i, i+1) for i in range(num_joints-1)]
                skeleton_type = f"Unknown ({num_joints} joints)"
            
            # è®¡ç®—æ‰€æœ‰å¸§çš„æ•°æ®è¾¹ç•Œ
            all_joints = np.vstack(skeleton_frames)
            x_min, x_max = all_joints[:, 0].min(), all_joints[:, 0].max()
            y_min, y_max = all_joints[:, 1].min(), all_joints[:, 1].max()
            z_min, z_max = all_joints[:, 2].min(), all_joints[:, 2].max()
            
            # è®¡ç®—ç»Ÿä¸€èŒƒå›´
            x_range = x_max - x_min
            y_range = y_max - y_min
            z_range = z_max - z_min
            max_range = max(x_range, y_range, z_range)
            margin = max_range * 0.2
            
            x_center = (x_min + x_max) / 2
            y_center = (y_min + y_max) / 2
            z_center = (z_min + z_max) / 2
            half_range = max_range / 2 + margin
            
            # åˆ›å»ºå›¾å½¢
            fig = plt.figure(figsize=(10, 8))
            fig.suptitle(f'{title} - {skeleton_type}', fontsize=14, fontweight='bold')
            ax = fig.add_subplot(111, projection='3d')
            
            # è®¾ç½®è½´å±æ€§
            ax.set_xlabel('X', fontsize=10)
            ax.set_ylabel('Y', fontsize=10)
            ax.set_zlabel('Z', fontsize=10)
            ax.set_xlim([x_center - half_range, x_center + half_range])
            ax.set_ylim([y_center - half_range, y_center + half_range])
            ax.set_zlim([z_center - half_range, z_center + half_range])
            ax.view_init(elev=20, azim=45)
            
            # æ·»åŠ å¸§ä¿¡æ¯æ–‡æœ¬
            frame_text = fig.text(0.5, 0.02, '', ha='center', fontsize=10, fontweight='bold')
            
            def animate(frame_idx):
                """åŠ¨ç”»æ›´æ–°å‡½æ•°"""
                ax.clear()
                
                joints = skeleton_frames[frame_idx]
                
                # ç¿»è½¬Zè½´è®©éª¨æ¶æ­£ç«‹æ˜¾ç¤º (æ•°æ®ä¸­å¤´éƒ¨Zå€¼ < è„šéƒ¨Zå€¼ï¼Œæ˜¯å€’ç«‹çš„)
                # å‚è€ƒ vis_gif_skeleton_extractor.py çš„å¤„ç†æ–¹å¼
                joints_display = joints.copy()
                joints_display[:, 2] = -joints_display[:, 2]  # ç¿»è½¬Zè½´
                
                # ç»˜åˆ¶å…³èŠ‚ç‚¹
                ax.scatter(joints_display[:, 0], joints_display[:, 1], joints_display[:, 2],
                          c='blue', s=60, alpha=0.8, edgecolors='black', linewidths=0.5)
                
                # ç»˜åˆ¶éª¨æ¶è¿æ¥çº¿
                for connection in skeleton_connections:
                    if connection[0] < len(joints_display) and connection[1] < len(joints_display):
                        joint1 = joints_display[connection[0]]
                        joint2 = joints_display[connection[1]]
                        ax.plot([joint1[0], joint2[0]],
                               [joint1[1], joint2[1]],
                               [joint1[2], joint2[2]],
                               color='blue', alpha=0.7, linewidth=2)
                
                # é‡æ–°è®¾ç½®è½´å±æ€§
                ax.set_xlabel('X', fontsize=10)
                ax.set_ylabel('Y', fontsize=10)
                ax.set_zlabel('Z (Up)', fontsize=10)
                ax.set_xlim([x_center - half_range, x_center + half_range])
                ax.set_ylim([y_center - half_range, y_center + half_range])
                # Zè½´èŒƒå›´ä¹Ÿéœ€è¦ç¿»è½¬
                ax.set_zlim([-(z_center + half_range), -(z_center - half_range)])
                ax.view_init(elev=20, azim=45)
                
                # æ›´æ–°å¸§ä¿¡æ¯
                frame_text.set_text(f'{frame_labels[frame_idx]} | Frame {frame_idx+1}/{len(skeleton_frames)}')
                
                return []
            
            # åˆ›å»ºåŠ¨ç”»
            anim = animation.FuncAnimation(
                fig, animate, frames=len(skeleton_frames),
                interval=int(1000/fps), blit=False, repeat=True
            )
            
            # ä¿å­˜GIF
            try:
                anim.save(output_path, writer='pillow', fps=fps, dpi=80)
                plt.close(fig)
                return True
            except Exception as e:
                print(f"âŒ ä¿å­˜GIFå¤±è´¥: {e}")
                plt.close(fig)
                return False
                
        except Exception as e:
            print(f"âŒ åˆ›å»ºGIFåŠ¨ç”»å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            return False
    
    # ==================== æ ‡æ³¨åŠŸèƒ½ ====================

    
    def _detailed_part_annotation_cli(self, sample):
        """CLIè¯¦ç»†åˆ†éƒ¨ä½æ ‡æ³¨"""
        print(f"\nğŸ” è¯¦ç»†åˆ†éƒ¨ä½æ ‡æ³¨æ¨¡å¼")
        print("=" * 50)
        
        annotations = {}
        tokens = sample.get('tokens', [])
        
        if len(tokens) != 5:
            print(f"âŒ Tokenæ•°é‡å¼‚å¸¸: {len(tokens)}, æœŸæœ›5ä¸ª")
            return False
        
        part_names = ['head_spine', 'left_arm', 'right_arm', 'left_leg', 'right_leg']
        part_display = ['å¤´é¢ˆ', 'å·¦è‡‚', 'å³è‡‚', 'å·¦è…¿', 'å³è…¿']
        
        for i, (part, display, token) in enumerate(zip(part_names, part_display, tokens)):
            print(f"\nğŸ¦´ æ ‡æ³¨ {display} (Token: {token}) - [{i+1}/5]")
            print("-" * 30)
            
            # ä½¿ç”¨ mars_action_templates çš„åŠ¨ä½œé€‰é¡¹
            common_actions = self.action_templates[part]
            
            print("å¸¸è§åŠ¨ä½œ:")
            for j, action in enumerate(common_actions, 1):
                print(f"  {j:2d}. {action}")
            
            while True:
                print(f"\nè¾“å…¥é€‰é¡¹:")
                print(f"1-{len(common_actions)}. é€‰æ‹©é¢„è®¾åŠ¨ä½œ")
                print("c. è‡ªå®šä¹‰æè¿°")
                print("s. è·³è¿‡æ­¤éƒ¨ä½")
                
                choice = safe_input(f"è¯·é€‰æ‹©: ", "s").lower()
                
                if choice == 's' or choice == "":
                    print(f"â­ï¸ è·³è¿‡ {display}")
                    break
                elif choice == 'c':
                    custom_desc = safe_input(f"è¯·æè¿°{display}çš„åŠ¨ä½œ: ", "æ­£å¸¸çŠ¶æ€")
                    if custom_desc and custom_desc != "æ­£å¸¸çŠ¶æ€":
                        annotations[part] = {
                            'token': token,
                            'description': custom_desc,
                            'timestamp': datetime.now().isoformat()
                        }
                        print(f"âœ… {display}: {custom_desc}")
                        break
                    else:
                        # ä½¿ç”¨é»˜è®¤æè¿°
                        annotations[part] = {
                            'token': token,
                            'description': "æ­£å¸¸çŠ¶æ€",
                            'timestamp': datetime.now().isoformat()
                        }
                        print(f"âœ… {display}: æ­£å¸¸çŠ¶æ€ (é»˜è®¤)")
                        break
                else:
                    try:
                        action_idx = int(choice) - 1
                        if 0 <= action_idx < len(common_actions):
                            selected_action = common_actions[action_idx]
                            annotations[part] = {
                                'token': token,
                                'description': selected_action,
                                'timestamp': datetime.now().isoformat()
                            }
                            print(f"âœ… {display}: {selected_action}")
                            break
                        else:
                            print(f"âŒ è¯·è¾“å…¥1-{len(common_actions)}ä¹‹é—´çš„æ•°å­—")
                    except ValueError:
                        print("âŒ æ— æ•ˆè¾“å…¥ï¼Œè¯·é‡è¯•")
        
        # æ•´ä½“åŠ¨ä½œæè¿°
        print(f"\nğŸ­ æ•´ä½“åŠ¨ä½œæè¿°:")
        if 'ground_truth_action' in sample:
            print(f"ğŸ’¡ å‚è€ƒGT: {sample['ground_truth_action']}")
        
        overall_action = safe_input("è¯·æè¿°æ•´ä½“åŠ¨ä½œ (å¯å‚è€ƒGT): ", sample.get('ground_truth_action', 'æœªæè¿°'))
        if not overall_action:
            overall_action = sample.get('ground_truth_action', 'æœªæè¿°')
        
        # ä¿å­˜æ ‡æ³¨
        sample['annotations'] = annotations
        sample['overall_action'] = overall_action
        sample['annotation_time'] = datetime.now().isoformat()
        
        # è‡ªåŠ¨ä¿å­˜åˆ°ä¼šè¯æ–‡ä»¶
        self._auto_save_sample(sample)
        
        print(f"\nâœ… è¯¦ç»†æ ‡æ³¨å®Œæˆï¼Œå…±æ ‡æ³¨ {len(annotations)} ä¸ªéƒ¨ä½")
        print(f"ğŸ’¾ æ ‡æ³¨ç»“æœå·²è‡ªåŠ¨ä¿å­˜")
        return True
    
    def _quick_overall_annotation_cli(self, sample):
        """CLIå¿«é€Ÿæ•´ä½“æ ‡æ³¨ï¼ˆä»…æ ‡æ³¨æ•´ä½“åŠ¨ä½œï¼Œéƒ¨ä½ä½¿ç”¨é»˜è®¤æè¿°ï¼‰"""
        print(f"\nâš¡ å¿«é€Ÿæ•´ä½“æ ‡æ³¨æ¨¡å¼")
        print("=" * 30)
        
        # æ˜¾ç¤ºå‚è€ƒä¿¡æ¯
        if 'ground_truth_action' in sample:
            print(f"ğŸ¯ Ground Truth: {sample['ground_truth_action']}")
        
        default_action = sample.get('ground_truth_action', 'æœªæè¿°')
        overall_action = safe_input("è¯·æè¿°æ•´ä½“åŠ¨ä½œ: ", default_action)
        if not overall_action:
            overall_action = default_action
        
        # è‡ªåŠ¨ä¸ºå„éƒ¨ä½ç”Ÿæˆç®€å•é»˜è®¤æè¿°
        tokens = sample.get('tokens', [])
        if len(tokens) != 5:
            print(f"âŒ Tokenæ•°é‡å¼‚å¸¸")
            return False
        
        annotations = {}
        part_names = ['head_spine', 'left_arm', 'right_arm', 'left_leg', 'right_leg']
        part_defaults = ['æ­£å¸¸å§¿æ€', 'è‡ªç„¶çŠ¶æ€', 'è‡ªç„¶çŠ¶æ€', 'ç«™ç«‹æ”¯æ’‘', 'ç«™ç«‹æ”¯æ’‘']
        
        for part, token, default_desc in zip(part_names, tokens, part_defaults):
            annotations[part] = {
                'token': token,
                'description': default_desc,
                'timestamp': datetime.now().isoformat(),
                'auto_generated': True
            }
        
        # ä¿å­˜æ ‡æ³¨
        sample['annotations'] = annotations
        sample['overall_action'] = overall_action
        sample['annotation_time'] = datetime.now().isoformat()
        
        # è‡ªåŠ¨ä¿å­˜åˆ°ä¼šè¯æ–‡ä»¶
        self._auto_save_sample(sample)
        
        print(f"âœ… å¿«é€Ÿæ ‡æ³¨å®Œæˆ")
        print(f"ğŸ’¾ æ ‡æ³¨ç»“æœå·²è‡ªåŠ¨ä¿å­˜")
        return True
    
    def _auto_save_sample(self, sample):
        """è‡ªåŠ¨ä¿å­˜å•ä¸ªæ ·æœ¬æ ‡æ³¨"""
        try:
            # ä¼šè¯æ–‡ä»¶è·¯å¾„
            session_file = os.path.join(self.save_dir, "sessions", f"session_{self.session_id}.json")
            
            # åŠ è½½ç°æœ‰ä¼šè¯æ•°æ®
            if os.path.exists(session_file):
                with open(session_file, 'r', encoding='utf-8') as f:
                    session_data = json.load(f)
            else:
                session_data = {
                    'session_id': self.session_id,
                    'created_time': datetime.now().isoformat(),
                    'samples': {},
                    'statistics': {
                        'total_samples': 0,
                        'annotated_samples': 0,
                        'annotation_modes': {}
                    }
                }
            
            # æ·»åŠ /æ›´æ–°æ ·æœ¬æ•°æ®
            sample_key = f"sample_{sample.get('id', len(session_data['samples']))}"
            
            # åˆ›å»ºæ¸…ç†åçš„æ ·æœ¬æ•°æ®ï¼ˆç§»é™¤å¤§æ•°æ®å¯¹è±¡ï¼‰
            clean_sample = {}
            for key, value in sample.items():
                # æ’é™¤numpyæ•°ç»„æ•°æ®å­—æ®µ
                if key not in ['point_cloud_data', 'radar_data', 'skeleton_data', 'extracted', 'reconstructed']:
                    # è½¬æ¢numpyç±»å‹ä¸ºPythonåŸç”Ÿç±»å‹
                    if isinstance(value, np.ndarray):
                        clean_sample[f"{key}_summary"] = {
                            'shape': list(value.shape),
                            'dtype': str(value.dtype),
                            'size': int(value.size)
                        }
                    elif isinstance(value, (np.integer, np.floating)):
                        clean_sample[key] = int(value) if isinstance(value, np.integer) else float(value)
                    elif isinstance(value, list):
                        # è½¬æ¢åˆ—è¡¨ä¸­çš„numpyç±»å‹
                        clean_sample[key] = [int(x) if isinstance(x, np.integer) else 
                                            float(x) if isinstance(x, np.floating) else x 
                                            for x in value]
                    else:
                        clean_sample[key] = value
                else:
                    # åªä¿å­˜æ•°æ®æ¦‚è¦ä¿¡æ¯
                    if isinstance(value, np.ndarray):
                        clean_sample[f"{key}_summary"] = {
                            'shape': list(value.shape),
                            'dtype': str(value.dtype),
                            'size': int(value.size)
                        }
            
            session_data['samples'][sample_key] = clean_sample
            session_data['last_updated'] = datetime.now().isoformat()
            
            # æ›´æ–°ç»Ÿè®¡ä¿¡æ¯
            session_data['statistics']['total_samples'] = len(session_data['samples'])
            session_data['statistics']['annotated_samples'] = len([s for s in session_data['samples'].values() if s.get('annotated', False)])
            
            # ä¿å­˜ä¼šè¯æ–‡ä»¶
            with open(session_file, 'w', encoding='utf-8') as f:
                json.dump(session_data, f, ensure_ascii=False, indent=2)
                
            return True
        except Exception as e:
            print(f"âš ï¸ è‡ªåŠ¨ä¿å­˜å¤±è´¥: {e}")
            return False
    
    def export_annotations(self):
        """å¯¼å‡ºæ ‡æ³¨ç»“æœ"""
        if not any(s.get('annotated', False) for s in self.samples_to_annotate):
            print("âŒ æš‚æ— å·²æ ‡æ³¨çš„æ ·æœ¬æ•°æ®")
            return
        
        print("\nğŸ“¤ å¯¼å‡ºæ ‡æ³¨ç»“æœ")
        print("=" * 40)
        
        # é€‰æ‹©å¯¼å‡ºæ ¼å¼
        print("é€‰æ‹©å¯¼å‡ºæ ¼å¼:")
        print("1. JSONæ ¼å¼ (å®Œæ•´æ•°æ®)")
        print("2. CSVæ ¼å¼ (è¡¨æ ¼æ•°æ®)")
        print("3. ç æœ¬æ˜ å°„è¡¨ (Token->Action)")
        print("4. å…¨éƒ¨æ ¼å¼")
        
        choice = safe_input("é€‰æ‹©å¯¼å‡ºæ ¼å¼ (1-4): ", "1")
        
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        export_base_name = f"annotations_export_{timestamp}"
        
        try:
            if choice in ['1', '4']:
                self._export_json(export_base_name)
            if choice in ['2', '4']:
                self._export_csv(export_base_name)
            if choice in ['3', '4']:
                self._export_mapping_table(export_base_name)
            
            print(f"âœ… å¯¼å‡ºå®Œæˆï¼Œæ–‡ä»¶ä¿å­˜åœ¨: {self.save_dir}/exports/")
            
        except Exception as e:
            print(f"âŒ å¯¼å‡ºå¤±è´¥: {e}")
    
    def _export_json(self, base_name):
        """å¯¼å‡ºJSONæ ¼å¼"""
        export_data = {
            'export_info': {
                'timestamp': datetime.now().isoformat(),
                'session_id': self.session_id,
                'total_samples': len(self.samples_to_annotate),
                'annotated_samples': len([s for s in self.samples_to_annotate if s.get('annotated', False)])
            },
            'samples': []
        }
        
        for sample in self.samples_to_annotate:
            if sample.get('annotated', False):
                # æ¸…ç†æ ·æœ¬æ•°æ®
                clean_sample = {}
                for key, value in sample.items():
                    if key not in ['point_cloud_data', 'radar_data', 'skeleton_data']:
                        clean_sample[key] = value
                export_data['samples'].append(clean_sample)
        
        json_file = os.path.join(self.save_dir, "exports", f"{base_name}.json")
        with open(json_file, 'w', encoding='utf-8') as f:
            json.dump(export_data, f, ensure_ascii=False, indent=2)
        
        print(f"ğŸ“„ JSONæ–‡ä»¶å·²å¯¼å‡º: {json_file}")
    
    def _export_csv(self, base_name):
        """å¯¼å‡ºCSVæ ¼å¼"""
        import csv
        
        csv_file = os.path.join(self.save_dir, "exports", f"{base_name}.csv")
        
        with open(csv_file, 'w', newline='', encoding='utf-8') as f:
            writer = csv.writer(f)
            
            # å†™å…¥è¡¨å¤´
            headers = [
                'sample_id', 'filename', 'source', 'ground_truth_action', 'overall_action',
                'head_spine_token', 'head_spine_desc',
                'left_arm_token', 'left_arm_desc',
                'right_arm_token', 'right_arm_desc', 
                'left_leg_token', 'left_leg_desc',
                'right_leg_token', 'right_leg_desc',
                'annotation_time'
            ]
            writer.writerow(headers)
            
            # å†™å…¥æ•°æ®è¡Œ
            for sample in self.samples_to_annotate:
                if sample.get('annotated', False):
                    annotations = sample.get('annotations', {})
                    tokens = sample.get('tokens', [0, 0, 0, 0, 0])
                    
                    row = [
                        sample.get('id', ''),
                        sample.get('filename', ''),
                        sample.get('source', ''),
                        sample.get('ground_truth_action', ''),
                        sample.get('overall_action', ''),
                    ]
                    
                    # æ·»åŠ å„éƒ¨ä½çš„tokenå’Œæè¿°
                    part_names = ['head_spine', 'left_arm', 'right_arm', 'left_leg', 'right_leg']
                    for i, part in enumerate(part_names):
                        token = tokens[i] if i < len(tokens) else 0
                        desc = annotations.get(part, {}).get('description', '')
                        row.extend([token, desc])
                    
                    row.append(sample.get('annotation_time', ''))
                    writer.writerow(row)
        
        print(f"ğŸ“Š CSVæ–‡ä»¶å·²å¯¼å‡º: {csv_file}")
    
    def _export_mapping_table(self, base_name):
        """å¯¼å‡ºç æœ¬æ˜ å°„è¡¨"""
        # æ„å»ºTokenåˆ°åŠ¨ä½œçš„æ˜ å°„
        token_mapping = {}
        
        for sample in self.samples_to_annotate:
            if sample.get('annotated', False):
                annotations = sample.get('annotations', {})
                tokens = sample.get('tokens', [])
                
                part_names = ['head_spine', 'left_arm', 'right_arm', 'left_leg', 'right_leg']
                for i, part in enumerate(part_names):
                    if i < len(tokens):
                        token = tokens[i]
                        desc = annotations.get(part, {}).get('description', '')
                        
                        if token not in token_mapping:
                            token_mapping[token] = {}
                        if part not in token_mapping[token]:
                            token_mapping[token][part] = []
                        
                        if desc and desc not in token_mapping[token][part]:
                            token_mapping[token][part].append(desc)
        
        # å¯¼å‡ºæ˜ å°„è¡¨
        mapping_file = os.path.join(self.save_dir, "exports", f"{base_name}_mapping.json")
        
        with open(mapping_file, 'w', encoding='utf-8') as f:
            json.dump(token_mapping, f, ensure_ascii=False, indent=2)
        
        print(f"ğŸ—‚ï¸ ç æœ¬æ˜ å°„è¡¨å·²å¯¼å‡º: {mapping_file}")
        
        # åŒæ—¶ç”Ÿæˆå¯è¯»çš„æ˜ å°„è¡¨
        readable_file = os.path.join(self.save_dir, "exports", f"{base_name}_mapping.txt")
        with open(readable_file, 'w', encoding='utf-8') as f:
            f.write("ç æœ¬-åŠ¨ä½œæ˜ å°„è¡¨\n")
            f.write("=" * 50 + "\n\n")
            
            for token in sorted(token_mapping.keys()):
                f.write(f"Token {token}:\n")
                for part, descriptions in token_mapping[token].items():
                    f.write(f"  {part}: {', '.join(descriptions)}\n")
                f.write("\n")
        
        print(f"ğŸ“– å¯è¯»æ˜ å°„è¡¨å·²å¯¼å‡º: {readable_file}")
    
    def show_progress_cli(self):
        """æ˜¾ç¤ºæ ‡æ³¨è¿›åº¦"""
        total_samples = len(self.samples_to_annotate)
        if total_samples == 0:
            print("âŒ æš‚æ— æ ·æœ¬æ•°æ®")
            return
        
        annotated_samples = len([s for s in self.samples_to_annotate if s.get('annotated', False)])
        progress_percent = (annotated_samples / total_samples) * 100 if total_samples > 0 else 0
        
        print("\nğŸ“Š æ ‡æ³¨è¿›åº¦ç»Ÿè®¡")
        print("=" * 40)
        print(f"æ€»æ ·æœ¬æ•°: {total_samples}")
        print(f"å·²æ ‡æ³¨æ•°: {annotated_samples}")
        print(f"å®Œæˆè¿›åº¦: {progress_percent:.1f}%")
        
        # è¿›åº¦æ¡
        bar_length = 30
        filled_length = int(bar_length * annotated_samples // total_samples)
        bar = 'â–ˆ' * filled_length + '-' * (bar_length - filled_length)
        print(f"è¿›åº¦æ¡: |{bar}| {progress_percent:.1f}%")
        
        # æ•°æ®æºç»Ÿè®¡
        source_stats = {}
        for sample in self.samples_to_annotate:
            source = sample.get('source', 'unknown')
            source_stats[source] = source_stats.get(source, 0) + 1
        
        print(f"\nğŸ“‹ æ•°æ®æºåˆ†å¸ƒ:")
        for source, count in source_stats.items():
            print(f"  {source}: {count} ä¸ªæ ·æœ¬")
        
        # æœ€è¿‘æ ‡æ³¨æ´»åŠ¨
        recent_annotations = [s for s in self.samples_to_annotate 
                            if s.get('annotated', False) and 'annotation_time' in s]
        recent_annotations.sort(key=lambda x: x['annotation_time'], reverse=True)
        
        if recent_annotations:
            print(f"\nğŸ• æœ€è¿‘æ ‡æ³¨æ´»åŠ¨:")
            for i, sample in enumerate(recent_annotations[:5]):
                time_str = sample['annotation_time'][:19].replace('T', ' ')
                filename = sample.get('filename', 'unknown')[:20]
                print(f"  {i+1}. {time_str} - {filename}")
    
    def load_previous_session(self):
        """åŠ è½½ä¹‹å‰çš„æ ‡æ³¨ä¼šè¯"""
        sessions_dir = os.path.join(self.save_dir, "sessions")
        if not os.path.exists(sessions_dir):
            print("âŒ æš‚æ— ä¿å­˜çš„ä¼šè¯")
            return
        
        session_files = [f for f in os.listdir(sessions_dir) if f.endswith('.json')]
        if not session_files:
            print("âŒ æš‚æ— ä¿å­˜çš„ä¼šè¯æ–‡ä»¶")
            return
        
        print(f"\nğŸ“‚ å‘ç° {len(session_files)} ä¸ªä¼šè¯æ–‡ä»¶:")
        for i, session_file in enumerate(session_files, 1):
            session_path = os.path.join(sessions_dir, session_file)
            try:
                with open(session_path, 'r', encoding='utf-8') as f:
                    session_data = json.load(f)
                created_time = session_data.get('created_time', 'æœªçŸ¥')[:19].replace('T', ' ')
                sample_count = len(session_data.get('samples', {}))
                print(f"  {i}. {session_file} - {created_time} ({sample_count} æ ·æœ¬)")
            except:
                print(f"  {i}. {session_file} - æŸåçš„æ–‡ä»¶")
        
        choice = safe_input(f"\né€‰æ‹©è¦åŠ è½½çš„ä¼šè¯ (1-{len(session_files)}, 0=å–æ¶ˆ): ", "0")
        
        try:
            choice_idx = int(choice) - 1
            if 0 <= choice_idx < len(session_files):
                session_file = session_files[choice_idx]
                session_path = os.path.join(sessions_dir, session_file)
                
                with open(session_path, 'r', encoding='utf-8') as f:
                    session_data = json.load(f)
                
                # æ¢å¤ä¼šè¯æ•°æ®
                self.session_id = session_data['session_id']
                print(f"âœ… å·²åŠ è½½ä¼šè¯: {session_file}")
                print(f"ğŸ“Š åŒ…å« {len(session_data['samples'])} ä¸ªæ ·æœ¬")
                
        except ValueError:
            print("âŒ æ— æ•ˆé€‰æ‹©")
        except Exception as e:
            print(f"âŒ åŠ è½½ä¼šè¯å¤±è´¥: {e}")
    
    def open_visualization_window(self):
        """æ‰‹åŠ¨æ‰“å¼€å¯è§†åŒ–çª—å£"""
        if not VISUALIZATION_AVAILABLE:
            print("âŒ å¯è§†åŒ–çª—å£ä¸å¯ç”¨")
            return
        
        if not self.samples_to_annotate:
            print("âŒ æš‚æ— æ ·æœ¬æ•°æ®ï¼Œè¯·å…ˆåŠ è½½æ•°æ®")
            return
        
        print("\nğŸ–¼ï¸ é€‰æ‹©è¦å¯è§†åŒ–çš„æ ·æœ¬:")
        
        # æ˜¾ç¤ºæ ·æœ¬å¹¶æ ‡è®°æ•°æ®é›†ç±»å‹
        for i, sample in enumerate(self.samples_to_annotate[:10]):  # åªæ˜¾ç¤ºå‰10ä¸ª
            filename = sample.get('filename', f'sample_{i}')
            source = sample.get('source', 'unknown')
            status = "âœ…" if sample.get('annotated', False) else "â­•"
            
            # æ•°æ®é›†ç±»å‹æ ‡è¯†
            if 'ntu' in source.lower():
                dataset_tag = "[NTU]"
            elif 'mars' in source.lower():
                dataset_tag = "[MARS]"
            else:
                dataset_tag = "[UNKNOWN]"
                
            print(f"  {i+1}. {status} {dataset_tag} {filename}")
        
        if len(self.samples_to_annotate) > 10:
            print(f"  ... å’Œå…¶ä»– {len(self.samples_to_annotate)-10} ä¸ªæ ·æœ¬")
        
        choice = safe_input(f"é€‰æ‹©æ ·æœ¬ (1-{min(10, len(self.samples_to_annotate))}): ", "1")
        
        try:
            idx = int(choice) - 1
            if 0 <= idx < len(self.samples_to_annotate):
                sample = self.samples_to_annotate[idx]
                
                # æ˜¾ç¤ºå°†è¦å¯è§†åŒ–çš„æ ·æœ¬ä¿¡æ¯
                source = sample.get('source', 'unknown')
                print(f"\nğŸ” å³å°†å¯è§†åŒ–: {sample.get('filename', 'unknown')}")
                print(f"ğŸ“Š æ•°æ®é›†ç±»å‹: {source}")
                
                if 'skeleton_data' in sample:
                    joints_count = sample['skeleton_data'].shape[0]
                    print(f"ğŸ¦´ éª¨æ¶å…³èŠ‚æ•°: {joints_count}")
                    if joints_count == 25:
                        print("ğŸ’¡ è¿™æ˜¯NTU RGB+D 25å…³èŠ‚éª¨æ¶æ•°æ®")
                    elif joints_count == 19:
                        print("ğŸ’¡ è¿™æ˜¯MARS 19å…³èŠ‚éª¨æ¶æ•°æ®") 
                elif 'point_cloud_data' in sample:
                    points_count = sample['point_cloud_data'].shape[0]
                    print(f"â˜ï¸ ç‚¹äº‘æ•°æ®ç‚¹æ•°: {points_count}")
                elif 'radar_data' in sample:
                    print("ğŸ“¡ è¿™æ˜¯é›·è¾¾ç‰¹å¾æ•°æ®")
                
                success = show_sample_visualization(sample, sample)
                if success:
                    print("âœ… å¯è§†åŒ–çª—å£å·²æ‰“å¼€")
                    print("ğŸ’¡ æ‚¨å¯ä»¥åœ¨å¯è§†åŒ–çª—å£ä¸­æŸ¥çœ‹3Déª¨æ¶æ•°æ®")
                    print("ğŸ’¡ å…³é—­å¯è§†åŒ–çª—å£åä¼šè‡ªåŠ¨è¿”å›ä¸»èœå•")
                    
                    # ç­‰å¾…ç”¨æˆ·ç¡®è®¤æˆ–å¯è§†åŒ–çª—å£å…³é—­
                    input("\nğŸ‘€ æŸ¥çœ‹å®Œæ¯•åè¯·æŒ‰å›è½¦é”®è¿”å›ä¸»èœå•...")
                else:
                    print("âŒ å¯è§†åŒ–çª—å£æ‰“å¼€å¤±è´¥")
            else:
                print("âŒ æ— æ•ˆçš„æ ·æœ¬ç¼–å·")
        except ValueError:
            print("âŒ è¯·è¾“å…¥æœ‰æ•ˆæ•°å­—")
        
    def sequence_frame_annotation(self):
        """åºåˆ—å¸§æ‰¹æ³¨åŠŸèƒ½ - å¯è§†åŒ–ç›¸é‚»å‡ å¸§åŠ¨ä½œï¼Œæ‰¹é‡æ ‡æ³¨æ•´ä¸ªåºåˆ—"""
        if not self.samples_to_annotate:
            print("âŒ æš‚æ— æ ·æœ¬æ•°æ®")
            return
            
        print("\nğŸ¬ åºåˆ—å¸§æ‰¹æ³¨æ¨¡å¼")
        print("=" * 60)
        print("ğŸ’¡ æ­¤æ¨¡å¼é€‚åˆMARSæ•°æ®é›†ï¼šå¯è§†åŒ–ç›¸é‚»å¸§ï¼Œæ‰¹é‡æ ‡æ³¨æ•´ä¸ªåŠ¨ä½œåºåˆ—")
        
        # é…ç½®åºåˆ—å‚æ•°
        try:
            sequence_length = int(safe_input("è¯·è¾“å…¥åºåˆ—é•¿åº¦ (å»ºè®®3-8å¸§): ", "5"))
            sequence_length = max(2, min(10, sequence_length))  # é™åˆ¶åœ¨2-10å¸§
            
            step_size = int(safe_input("è¯·è¾“å…¥æ­¥é•¿ (è·³è¿‡å¤šå°‘å¸§): ", "1"))
            step_size = max(1, step_size)
            
        except ValueError:
            sequence_length = 5
            step_size = 1
            print(f"ä½¿ç”¨é»˜è®¤å‚æ•°: åºåˆ—é•¿åº¦={sequence_length}, æ­¥é•¿={step_size}")
        
        print(f"\nğŸ“‹ åºåˆ—é…ç½®: {sequence_length}å¸§/åºåˆ—, æ­¥é•¿={step_size}")
        
        # æŒ‰åºåˆ—å¤„ç†æ ·æœ¬
        sequence_count = 0
        total_sequences = (len(self.samples_to_annotate) - sequence_length + 1) // step_size
        
        i = 0
        while i <= len(self.samples_to_annotate) - sequence_length:
            sequence_count += 1
            
            # æå–å½“å‰åºåˆ—
            current_sequence = self.samples_to_annotate[i:i+sequence_length]
            
            print(f"\n" + "="*80)
            print(f"ğŸ¬ åºåˆ— {sequence_count}/{total_sequences}")
            print(f"ğŸ“ å¸§èŒƒå›´: {i+1} - {i+sequence_length}")
            
            # æ˜¾ç¤ºåºåˆ—ä¸­æ¯ä¸€å¸§çš„ä¿¡æ¯
            print("åºåˆ—å¸§ä¿¡æ¯:")
            sequence_tokens = []
            for j, frame in enumerate(current_sequence):
                status = "âœ“å·²æ ‡æ³¨" if frame.get('annotated', False) else "â—‹æœªæ ‡æ³¨"
                print(f"  å¸§{i+j+1}: {frame['tokens']} {status}")
                sequence_tokens.append(tuple(frame['tokens']))
            
            # æ£€æŸ¥åºåˆ—æ˜¯å¦å·²å…¨éƒ¨æ ‡æ³¨
            already_annotated = all(frame.get('annotated', False) for frame in current_sequence)
            if already_annotated:
                print("âœ“ æ­¤åºåˆ—å·²å®Œå…¨æ ‡æ³¨ï¼Œè·³è¿‡")
                i += step_size
                continue
            
            # æ˜¾ç¤ºå¯è§†åŒ–é€‰é¡¹
            print(f"\nğŸ¯ åºåˆ—æ‰¹æ³¨é€‰é¡¹:")
            print("1. å¯è§†åŒ–æ­¤åºåˆ—")
            print("2. ç›´æ¥æ ‡æ³¨æ­¤åºåˆ—")
            print("3. è·³è¿‡æ­¤åºåˆ—")
            print("4. é€€å‡ºåºåˆ—æ ‡æ³¨")
            
            choice = safe_input("è¯·é€‰æ‹©æ“ä½œ: ", "1").strip()
            
            if choice == '1':
                # å¯è§†åŒ–åºåˆ—
                self._visualize_sequence(current_sequence, i)
                
                # å¯è§†åŒ–åè¯¢é—®æ˜¯å¦æ ‡æ³¨
                annotate_choice = safe_input("æ˜¯å¦æ ‡æ³¨æ­¤åºåˆ—? (y/n): ", "n").strip().lower()
                if annotate_choice in ['y', 'yes']:
                    self._annotate_sequence(current_sequence, i)
                    
            elif choice == '2':
                # ç›´æ¥æ ‡æ³¨åºåˆ—
                self._annotate_sequence(current_sequence, i)
                
            elif choice == '3':
                # è·³è¿‡åºåˆ—
                print("â­ï¸ è·³è¿‡åºåˆ—")
                
            elif choice == '4':
                # é€€å‡º
                print("ğŸšª é€€å‡ºåºåˆ—æ ‡æ³¨æ¨¡å¼")
                break
            else:
                print("âŒ æ— æ•ˆé€‰æ‹©ï¼Œè·³è¿‡æ­¤åºåˆ—")
            
            i += step_size
            
        print(f"\nâœ… åºåˆ—æ ‡æ³¨æ¨¡å¼ç»“æŸï¼Œå…±å¤„ç† {sequence_count} ä¸ªåºåˆ—")
        
    def _visualize_sequence(self, sequence, start_index):
        """å¯è§†åŒ–åŠ¨ä½œåºåˆ—"""
        print(f"\nğŸ–¼ï¸ å¯è§†åŒ–åºåˆ— (å¸§ {start_index+1}-{start_index+len(sequence)})")
        
        # å°è¯•ä½¿ç”¨å¯è§†åŒ–çª—å£
        if VISUALIZATION_AVAILABLE:
            try:
                # å°è¯•åˆå§‹åŒ–å¯è§†åŒ–çª—å£ï¼ˆå¦‚æœå°šæœªåˆå§‹åŒ–ï¼‰
                if not hasattr(self, 'visualization_window') or not self.visualization_window:
                    print("ğŸ”§ åˆå§‹åŒ–å¯è§†åŒ–çª—å£...")
                    self._init_visualization_window()
                
                # æ£€æŸ¥æ˜¯å¦æœ‰å¯ç”¨çš„å¯è§†åŒ–çª—å£
                if hasattr(self, 'visualization_window') and self.visualization_window:
                    # ä¾æ¬¡æ˜¾ç¤ºåºåˆ—ä¸­çš„æ¯ä¸€å¸§
                    for j, frame in enumerate(sequence):
                        if 'skeleton' in frame:
                            print(f"æ˜¾ç¤ºç¬¬ {j+1} å¸§...")
                            self.visualization_window.update_skeleton(frame['skeleton'])
                            self.visualization_window.update_display()
                            
                            if j < len(sequence) - 1:  # ä¸æ˜¯æœ€åä¸€å¸§
                                input("æŒ‰Enteré”®æŸ¥çœ‹ä¸‹ä¸€å¸§...")
                            
                    print("åºåˆ—å¯è§†åŒ–å®Œæˆ")
                    return
                else:
                    print("âš ï¸ å¯è§†åŒ–çª—å£åˆå§‹åŒ–å¤±è´¥ï¼Œä½¿ç”¨æ–‡æœ¬æ˜¾ç¤º")
                
            except Exception as e:
                print(f"âš ï¸ å¯è§†åŒ–å¤±è´¥: {e}ï¼Œä½¿ç”¨æ–‡æœ¬æ˜¾ç¤º")
        else:
            print("âš ï¸ å¯è§†åŒ–æ¨¡å—ä¸å¯ç”¨ï¼Œä½¿ç”¨æ–‡æœ¬æ˜¾ç¤º")
        
        # å¤‡é€‰æ–¹æ¡ˆï¼šæ˜¾ç¤ºè¯¦ç»†æ–‡æœ¬ä¿¡æ¯
        print("ğŸ“Š åºåˆ—å¸§è¯¦ç»†ä¿¡æ¯:")
        print("=" * 60)
        
        for j, frame in enumerate(sequence):
            print(f"ğŸ“‹ å¸§ {start_index+j+1}:")
            
            # Tokenä¿¡æ¯è§£æ
            tokens = frame['tokens']
            print(f"  ğŸ¯ Tokenåºåˆ—: {tokens}")
            print(f"     å¤´éƒ¨è„Šæ¤: {tokens[0]}")
            print(f"     å·¦è‡‚: {tokens[1]}")  
            print(f"     å³è‡‚: {tokens[2]}")
            print(f"     å·¦è…¿: {tokens[3]}")
            print(f"     å³è…¿: {tokens[4]}")
            
            # éª¨æ¶ä¿¡æ¯
            if 'skeleton' in frame:
                skeleton = frame['skeleton']
                if isinstance(skeleton, np.ndarray):
                    joint_count = skeleton.shape[0]
                    dataset_type = "MARS" if joint_count == 19 else ("NTU" if joint_count == 25 else "æœªçŸ¥")
                    
                    # è®¡ç®—éª¨æ¶ç»Ÿè®¡
                    if len(skeleton) > 0:
                        avg_pos = np.mean(skeleton, axis=0)
                        min_pos = np.min(skeleton, axis=0)
                        max_pos = np.max(skeleton, axis=0)
                        range_pos = max_pos - min_pos
                        
                        print(f"  ğŸ¦´ éª¨æ¶ä¿¡æ¯: {joint_count}å…³èŠ‚ç‚¹ ({dataset_type}æ ¼å¼)")
                        print(f"     ä¸­å¿ƒä½ç½®: [{avg_pos[0]:.2f}, {avg_pos[1]:.2f}, {avg_pos[2]:.2f}]")
                        print(f"     èŒƒå›´: X={range_pos[0]:.2f}, Y={range_pos[1]:.2f}, Z={range_pos[2]:.2f}")
                        
                        # æ˜¾ç¤ºä¸»è¦å…³èŠ‚ä½ç½®ï¼ˆå¤´éƒ¨å’Œéª¨ç›†ï¼‰
                        if joint_count == 19:  # MARSæ ¼å¼
                            head_pos = skeleton[2]  # head
                            spine_pos = skeleton[0]  # spinebase
                            print(f"     å¤´éƒ¨ä½ç½®: [{head_pos[0]:.2f}, {head_pos[1]:.2f}, {head_pos[2]:.2f}]")
                            print(f"     éª¨ç›†ä½ç½®: [{spine_pos[0]:.2f}, {spine_pos[1]:.2f}, {spine_pos[2]:.2f}]")
                        elif joint_count == 25:  # NTUæ ¼å¼
                            head_pos = skeleton[3]  # head
                            spine_pos = skeleton[0]  # spinebase
                            print(f"     å¤´éƒ¨ä½ç½®: [{head_pos[0]:.2f}, {head_pos[1]:.2f}, {head_pos[2]:.2f}]")
                            print(f"     éª¨ç›†ä½ç½®: [{spine_pos[0]:.2f}, {spine_pos[1]:.2f}, {spine_pos[2]:.2f}]")
            
            print(f"  ğŸ“ æ–‡ä»¶: {frame.get('filename', 'æœªçŸ¥')}")
            
            if j < len(sequence) - 1:
                print("  " + "-" * 50)
        
        print("=" * 60)
        print(f"ğŸ’¡ æç¤º: è¿™æ˜¯ {len(sequence)} å¸§çš„åŠ¨ä½œåºåˆ—")
        print(f"   Tokenå˜åŒ–å¯ä»¥åæ˜ åŠ¨ä½œçš„è¿ç»­æ€§")
        print(f"   å»ºè®®åŸºäºæ•´ä½“åŠ¨ä½œæ¨¡å¼è¿›è¡Œæ ‡æ³¨")
    
    def _annotate_sequence(self, sequence, start_index):
        """æ ‡æ³¨æ•´ä¸ªåŠ¨ä½œåºåˆ—"""
        print(f"\nğŸ·ï¸ æ ‡æ³¨åºåˆ— (å¸§ {start_index+1}-{start_index+len(sequence)})")
        
        # è·å–åºåˆ—çš„ç»Ÿä¸€æ ‡æ³¨
        print("ä¸ºæ•´ä¸ªåºåˆ—é€‰æ‹©åŠ¨ä½œæè¿°:")
        
        # åˆ†éƒ¨ä½æ ‡æ³¨
        sequence_annotation = {
            'part_annotations': {},
            'global_action': '',
            'sequence_info': {
                'start_frame': start_index + 1,
                'end_frame': start_index + len(sequence),
                'length': len(sequence)
            }
        }
        
        for part_name, display_name in zip(self.part_names, self.part_display_names):
            print(f"\n{display_name} åŠ¨ä½œ:")
            
            # æ˜¾ç¤ºåŠ¨ä½œé€‰é¡¹
            templates = self.action_templates.get(part_name, ['æ­£å¸¸å§¿æ€'])
            for i, template in enumerate(templates, 1):
                print(f"  {i}. {template}")
            
            # è·å–ç”¨æˆ·é€‰æ‹©
            try:
                choice_input = safe_input(f"é€‰æ‹© {display_name} åŠ¨ä½œ (1-{len(templates)}) æˆ–è¾“å…¥è‡ªå®šä¹‰: ", "1")
                
                if choice_input.isdigit():
                    choice_idx = int(choice_input) - 1
                    if 0 <= choice_idx < len(templates):
                        selected_action = templates[choice_idx]
                    else:
                        selected_action = templates[0]  # é»˜è®¤é€‰æ‹©ç¬¬ä¸€ä¸ª
                else:
                    # è‡ªå®šä¹‰è¾“å…¥
                    selected_action = choice_input.strip() or templates[0]
                    
                sequence_annotation['part_annotations'][part_name] = selected_action
                print(f"âœ“ {display_name}: {selected_action}")
                
            except (ValueError, IndexError):
                # ä½¿ç”¨é»˜è®¤
                default_action = templates[0]
                sequence_annotation['part_annotations'][part_name] = default_action
                print(f"âœ“ {display_name}: {default_action} (é»˜è®¤)")
        
        # æ•´ä½“åŠ¨ä½œæè¿°
        global_action = safe_input("\næ•´ä½“åŠ¨ä½œæè¿° (å¯é€‰): ", "").strip()
        sequence_annotation['global_action'] = global_action or self._generate_action_description(sequence_annotation['part_annotations'])
        
        # åº”ç”¨æ ‡æ³¨åˆ°åºåˆ—ä¸­çš„æ‰€æœ‰å¸§
        annotated_count = 0
        for j, frame in enumerate(sequence):
            if not frame.get('annotated', False):  # åªæ ‡æ³¨æœªæ ‡æ³¨çš„å¸§
                frame_annotation = sequence_annotation.copy()
                frame_annotation['sample_id'] = frame['id']
                frame_annotation['tokens'] = frame['tokens']
                frame_annotation['timestamp'] = datetime.now().isoformat()
                frame_annotation['annotation_method'] = 'sequence_batch'
                frame_annotation['frame_in_sequence'] = j + 1
                
                self.annotation_data[frame['id']] = frame_annotation
                frame['annotated'] = True
                annotated_count += 1
                
                # è‡ªåŠ¨ä¿å­˜
                self._auto_save_sample(frame)
        
        print(f"\nâœ… åºåˆ—æ ‡æ³¨å®Œæˆï¼")
        print(f"   æ ‡æ³¨åŠ¨ä½œ: {sequence_annotation['global_action']}")
        print(f"   åº”ç”¨åˆ° {annotated_count} å¸§")
        print(f"   åºåˆ—èŒƒå›´: å¸§ {start_index+1}-{start_index+len(sequence)}")

    def _init_visualization_window(self):
        """åˆå§‹åŒ–å¯è§†åŒ–çª—å£"""
        try:
            if VISUALIZATION_AVAILABLE:
                # å°è¯•å¯¼å…¥å¯è§†åŒ–çª—å£ç±»
                from tools.visualization_window import VisualizationWindow
                self.visualization_window = VisualizationWindow()
                print("âœ… å¯è§†åŒ–çª—å£åˆå§‹åŒ–æˆåŠŸ")
                return True
        except ImportError:
            try:
                # å¤‡é€‰å¯¼å…¥è·¯å¾„
                from visualization_window import VisualizationWindow
                self.visualization_window = VisualizationWindow()
                print("âœ… å¯è§†åŒ–çª—å£åˆå§‹åŒ–æˆåŠŸ")
                return True
            except ImportError:
                print("âŒ æ— æ³•å¯¼å…¥å¯è§†åŒ–çª—å£ç±»")
        except Exception as e:
            print(f"âŒ å¯è§†åŒ–çª—å£åˆå§‹åŒ–å¤±è´¥: {e}")
        
        self.visualization_window = None
        return False

    def batch_annotate_cli(self):
        """æ™ºèƒ½æ‰¹é‡æ ‡æ³¨ - åŸºäºTokenèšç±»"""
        if not self.samples_to_annotate:
            print("âŒ æš‚æ— æ ·æœ¬æ•°æ®")
            return
            
        print("\nğŸ”¥ æ™ºèƒ½æ‰¹é‡æ ‡æ³¨ç³»ç»Ÿ")
        print("=" * 70)
        print("ğŸ’¡ é€‚ç”¨åœºæ™¯: å¤§è§„æ¨¡æ•°æ®é›†(å¦‚MARS 40kæ ·æœ¬)")
        print("ğŸ’¡ æ ‡æ³¨ç­–ç•¥: ä»£è¡¨æ€§é‡‡æ · + è‡ªåŠ¨æ¨å¹¿")
        print("=" * 70)
        
        # 1. Tokenç»Ÿè®¡åˆ†æ
        print("\nğŸ“Š æ­¥éª¤1: Tokenç»„åˆåˆ†æ")
        token_groups = self._analyze_token_patterns()
        
        if not token_groups:
            print("âŒ Tokenåˆ†æå¤±è´¥")
            return
        
        print(f"\nâœ… å‘ç° {len(token_groups)} ä¸ªä¸åŒçš„Tokenç»„åˆ")
        print(f"   è¦†ç›– {sum(len(g['samples']) for g in token_groups.values())} ä¸ªæ ·æœ¬")
        
        # 2. æ˜¾ç¤ºTop Tokenç»„åˆ
        print("\nğŸ“ˆ Top 20 æœ€å¸¸è§Tokenç»„åˆ:")
        sorted_groups = sorted(token_groups.items(), 
                              key=lambda x: len(x[1]['samples']), 
                              reverse=True)
        
        for i, (token_key, group_info) in enumerate(sorted_groups[:20], 1):
            count = len(group_info['samples'])
            percentage = count / len(self.samples_to_annotate) * 100
            annotated = group_info['annotated_count']
            status = f"âœ… å·²æ ‡æ³¨{annotated}" if annotated > 0 else "â­• æœªæ ‡æ³¨"
            print(f"  {i:2d}. {token_key} â†’ {count:5d}æ ·æœ¬ ({percentage:5.2f}%) {status}")
        
        # 3. æ ‡æ³¨ç­–ç•¥é€‰æ‹©
        print("\nğŸ¯ æ ‡æ³¨ç­–ç•¥:")
        print("1. æŒ‰é¢‘ç‡æ ‡æ³¨ - ä¼˜å…ˆæ ‡æ³¨æœ€å¸¸è§çš„ç»„åˆ(è¦†ç›–ç‡é«˜)")
        print("2. é‡‡æ ·æ ‡æ³¨ - æ¯ä¸ªç»„åˆæ ‡æ³¨1ä¸ªä»£è¡¨æ ·æœ¬")
        print("3. è‡ªå®šä¹‰ - é€‰æ‹©ç‰¹å®šTokenç»„åˆæ‰¹é‡æ ‡æ³¨")
        print("4. è¿”å›ä¸»èœå•")
        
        choice = safe_input("è¯·é€‰æ‹©ç­–ç•¥ (1-4): ", "1")
        
        if choice == '1':
            self._annotate_by_frequency(sorted_groups)
        elif choice == '2':
            self._annotate_by_sampling(sorted_groups)
        elif choice == '3':
            self._annotate_by_custom_selection(token_groups)
        else:
            return
    
    def _analyze_token_patterns(self):
        """åˆ†æTokenç»„åˆæ¨¡å¼"""
        token_groups = {}
        
        for sample in self.samples_to_annotate:
            tokens = tuple(sample.get('tokens', []))
            token_key = str(list(tokens))
            
            if token_key not in token_groups:
                token_groups[token_key] = {
                    'tokens': tokens,
                    'samples': [],
                    'annotated_count': 0
                }
            
            token_groups[token_key]['samples'].append(sample)
            if sample.get('annotated', False):
                token_groups[token_key]['annotated_count'] += 1
        
        return token_groups
    
    def _annotate_by_frequency(self, sorted_groups):
        """æŒ‰é¢‘ç‡ä¼˜å…ˆæ ‡æ³¨"""
        print("\nğŸ“Œ æŒ‰é¢‘ç‡æ ‡æ³¨æ¨¡å¼")
        print("=" * 70)
        
        try:
            target_coverage = float(safe_input("ç›®æ ‡è¦†ç›–ç‡ (0-100%, æ¨è80): ", "80"))
            target_coverage = min(100, max(0, target_coverage))
        except:
            target_coverage = 80
        
        total_samples = len(self.samples_to_annotate)
        target_count = int(total_samples * target_coverage / 100)
        
        print(f"\nğŸ¯ ç›®æ ‡: æ ‡æ³¨ {target_count}/{total_samples} ä¸ªæ ·æœ¬ ({target_coverage}%è¦†ç›–)")
        
        covered_samples = 0
        groups_to_annotate = []
        
        for token_key, group_info in sorted_groups:
            if covered_samples >= target_count:
                break
            
            group_size = len(group_info['samples'])
            if group_info['annotated_count'] == 0:  # æœªæ ‡æ³¨çš„ç»„
                groups_to_annotate.append((token_key, group_info))
                covered_samples += group_size
        
        print(f"\nğŸ“‹ éœ€è¦æ ‡æ³¨ {len(groups_to_annotate)} ä¸ªTokenç»„åˆ")
        print(f"   å°†è¦†ç›– {covered_samples} ä¸ªæ ·æœ¬")
        
        confirm = safe_input("\nå¼€å§‹æ ‡æ³¨? (y/n): ", "n")
        if confirm.lower() != 'y':
            return
        
        # ä¾æ¬¡æ ‡æ³¨æ¯ä¸ªç»„çš„ä»£è¡¨æ ·æœ¬
        for i, (token_key, group_info) in enumerate(groups_to_annotate, 1):
            print(f"\n{'='*70}")
            print(f"ğŸ“¦ Tokenç»„ [{i}/{len(groups_to_annotate)}]: {token_key}")
            print(f"   åŒ…å« {len(group_info['samples'])} ä¸ªæ ·æœ¬")
            print(f"{'='*70}")
            
            # é€‰æ‹©ä»£è¡¨æ ·æœ¬ï¼ˆç¬¬ä¸€ä¸ªï¼‰
            representative = group_info['samples'][0]
            
            # æ ‡æ³¨ä»£è¡¨æ ·æœ¬
            success = self._annotate_single_sample_with_visual(representative)
            
            if success and representative.get('annotated', False):
                # è¯¢é—®æ˜¯å¦åº”ç”¨åˆ°æ•´ä¸ªç»„
                apply_choice = safe_input(
                    f"\nåº”ç”¨åˆ°è¯¥ç»„çš„å…¶ä»– {len(group_info['samples'])-1} ä¸ªæ ·æœ¬? (y/n/s=è·³è¿‡æ•´ç»„): ",
                    "y"
                )
                
                if apply_choice.lower() == 's':
                    print("â­ï¸ è·³è¿‡æ­¤Tokenç»„")
                    continue
                elif apply_choice.lower() == 'y':
                    # å¤åˆ¶æ ‡æ³¨åˆ°æ•´ä¸ªç»„
                    self._apply_annotation_to_group(representative, group_info['samples'][1:])
                    print(f"âœ… å·²å°†æ ‡æ³¨åº”ç”¨åˆ° {len(group_info['samples'])} ä¸ªæ ·æœ¬")
            
            # è¯¢é—®æ˜¯å¦ç»§ç»­
            if i < len(groups_to_annotate):
                continue_choice = safe_input("ç»§ç»­ä¸‹ä¸€ç»„? (y/n): ", "y")
                if continue_choice.lower() != 'y':
                    break
        
        print(f"\nğŸ‰ æ‰¹é‡æ ‡æ³¨å®Œæˆ!")
        self._show_annotation_summary()
    
    def _annotate_by_sampling(self, sorted_groups):
        """é‡‡æ ·æ ‡æ³¨ - æ¯ç»„æ ‡æ³¨1ä¸ªä»£è¡¨"""
        print("\nğŸ² é‡‡æ ·æ ‡æ³¨æ¨¡å¼")
        print("=" * 70)
        print("ğŸ’¡ æ¯ä¸ªTokenç»„åˆåªæ ‡æ³¨1ä¸ªä»£è¡¨æ ·æœ¬ï¼Œå¿«é€Ÿå»ºç«‹åŸºç¡€æ ‡æ³¨åº“")
        
        try:
            max_groups = int(safe_input("æœ€å¤šæ ‡æ³¨å¤šå°‘ä¸ªç»„? (æ¨è50-100): ", "50"))
        except:
            max_groups = 50
        
        # è¿‡æ»¤æœªæ ‡æ³¨çš„ç»„
        unannotated_groups = [(k, v) for k, v in sorted_groups 
                              if v['annotated_count'] == 0]
        
        groups_to_annotate = unannotated_groups[:max_groups]
        
        print(f"\nğŸ“‹ å°†æ ‡æ³¨ {len(groups_to_annotate)} ä¸ªTokenç»„åˆ")
        total_coverage = sum(len(g[1]['samples']) for g in groups_to_annotate)
        print(f"   æ½œåœ¨è¦†ç›–: {total_coverage} ä¸ªæ ·æœ¬ "
              f"({total_coverage/len(self.samples_to_annotate)*100:.1f}%)")
        
        confirm = safe_input("\nå¼€å§‹é‡‡æ ·æ ‡æ³¨? (y/n): ", "n")
        if confirm.lower() != 'y':
            return
        
        for i, (token_key, group_info) in enumerate(groups_to_annotate, 1):
            print(f"\n{'='*70}")
            print(f"ğŸ“¦ [{i}/{len(groups_to_annotate)}] {token_key}")
            print(f"   ä»£è¡¨ {len(group_info['samples'])} ä¸ªæ ·æœ¬")
            
            representative = group_info['samples'][0]
            success = self._annotate_single_sample_with_visual(representative)
            
            if i < len(groups_to_annotate):
                continue_choice = safe_input("ç»§ç»­? (y/n): ", "y")
                if continue_choice.lower() != 'y':
                    break
        
        print(f"\nğŸ‰ é‡‡æ ·æ ‡æ³¨å®Œæˆ!")
        self._show_annotation_summary()
    
    def _annotate_by_custom_selection(self, token_groups):
        """è‡ªå®šä¹‰é€‰æ‹©Tokenç»„åˆæ ‡æ³¨"""
        print("\nâœï¸ è‡ªå®šä¹‰æ ‡æ³¨æ¨¡å¼")
        print("=" * 70)
        
        sorted_groups = sorted(token_groups.items(), 
                              key=lambda x: len(x[1]['samples']), 
                              reverse=True)
        
        print("\nå¯ç”¨Tokenç»„åˆ:")
        for i, (token_key, group_info) in enumerate(sorted_groups[:30], 1):
            count = len(group_info['samples'])
            status = "âœ…" if group_info['annotated_count'] > 0 else "â­•"
            print(f"  {i:2d}. {status} {token_key} ({count}ä¸ªæ ·æœ¬)")
        
        try:
            indices = safe_input("\nè¾“å…¥è¦æ ‡æ³¨çš„ç»„ç¼–å·(é€—å·åˆ†éš”ï¼Œå¦‚1,3,5): ", "")
            selected_indices = [int(x.strip()) - 1 for x in indices.split(',') if x.strip()]
            
            selected_groups = [sorted_groups[i] for i in selected_indices 
                             if 0 <= i < len(sorted_groups)]
            
            if not selected_groups:
                print("âŒ æ— æ•ˆé€‰æ‹©")
                return
            
            print(f"\nå·²é€‰æ‹© {len(selected_groups)} ä¸ªç»„")
            
            for i, (token_key, group_info) in enumerate(selected_groups, 1):
                print(f"\n{'='*70}")
                print(f"ğŸ“¦ [{i}/{len(selected_groups)}] {token_key}")
                print(f"   åŒ…å« {len(group_info['samples'])} ä¸ªæ ·æœ¬")
                
                representative = group_info['samples'][0]
                success = self._annotate_single_sample_with_visual(representative)
                
                if success and representative.get('annotated', False):
                    apply = safe_input(f"åº”ç”¨åˆ°è¯¥ç»„å…¨éƒ¨{len(group_info['samples'])}ä¸ªæ ·æœ¬? (y/n): ", "y")
                    if apply.lower() == 'y':
                        self._apply_annotation_to_group(representative, group_info['samples'][1:])
                
        except Exception as e:
            print(f"âŒ é€‰æ‹©å¤±è´¥: {e}")
    
    def _apply_annotation_to_group(self, source_sample, target_samples):
        """å°†æ ‡æ³¨åº”ç”¨åˆ°ä¸€ç»„æ ·æœ¬"""
        if not source_sample.get('annotated', False):
            return 0
        
        applied_count = 0
        for sample in target_samples:
            # å¤åˆ¶æ ‡æ³¨ä¿¡æ¯
            sample['annotations'] = source_sample.get('annotations', {}).copy()
            sample['overall_action'] = source_sample.get('overall_action', '')
            sample['annotation_time'] = datetime.now().isoformat()
            sample['annotated'] = True
            sample['batch_applied'] = True
            sample['batch_source_id'] = source_sample.get('id')
            
            # ä¿å­˜
            self._auto_save_sample(sample)
            applied_count += 1
        
        return applied_count
    
    def _annotate_single_sample_with_visual(self, sample):
        """æ ‡æ³¨å•ä¸ªæ ·æœ¬ï¼ˆå¸¦å¯è§†åŒ–ï¼‰"""
        print(f"\nğŸ“ æ–‡ä»¶: {sample.get('filename', 'æœªçŸ¥')}")
        print(f"ğŸ¯ Token: {sample.get('tokens', [])}")
        
        # æ˜¾ç¤ºå¯è§†åŒ–
        if VISUALIZATION_AVAILABLE:
            try:
                success = show_sample_visualization(sample, sample)
                if success:
                    print("âœ… å¯è§†åŒ–çª—å£å·²æ‰“å¼€")
                    input("ğŸ‘€ æŸ¥çœ‹å®Œæ¯•åæŒ‰å›è½¦ç»§ç»­...")
            except Exception as e:
                print(f"âš ï¸ å¯è§†åŒ–å¤±è´¥: {e}")
        
        # é€‰æ‹©æ ‡æ³¨æ¨¡å¼
        print("\næ ‡æ³¨é€‰é¡¹:")
        print("1. è¯¦ç»†åˆ†éƒ¨ä½æ ‡æ³¨")
        print("2. å¿«é€Ÿæ•´ä½“æ ‡æ³¨")
        print("3. è·³è¿‡")
        
        choice = safe_input("é€‰æ‹© (1-3): ", "1")
        
        if choice == '1':
            return self._detailed_annotation_cli(sample)
        elif choice == '2':
            return self._quick_overall_annotation_cli(sample)
        else:
            return False
    
    def _show_annotation_summary(self):
        """æ˜¾ç¤ºæ ‡æ³¨æ‘˜è¦"""
        annotated = sum(1 for s in self.samples_to_annotate if s.get('annotated', False))
        total = len(self.samples_to_annotate)
        
        print(f"\nğŸ“Š æ ‡æ³¨æ‘˜è¦:")
        print(f"   å·²æ ‡æ³¨: {annotated}/{total} ({annotated/total*100:.1f}%)")
        
        # Tokenè¦†ç›–ç‡
        token_groups = self._analyze_token_patterns()
        annotated_groups = sum(1 for g in token_groups.values() if g['annotated_count'] > 0)
        print(f"   Tokenç»„è¦†ç›–: {annotated_groups}/{len(token_groups)} "
              f"({annotated_groups/len(token_groups)*100:.1f}%)")
    
    def token_analysis_cli(self):
        """Tokenåˆ†æä¸é‡‡æ ·ç­–ç•¥"""
        if not self.samples_to_annotate:
            print("âŒ æš‚æ— æ ·æœ¬æ•°æ®")
            return
        
        print("\nğŸ“Š Tokenç»Ÿè®¡åˆ†æ")
        print("=" * 70)
        
        # Tokenç»„åˆåˆ†æ
        token_groups = self._analyze_token_patterns()
        
        print(f"ğŸ“ˆ æ•°æ®æ¦‚å†µ:")
        print(f"   æ€»æ ·æœ¬æ•°: {len(self.samples_to_annotate)}")
        print(f"   Tokenç»„åˆæ•°: {len(token_groups)}")
        print(f"   å¹³å‡æ¯ç»„: {len(self.samples_to_annotate)/len(token_groups):.1f} ä¸ªæ ·æœ¬")
        
        # åˆ†å¸ƒç»Ÿè®¡
        group_sizes = [len(g['samples']) for g in token_groups.values()]
        import statistics
        
        print(f"\nğŸ“Š åˆ†å¸ƒç»Ÿè®¡:")
        print(f"   æœ€å¤§ç»„: {max(group_sizes)} ä¸ªæ ·æœ¬")
        print(f"   æœ€å°ç»„: {min(group_sizes)} ä¸ªæ ·æœ¬")
        print(f"   ä¸­ä½æ•°: {statistics.median(group_sizes):.0f} ä¸ªæ ·æœ¬")
        print(f"   å¹³å‡å€¼: {statistics.mean(group_sizes):.1f} ä¸ªæ ·æœ¬")
        
        # Topç»„åˆ
        sorted_groups = sorted(token_groups.items(), 
                              key=lambda x: len(x[1]['samples']), 
                              reverse=True)
        
        print(f"\nğŸ” Top 10 Tokenç»„åˆ:")
        for i, (token_key, group_info) in enumerate(sorted_groups[:10], 1):
            count = len(group_info['samples'])
            percentage = count / len(self.samples_to_annotate) * 100
            status = f"âœ…{group_info['annotated_count']}" if group_info['annotated_count'] > 0 else "â­•"
            print(f"  {i:2d}. {status} {token_key}")
            print(f"      â†’ {count} æ ·æœ¬ ({percentage:.2f}%)")
        
        # é‡‡æ ·å»ºè®®
        print(f"\nğŸ’¡ æ ‡æ³¨ç­–ç•¥å»ºè®®:")
        
        # è®¡ç®—ä¸åŒè¦†ç›–ç‡éœ€è¦æ ‡æ³¨çš„ç»„æ•°
        cumulative = 0
        for coverage_target in [50, 80, 90, 95]:
            target_count = len(self.samples_to_annotate) * coverage_target / 100
            groups_needed = 0
            cumulative_temp = 0
            
            for _, group_info in sorted_groups:
                if cumulative_temp >= target_count:
                    break
                cumulative_temp += len(group_info['samples'])
                groups_needed += 1
            
            print(f"   {coverage_target}% è¦†ç›–ç‡ â†’ éœ€æ ‡æ³¨ {groups_needed} ä¸ªTokenç»„")
        
        print(f"\næ¨èç­–ç•¥:")
        print(f"   ğŸ”¸ å¿«é€Ÿå»ºç«‹åŸºç¡€: é‡‡æ ·æ ‡æ³¨æ¨¡å¼ (æ ‡æ³¨50-100ä¸ªä»£è¡¨)")
        print(f"   ğŸ”¸ é«˜è¦†ç›–ç‡: æŒ‰é¢‘ç‡æ ‡æ³¨ (ç›®æ ‡80%è¦†ç›–)")
        print(f"   ğŸ”¸ é’ˆå¯¹æ€§æ ‡æ³¨: è‡ªå®šä¹‰é€‰æ‹©ç‰¹å®šTokenç»„åˆ")
    
    def old_batch_annotate_cli(self):
        """å‘½ä»¤è¡Œæ‰¹é‡æ ‡æ³¨"""
        if not self.annotation_data:
            print("âŒ æš‚æ— å·²æ ‡æ³¨æ ·æœ¬ï¼Œæ— æ³•è¿›è¡Œæ‰¹é‡æ ‡æ³¨")
            return
            
        print("\nğŸ”„ æ‰¹é‡æ ‡æ³¨ç›¸ä¼¼æ ·æœ¬")
        
        # æ£€æŸ¥æ˜¯å¦æœ‰æ ‡æ³¨æ•°æ®
        if len(self.annotation_data) == 0:
            print("âŒ å½“å‰æ²¡æœ‰å·²æ ‡æ³¨æ ·æœ¬")
            return
        
        # æ˜¾ç¤ºå·²æ ‡æ³¨çš„æ ·æœ¬
        print(f"å·²æ ‡æ³¨æ ·æœ¬ (å…±{len(self.annotation_data)}ä¸ª):")
        displayed_count = 0
        for sample_id, annotation in self.annotation_data.items():
            if displayed_count >= 15:  # æœ€å¤šæ˜¾ç¤º15ä¸ª
                print(f"  ... è¿˜æœ‰{len(self.annotation_data) - displayed_count}ä¸ªæ ·æœ¬")
                break
                
            tokens_str = str(annotation.get('tokens', 'N/A'))
            global_action = annotation.get('global_action', 'æœªæè¿°')
            method = annotation.get('annotation_method', 'manual')
            
            print(f"  æ ·æœ¬{sample_id}: {tokens_str} -> {global_action} ({method})")
            displayed_count += 1
            
        try:
            reference_id = int(input("é€‰æ‹©å‚è€ƒæ ·æœ¬ID: ").strip())
            if reference_id not in self.annotation_data:
                print("âŒ æ ·æœ¬IDä¸å­˜åœ¨")
                return
                
            reference_annotation = self.annotation_data[reference_id]
            reference_tokens = tuple(reference_annotation['tokens'])
            
            # æ‰¾åˆ°ç›¸ä¼¼æ ·æœ¬
            similar_samples = []
            for sample in self.samples_to_annotate:
                if tuple(sample['tokens']) == reference_tokens and not sample['annotated']:
                    similar_samples.append(sample)
                    
            if not similar_samples:
                print("âŒ æœªæ‰¾åˆ°ç›¸ä¼¼çš„æœªæ ‡æ³¨æ ·æœ¬")
                return
                
            print(f"æ‰¾åˆ° {len(similar_samples)} ä¸ªç›¸ä¼¼æ ·æœ¬")
            confirm = input("æ˜¯å¦æ‰¹é‡åº”ç”¨æ ‡æ³¨? (y/n): ").strip().lower()
            
            if confirm in ['y', 'yes']:
                for sample in similar_samples:
                    new_annotation = reference_annotation.copy()
                    new_annotation['sample_id'] = sample['id']
                    new_annotation['timestamp'] = datetime.now().isoformat()
                    new_annotation['batch_source'] = reference_id
                    
                    self.annotation_data[sample['id']] = new_annotation
                    sample['annotated'] = True
                    
                print(f"âœ… æ‰¹é‡æ ‡æ³¨å®Œæˆï¼Œå…±æ ‡æ³¨ {len(similar_samples)} ä¸ªæ ·æœ¬")
            
        except ValueError:
            print("âŒ è¯·è¾“å…¥æœ‰æ•ˆçš„æ ·æœ¬ID")
            
    def show_progress_cli(self):
        """æ˜¾ç¤ºæ ‡æ³¨è¿›åº¦"""
        if not self.samples_to_annotate:
            print("âŒ æš‚æ— æ ·æœ¬æ•°æ®")
            return
            
        print("\nğŸ“Š æ ‡æ³¨è¿›åº¦ç»Ÿè®¡")
        print("=" * 60)
        
        # åŸºç¡€ç»Ÿè®¡
        total_samples = len(self.samples_to_annotate)
        annotated_count = len(self.annotation_data)
        progress_percentage = (annotated_count / total_samples * 100) if total_samples > 0 else 0
        
        print(f"æ€»æ ·æœ¬æ•°: {total_samples}")
        print(f"å·²æ ‡æ³¨: {annotated_count}")
        print(f"æœªæ ‡æ³¨: {total_samples - annotated_count}")
        print(f"å®Œæˆåº¦: {progress_percentage:.1f}%")
        
        if annotated_count == 0:
            print("æš‚æ— å·²æ ‡æ³¨æ ·æœ¬")
            return
        
        # ç»Ÿè®¡å„éƒ¨ä½æ ‡æ³¨æƒ…å†µ
        part_stats = {}
        action_stats = {}
        
        for annotation in self.annotation_data.values():
            for part, action in annotation['part_annotations'].items():
                if part not in part_stats:
                    part_stats[part] = {}
                part_stats[part][action] = part_stats[part].get(action, 0) + 1
                
                if action not in action_stats:
                    action_stats[action] = 0
                action_stats[action] += 1
                
        print(f"\nå„éƒ¨ä½æ ‡æ³¨åˆ†å¸ƒ:")
        for part, display_name in zip(self.part_names, self.part_display_names):
            if part in part_stats:
                print(f"  {display_name}: {len(part_stats[part])} ç§åŠ¨ä½œ")
                # æ˜¾ç¤ºå‰3ä¸ªæœ€å¸¸è§åŠ¨ä½œ
                top_actions = sorted(part_stats[part].items(), key=lambda x: x[1], reverse=True)[:3]
                for action, count in top_actions:
                    print(f"    {action}: {count}æ¬¡")
            else:
                print(f"  {display_name}: æš‚æ— æ ‡æ³¨")
                
    def export_annotations(self):
        """å¯¼å‡ºæ ‡æ³¨ç»“æœ"""
        if not self.annotation_data:
            print("âŒ æš‚æ— æ ‡æ³¨æ•°æ®ï¼Œæ— æ³•å¯¼å‡º")
            return
            
        print(f"\nğŸ“¤ å¯¼å‡ºæ ‡æ³¨ç»“æœ")
        print(f"å‡†å¤‡å¯¼å‡º {len(self.annotation_data)} ä¸ªå·²æ ‡æ³¨æ ·æœ¬")
        
        # æ„å»ºæ˜ å°„æ•°æ®
        mapping_data = {
            'part_mappings': {},
            'global_mappings': {},
            'statistics': {
                'total_samples': len(self.samples_to_annotate),
                'annotated_samples': len(self.annotation_data),
                'annotation_date': datetime.now().isoformat()
            },
            'raw_annotations': self.annotation_data
        }
        
        # æ„å»ºéƒ¨ä½æ˜ å°„
        for part in self.part_names:
            mapping_data['part_mappings'][part] = {}
            
        for annotation in self.annotation_data.values():
            tokens = annotation['tokens']
            part_annotations = annotation['part_annotations']
            
            for i, part in enumerate(self.part_names):
                if i < len(tokens) and part in part_annotations:
                    token_id = str(tokens[i])
                    action = part_annotations[part]
                    
                    if token_id not in mapping_data['part_mappings'][part]:
                        mapping_data['part_mappings'][part][token_id] = {
                            'semantic': action,
                            'frequency': 0,
                            'confidence': 0.8,  # é»˜è®¤ç½®ä¿¡åº¦
                            'samples': []
                        }
                    
                    mapping_data['part_mappings'][part][token_id]['frequency'] += 1
                    mapping_data['part_mappings'][part][token_id]['samples'].append(annotation['sample_id'])
                    
        # æ„å»ºå…¨å±€æ˜ å°„
        for annotation in self.annotation_data.values():
            if annotation.get('global_action'):
                token_combo = tuple(annotation['tokens'])
                global_action = annotation['global_action']
                
                combo_key = str(token_combo)
                if combo_key not in mapping_data['global_mappings']:
                    mapping_data['global_mappings'][combo_key] = {
                        'action': global_action,
                        'frequency': 0,
                        'confidence': 0.85,
                        'category': 'annotated',
                        'samples': []
                    }
                    
                mapping_data['global_mappings'][combo_key]['frequency'] += 1
                mapping_data['global_mappings'][combo_key]['samples'].append(annotation['sample_id'])
                
        # ä¿å­˜æ–‡ä»¶
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        filename = f"codebook_annotations_{timestamp}.json"
        
        with open(filename, 'w', encoding='utf-8') as f:
            json.dump(mapping_data, f, indent=2, ensure_ascii=False)
            
        print(f"ğŸ’¾ æ ‡æ³¨ç»“æœå·²å¯¼å‡ºåˆ°: {filename}")
        
        # åŒæ—¶ä¿å­˜ä¸ºæ ‡å‡†æ˜ å°„æ ¼å¼
        mapping_filename = "codebook_action_mappings.json"
        with open(mapping_filename, 'w', encoding='utf-8') as f:
            json.dump(mapping_data, f, indent=2, ensure_ascii=False)
            
        print(f"ğŸ’¾ æ˜ å°„è¡¨å·²ä¿å­˜åˆ°: {mapping_filename}")
        
        # æ˜¾ç¤ºå¯¼å‡ºç»Ÿè®¡
        print(f"\nğŸ“Š å¯¼å‡ºç»Ÿè®¡:")
        print(f"  å·²æ ‡æ³¨æ ·æœ¬: {len(self.annotation_data)}")
        print(f"  éƒ¨ä½æ˜ å°„æ•°: {sum(len(mappings) for mappings in mapping_data['part_mappings'].values())}")
        print(f"  å…¨å±€æ˜ å°„æ•°: {len(mapping_data['global_mappings'])}")
        
        # æ˜¾ç¤ºå„éƒ¨ä½çš„tokenæ•°é‡
        for part, mappings in mapping_data['part_mappings'].items():
            if mappings:
                print(f"    {part}: {len(mappings)} ä¸ªtoken")
        
        print("âœ… æ ‡å‡†æ ¼å¼å¯¼å‡ºå®Œæˆï¼")
        
        # ============= LLM å‹å¥½æ ¼å¼å¯¼å‡º =============
        if LLM_EXPORTER_AVAILABLE:
            print(f"\nğŸ¤– æ£€æµ‹åˆ° LLM å¯¼å‡ºå™¨")
            export_llm = safe_input("æ˜¯å¦åŒæ—¶å¯¼å‡º LLM å‹å¥½æ ¼å¼? (y/n, é»˜è®¤n): ", "n").lower()
            
            if export_llm == 'y':
                print(f"\nğŸš€ å¼€å§‹å¯¼å‡º LLM å‹å¥½æ ¼å¼...")
                print("   åŒ…æ‹¬: Few-shotå­¦ä¹ , æŒ‡ä»¤å¾®è°ƒ, å¯¹è¯æ ¼å¼, RAGçŸ¥è¯†åº“")
                
                try:
                    # å‡†å¤‡æ ·æœ¬æ•°æ®
                    samples_data = []
                    for sample in self.samples_to_annotate:
                        if sample['id'] in self.annotation_data:
                            sample_info = {
                                'id': sample['id'],
                                'split': sample.get('split', 'unknown'),
                                'file_path': sample.get('file_path', sample.get('filename', '')),
                                'vq_loss': sample.get('vq_loss', 0.0),
                                'ground_truth': sample.get('ground_truth', sample.get('ground_truth_action', ''))
                            }
                            samples_data.append(sample_info)
                    
                    # è·å–è¾“å‡ºç›®å½•
                    output_dir = safe_input("è¾“å‡ºç›®å½• (é»˜è®¤: llm_annotations): ", "llm_annotations")
                    dataset_name = safe_input("æ•°æ®é›†åç§° (é»˜è®¤: MARS): ", "MARS")
                    
                    # æ‰§è¡Œå¯¼å‡º
                    exporter = LLMAnnotationExporter()
                    exporter.export_enhanced_annotations(
                        annotation_data=self.annotation_data,
                        samples_data=samples_data,
                        output_dir=output_dir,
                        dataset_name=dataset_name
                    )
                    
                    print(f"\nâœ… LLM å‹å¥½æ ¼å¼å¯¼å‡ºå®Œæˆï¼")
                    print(f"ğŸ“‚ æ–‡ä»¶ä½ç½®: {output_dir}/")
                    print(f"ğŸ’¡ å¯ç”¨äº: Few-shotå­¦ä¹ , æŒ‡ä»¤å¾®è°ƒ, å¯¹è¯è®­ç»ƒ, RAGæ£€ç´¢")
                    
                except Exception as e:
                    print(f"\nâŒ LLM æ ¼å¼å¯¼å‡ºå¤±è´¥: {e}")
                    import traceback
                    traceback.print_exc()
            else:
                print("â­ï¸  è·³è¿‡ LLM æ ¼å¼å¯¼å‡º")
        
    def import_annotations(self):
        """å¯¼å…¥ç°æœ‰æ ‡æ³¨"""
        filename = input("è¯·è¾“å…¥æ ‡æ³¨æ–‡ä»¶è·¯å¾„ (é»˜è®¤: codebook_action_mappings.json): ").strip()
        if not filename:
            filename = "codebook_action_mappings.json"
            
        if not os.path.exists(filename):
            print(f"âŒ æ–‡ä»¶ä¸å­˜åœ¨: {filename}")
            return
            
        try:
            with open(filename, 'r', encoding='utf-8') as f:
                data = json.load(f)
                
            if 'raw_annotations' in data:
                imported_count = len(data['raw_annotations'])
                self.annotation_data.update(data['raw_annotations'])
                
                # æ›´æ–°æ ·æœ¬æ ‡æ³¨çŠ¶æ€
                for sample in self.samples_to_annotate:
                    if sample['id'] in self.annotation_data:
                        sample['annotated'] = True
                        
                print(f"âœ… å¯¼å…¥ {imported_count} ä¸ªæ ‡æ³¨")
            else:
                print("âš ï¸ æ–‡ä»¶æ ¼å¼ä¸åŒ…å«åŸå§‹æ ‡æ³¨æ•°æ®")
                
        except Exception as e:
            print(f"âŒ å¯¼å…¥å¤±è´¥: {e}")

def main():
    """ä¸»å‡½æ•°"""
    import sys
    
    # æ£€æŸ¥å‘½ä»¤è¡Œå‚æ•°
    batch_mode = '--batch' in sys.argv or '--auto' in sys.argv
    demo_mode = '--demo' in sys.argv
    
    print("ğŸ·ï¸ ç æœ¬åŠ¨ä½œæ ‡æ³¨å·¥å…·")
    print("=" * 50)
    
    if batch_mode:
        print("ğŸ¤– æ‰¹å¤„ç†æ¨¡å¼å¯åŠ¨")
        set_batch_mode(True)
        
    tool = SkeletonAnnotationTool()
    
    if demo_mode:
        print("ğŸ¬ æ¼”ç¤ºæ¨¡å¼ï¼šè‡ªåŠ¨åŠ è½½æ•°æ®å¹¶è¿›è¡Œå¿«é€Ÿæ ‡æ³¨")
        # è‡ªåŠ¨æ¼”ç¤ºæµç¨‹
        tool.load_real_data("ntu")
        if tool.samples_to_annotate:
            # å¯¹å‰3ä¸ªæ ·æœ¬è¿›è¡Œå¿«é€Ÿæ ‡æ³¨
            for i, sample in enumerate(tool.samples_to_annotate[:3]):
                print(f"\nğŸ­ è‡ªåŠ¨æ ‡æ³¨æ ·æœ¬ {i+1}: {sample.get('filename', 'unknown')}")
                tool._quick_overall_annotation_cli(sample)
                sample['annotated'] = True
            print("ğŸ‰ æ¼”ç¤ºå®Œæˆ")
        return
    
    if tool.use_gui and GUI_AVAILABLE and not batch_mode:
        print("ğŸ–¥ï¸ å¯åŠ¨å›¾å½¢ç•Œé¢...")
        # è¿™é‡Œå¯ä»¥å¯åŠ¨GUIç‰ˆæœ¬
        print("âš ï¸ GUIç•Œé¢å¼€å‘ä¸­ï¼Œä½¿ç”¨å‘½ä»¤è¡Œç•Œé¢")
        tool.run_cli_annotation()
    else:
        print("ğŸ’» ä½¿ç”¨å‘½ä»¤è¡Œç•Œé¢...")
        tool.run_cli_annotation()

if __name__ == "__main__":
    main()