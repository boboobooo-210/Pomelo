#!/usr/bin/env python3
"""
AdaptiveSkeletonDVAE å¯è§†åŒ–å·¥å…·
ç”¨äºå¯è§†åŒ–å®é™…è®­ç»ƒçš„ AdaptiveSkeletonDVAE æ¨¡å‹çš„è¾“å…¥è¾“å‡º
"""

import os
import sys
import numpy as np
import torch
import yaml
import argparse
from pathlib import Path

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
project_root = Path(__file__).parent.parent
sys.path.append(str(project_root))

from models.adaptive_skeleton_dvae import AdaptiveSkeletonDVAE
from datasets.build import build_dataset_from_cfg
from utils.config import cfg_from_yaml_file
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D
import matplotlib.animation as animation


class AdaptiveSkeletonDVAEVisualizer:
    """AdaptiveSkeletonDVAE å¯è§†åŒ–å™¨"""
    
    def __init__(self, config_path, checkpoint_path=None, device='cuda'):
        """
        åˆå§‹åŒ–å¯è§†åŒ–å™¨
        Args:
            config_path: é…ç½®æ–‡ä»¶è·¯å¾„
            checkpoint_path: æ¨¡å‹æ£€æŸ¥ç‚¹è·¯å¾„ (å¯é€‰)
            device: è®¾å¤‡
        """
        self.device = device
        
        # åŠ è½½é…ç½®
        self.config = cfg_from_yaml_file(config_path)
        print(f"ğŸ“‹ Loaded config from {config_path}")
        
        # åˆ›å»ºæ¨¡å‹ - å¤„ç†ä¸åŒçš„é…ç½®ç»“æ„
        if hasattr(self.config, 'model'):
            model_config = self.config.model
        else:
            # å¦‚æœæ²¡æœ‰modelå­—æ®µï¼Œåˆ›å»ºä¸€ä¸ªé»˜è®¤é…ç½®
            print("âš ï¸ No model config found, using default AdaptiveSkeletonDVAE config")
            from types import SimpleNamespace
            model_config = SimpleNamespace()
            model_config.NAME = 'AdaptiveSkeletonDVAE'
            model_config.latent_dim = 512
            model_config.num_tokens = 1024
            model_config.commitment_cost = 0.25
            model_config.loss_type = 'mse'  # é»˜è®¤ä½¿ç”¨MSE
        
        self.model = AdaptiveSkeletonDVAE(model_config).to(device)
        
        # åŠ è½½æ£€æŸ¥ç‚¹
        if checkpoint_path and os.path.exists(checkpoint_path):
            checkpoint = torch.load(checkpoint_path, map_location=device)
            if 'model' in checkpoint:
                self.model.load_state_dict(checkpoint['model'])
            else:
                self.model.load_state_dict(checkpoint)
            print(f"âœ… Loaded model from {checkpoint_path}")
        else:
            print("âš ï¸ No checkpoint loaded, using random weights")
        
        self.model.eval()
    
    def visualize_single_sample(self, data_sample, save_path=None):
        """
        å¯è§†åŒ–å•ä¸ªæ ·æœ¬çš„é‡å»ºç»“æœï¼ŒåŒ…å«éª¨æ¶å¢å¼ºç­–ç•¥çš„è¯¦ç»†å±•ç¤º
        Args:
            data_sample: (N, 3) ç‚¹äº‘æ•°æ® (650ç‚¹)
            save_path: ä¿å­˜è·¯å¾„
        """
        if isinstance(data_sample, np.ndarray):
            data_sample = torch.from_numpy(data_sample).float()
        
        # æ·»åŠ æ‰¹æ¬¡ç»´åº¦
        if len(data_sample.shape) == 2:
            data_sample = data_sample.unsqueeze(0)  # (1, N, 3)
        
        data_sample = data_sample.to(self.device)
        
        with torch.no_grad():
            # æ¨¡å‹æ¨ç†
            coarse, fine, encoding_indices = self.model(data_sample)
        
        # è½¬æ¢ä¸ºnumpy
        original = data_sample[0].cpu().numpy()  # (650, 3)
        coarse_recon = coarse[0].cpu().numpy()  # (64, 3)  
        fine_recon = fine[0].cpu().numpy()  # (650, 3)
        
        # åˆ›å»ºæ›´ä¸°å¯Œçš„å¯è§†åŒ–
        fig = plt.figure(figsize=(24, 16))
        
        # ç¬¬ä¸€è¡Œï¼šéª¨æ¶å¢å¼ºç­–ç•¥åˆ†æ
        # 1. åŸå§‹17å…³èŠ‚éª¨æ¶ç»“æ„
        ax1 = fig.add_subplot(241, projection='3d')
        self._plot_skeleton_structure(ax1, original, title='Original 17-Joint Skeleton\n(Extracted from 650 points)', 
                                     show_skeleton=True, show_augmented=False)
        
        # 2. éª¨æ¶å¢å¼ºç­–ç•¥å¯è§†åŒ– (æ˜¾ç¤ºæ’å€¼ç‚¹)
        ax2 = fig.add_subplot(242, projection='3d') 
        self._plot_skeleton_structure(ax2, original, title='Skeleton Augmentation Strategy\n(650 points with interpolation)',
                                     show_skeleton=True, show_augmented=True)
        
        # 3. å¯†é›†ç‚¹äº‘å±•ç¤º
        ax3 = fig.add_subplot(243, projection='3d')
        self._plot_point_cloud(ax3, original, title=f'Dense Point Cloud\n({original.shape[0]} points)', 
                              color='blue', size=8)
        
        # 4. Coarseé‡å»º (64ç‚¹)
        ax4 = fig.add_subplot(244, projection='3d')
        self._plot_point_cloud(ax4, coarse_recon, title=f'Coarse Reconstruction\n({coarse_recon.shape[0]} points)', 
                              color='red', size=30)
        
        # ç¬¬äºŒè¡Œï¼šé‡å»ºå¯¹æ¯”åˆ†æ
        # 5. Fineé‡å»º
        ax5 = fig.add_subplot(245, projection='3d')
        self._plot_point_cloud(ax5, fine_recon, title=f'Fine Reconstruction\n({fine_recon.shape[0]} points)', 
                              color='green', size=8)
        
        # 6. é‡å»ºéª¨æ¶ç»“æ„
        ax6 = fig.add_subplot(246, projection='3d')
        self._plot_skeleton_structure(ax6, fine_recon, title='Reconstructed Skeleton\n(From 650 points)',
                                     show_skeleton=True, show_augmented=False)
        
        # 7. å åŠ å¯¹æ¯”
        ax7 = fig.add_subplot(247, projection='3d')
        self._plot_overlay_comparison(ax7, original, fine_recon, title='Original vs Reconstructed\n(Overlay)')
        
        # 8. è¯¯å·®åˆ†æ
        ax8 = fig.add_subplot(248)
        self._plot_error_analysis(ax8, original, fine_recon)
        
        plt.tight_layout()
        
        if save_path:
            os.makedirs(os.path.dirname(save_path), exist_ok=True)
            plt.savefig(save_path, dpi=150, bbox_inches='tight')
            print(f"ğŸ’¾ Saved visualization to {save_path}")
        
        plt.show()
        
        # æ‰“å°ç»Ÿè®¡ä¿¡æ¯
        mse_coarse = np.mean((original - coarse_recon) ** 2) if original.shape[0] == 64 else "N/A (different sizes)"
        mse_fine = np.mean((original - fine_recon) ** 2)
        print(f"ğŸ“Š MSE - Coarse: {mse_coarse}, Fine: {mse_fine:.6f}")
        print(f"ğŸ”¢ VQ Index: {encoding_indices[0].item()}")
        print(f"ğŸ¦´ Skeleton Analysis:")
        print(f"   Original joints (estimated): 17")
        print(f"   Augmented points: {original.shape[0] - 17} (interpolated)")
        print(f"   Augmentation ratio: {(original.shape[0] - 17) / 17:.1f}x")
        
        return {
            'original': original,
            'coarse': coarse_recon,
            'fine': fine_recon,
            'vq_index': encoding_indices[0].item(),
            'mse_fine': mse_fine
        }
    
    def _plot_point_cloud(self, ax, points, title, color='blue', size=20):
        """ç»˜åˆ¶ç‚¹äº‘"""
        ax.scatter(points[:, 0], points[:, 1], points[:, 2], 
                  c=color, s=size, alpha=0.6)
        
        ax.set_title(title, fontsize=12, fontweight='bold')
        ax.set_xlabel('X')
        ax.set_ylabel('Y')
        ax.set_zlabel('Z')
        
        # è®¾ç½®ç›¸åŒçš„åæ ‡èŒƒå›´
        all_points = points.reshape(-1, 3)
        max_range = np.max(np.abs(all_points)) * 1.1
        ax.set_xlim([-max_range, max_range])
        ax.set_ylim([-max_range, max_range])
        ax.set_zlim([-max_range, max_range])
    
    def _get_mmfi_skeleton_connections(self):
        """è·å–MMFI 17å…³èŠ‚è¿æ¥å…³ç³»"""
        return [
            # è…¿éƒ¨è¿æ¥
            (1, 2),   # J1-J2: å³ä¾§è…°éƒ¨ -> å³è†ç›–
            (4, 5),   # J4-J5: å·¦ä¾§è…°éƒ¨ -> å·¦è†ç›–
            (0, 1),   # J0-J1: èº¯å¹²ä¸­å¿ƒ -> å³ä¾§è…°éƒ¨
            (0, 4),   # J0-J4: èº¯å¹²ä¸­å¿ƒ -> å·¦ä¾§è…°éƒ¨
            (2, 3),   # J2-J3: å³è†ç›– -> å³è„š
            (5, 6),   # J5-J6: å·¦è†ç›– -> å·¦è„š
            
            # èº¯å¹²åˆ°è‚©è†€
            (0, 7),   # J0-J7: èº¯å¹²ä¸­å¿ƒ -> è‚©è†€ä¸­å¿ƒ
            
            # å¤´éƒ¨è¿æ¥é“¾
            (10, 9),  # J10-J9: å¤´é¡¶ -> å¤´é¢ˆéƒ¨
            (9, 8),   # J9-J8: å¤´é¢ˆéƒ¨ -> å¤´é¢ˆéƒ¨
            (8, 7),   # J8-J7: å¤´é¢ˆéƒ¨ -> è‚©è†€ä¸­å¿ƒ
            
            # æ‰‹è‡‚è¿æ¥é“¾
            (8, 11),  # J8-J11: å¤´é¢ˆéƒ¨ -> è‚©è†€
            (11, 12), # J11-J12: è‚©è†€ -> å·¦è‚©è†€
            (12, 13), # J12-J13: å·¦è‚©è†€ -> å·¦æ‰‹
            
            (8, 14),  # J8-J14: å¤´é¢ˆéƒ¨ -> é¢ˆéƒ¨
            (14, 15), # J14-J15: é¢ˆéƒ¨ -> å³è‚©è†€
            (15, 16), # J15-J16: å³è‚©è†€ -> å³æ‰‹
        ]
    
    def _get_joint_colors(self):
        """è·å–å…³èŠ‚ç‚¹é¢œè‰²ï¼ˆèº«ä½“éƒ¨ä½ç¼–ç ï¼‰"""
        return [
            '#E74C3C',  # J0: SpineBase - çº¢è‰² (èº¯å¹²)
            '#E74C3C',  # J1: å³ä¾§è…°éƒ¨ - çº¢è‰² (èº¯å¹²)
            '#E67E22',  # J2: å³è†ç›– - æ©˜è‰² (å³è…¿)
            '#E67E22',  # J3: å³è„š - æ©˜è‰² (å³è…¿)
            '#E74C3C',  # J4: å·¦ä¾§è…°éƒ¨ - çº¢è‰² (èº¯å¹²)
            '#9B59B6',  # J5: å·¦è†ç›– - ç´«è‰² (å·¦è…¿)
            '#9B59B6',  # J6: å·¦è„š - ç´«è‰² (å·¦è…¿)
            '#E74C3C',  # J7: è‚©è†€ä¸­å¿ƒ - çº¢è‰² (èº¯å¹²)
            '#E74C3C',  # J8: å¤´é¢ˆéƒ¨ - çº¢è‰² (èº¯å¹²)
            '#F39C12',  # J9: å¤´é¢ˆéƒ¨ - æ©™è‰² (å¤´éƒ¨)
            '#F39C12',  # J10: å¤´é¡¶ - æ©™è‰² (å¤´éƒ¨)
            '#E74C3C',  # J11: è‚©è†€ - çº¢è‰² (èº¯å¹²)
            '#3498DB',  # J12: å·¦è‚©è†€ - è“è‰² (å·¦è‡‚)
            '#3498DB',  # J13: å·¦æ‰‹ - è“è‰² (å·¦è‡‚)
            '#E74C3C',  # J14: é¢ˆéƒ¨ - çº¢è‰² (èº¯å¹²)
            '#27AE60',  # J15: å³è‚©è†€ - ç»¿è‰² (å³è‡‚)
            '#27AE60',  # J16: å³æ‰‹ - ç»¿è‰² (å³è‡‚)
        ]
    
    def _extract_skeleton_joints(self, points, method='uniform'):
        """
        ä»650ä¸ªç‚¹ä¸­æå–17ä¸ªä¸»è¦å…³èŠ‚ç‚¹
        Args:
            points: (650, 3) å¯†é›†ç‚¹äº‘
            method: æå–æ–¹æ³• ('uniform', 'clustering')
        Returns:
            skeleton_joints: (17, 3) éª¨æ¶å…³èŠ‚ç‚¹
        """
        if method == 'uniform':
            # å‡åŒ€é‡‡æ ·17ä¸ªç‚¹
            indices = np.linspace(0, len(points)-1, 17, dtype=int)
            return points[indices]
        elif method == 'clustering':
            # TODO: ä½¿ç”¨èšç±»æ–¹æ³•æå–å…³é”®ç‚¹
            # è¿™é‡Œç®€åŒ–ä¸ºå‡åŒ€é‡‡æ ·
            indices = np.linspace(0, len(points)-1, 17, dtype=int) 
            return points[indices]
        else:
            return points[:17]  # å–å‰17ä¸ªç‚¹
    
    def _plot_skeleton_structure(self, ax, points, title, show_skeleton=True, show_augmented=False):
        """
        ç»˜åˆ¶éª¨æ¶ç»“æ„ï¼Œå±•ç¤ºéª¨æ¶å¢å¼ºç­–ç•¥
        Args:
            ax: matplotlibè½´
            points: (650, 3) ç‚¹äº‘æ•°æ®
            title: æ ‡é¢˜
            show_skeleton: æ˜¯å¦æ˜¾ç¤ºéª¨æ¶è¿æ¥
            show_augmented: æ˜¯å¦æ˜¾ç¤ºå¢å¼ºæ’å€¼ç‚¹
        """
        # æå–17ä¸ªä¸»è¦å…³èŠ‚
        skeleton_joints = self._extract_skeleton_joints(points)
        connections = self._get_mmfi_skeleton_connections()
        joint_colors = self._get_joint_colors()
        
        if show_skeleton:
            # ç»˜åˆ¶å…³èŠ‚ç‚¹
            ax.scatter(skeleton_joints[:, 0], skeleton_joints[:, 1], skeleton_joints[:, 2],
                      c=joint_colors, s=100, alpha=0.9, edgecolors='black', linewidths=1,
                      label='Original Joints (17)')
            
            # ç»˜åˆ¶éª¨æ¶è¿æ¥çº¿
            for connection in connections:
                if connection[0] < len(skeleton_joints) and connection[1] < len(skeleton_joints):
                    start_joint = skeleton_joints[connection[0]]
                    end_joint = skeleton_joints[connection[1]]
                    ax.plot([start_joint[0], end_joint[0]], 
                           [start_joint[1], end_joint[1]], 
                           [start_joint[2], end_joint[2]], 
                           color='darkgray', alpha=0.8, linewidth=2, solid_capstyle='round')
        
        if show_augmented:
            # æ˜¾ç¤ºæ‰€æœ‰650ä¸ªç‚¹ï¼Œç”¨ä¸åŒé¢œè‰²åŒºåˆ†åŸå§‹å…³èŠ‚å’Œæ’å€¼ç‚¹
            augmented_points = points[17:]  # å‡è®¾å‰17ä¸ªæ˜¯å…³èŠ‚ç‚¹ï¼Œåé¢æ˜¯æ’å€¼ç‚¹
            
            # æ’å€¼ç‚¹ç”¨å°åœ†ç‚¹æ˜¾ç¤º
            ax.scatter(augmented_points[:, 0], augmented_points[:, 1], augmented_points[:, 2],
                      c='lightcoral', s=15, alpha=0.4, label=f'Interpolated Points ({len(augmented_points)})')
            
            # åŸå§‹å…³èŠ‚ç‚¹ç”¨å¤§åœ†ç‚¹æ˜¾ç¤º
            ax.scatter(skeleton_joints[:, 0], skeleton_joints[:, 1], skeleton_joints[:, 2],
                      c=joint_colors, s=80, alpha=0.9, edgecolors='black', linewidths=1,
                      label='Original Joints (17)')
        
        ax.set_title(title, fontsize=12, fontweight='bold')
        ax.set_xlabel('X')
        ax.set_ylabel('Y') 
        ax.set_zlabel('Z')
        
        # è®¾ç½®åæ ‡èŒƒå›´
        all_points = points.reshape(-1, 3)
        max_range = np.max(np.abs(all_points)) * 1.1
        ax.set_xlim([-max_range, max_range])
        ax.set_ylim([-max_range, max_range])
        ax.set_zlim([-max_range, max_range])
        
        # æ·»åŠ å›¾ä¾‹
        if show_skeleton or show_augmented:
            ax.legend(loc='upper right', fontsize=8)
    
    def _plot_overlay_comparison(self, ax, original, reconstructed, title):
        """ç»˜åˆ¶å åŠ å¯¹æ¯”å›¾"""
        # åŸå§‹ç‚¹äº‘ï¼ˆè“è‰²ï¼Œé€æ˜ï¼‰
        ax.scatter(original[:, 0], original[:, 1], original[:, 2],
                  c='blue', s=15, alpha=0.5, label='Original')
        
        # é‡å»ºç‚¹äº‘ï¼ˆçº¢è‰²ï¼Œé€æ˜ï¼‰
        ax.scatter(reconstructed[:, 0], reconstructed[:, 1], reconstructed[:, 2],
                  c='red', s=15, alpha=0.5, label='Reconstructed')
        
        ax.set_title(title, fontsize=12, fontweight='bold')
        ax.set_xlabel('X')
        ax.set_ylabel('Y')
        ax.set_zlabel('Z')
        ax.legend()
        
        # è®¾ç½®åæ ‡èŒƒå›´
        all_points = np.concatenate([original, reconstructed])
        max_range = np.max(np.abs(all_points)) * 1.1
        ax.set_xlim([-max_range, max_range])
        ax.set_ylim([-max_range, max_range])
        ax.set_zlim([-max_range, max_range])
    
    def _plot_error_analysis(self, ax, original, reconstructed):
        """ç»˜åˆ¶è¯¯å·®åˆ†æå›¾"""
        # è®¡ç®—é€ç‚¹è¯¯å·®
        errors = np.linalg.norm(original - reconstructed, axis=1)
        
        # ç»˜åˆ¶è¯¯å·®åˆ†å¸ƒç›´æ–¹å›¾
        ax.hist(errors, bins=30, alpha=0.7, color='skyblue', edgecolor='black')
        ax.set_xlabel('Point-wise L2 Error', fontsize=10)
        ax.set_ylabel('Frequency', fontsize=10)
        ax.set_title('Reconstruction Error Distribution', fontsize=12, fontweight='bold')
        ax.grid(True, alpha=0.3)
        
        # æ·»åŠ ç»Ÿè®¡ä¿¡æ¯
        mean_error = np.mean(errors)
        std_error = np.std(errors)
        max_error = np.max(errors)
        
        ax.axvline(mean_error, color='red', linestyle='--', alpha=0.8, 
                  label=f'Mean: {mean_error:.4f}')
        ax.axvline(mean_error + std_error, color='orange', linestyle='--', alpha=0.8,
                  label=f'Mean+Std: {mean_error + std_error:.4f}')
        
        ax.legend(fontsize=8)
        
        # åœ¨æ ‡é¢˜ä¸­æ·»åŠ å…³é”®ç»Ÿè®¡ä¿¡æ¯
        ax.text(0.02, 0.95, f'Max: {max_error:.4f}\nStd: {std_error:.4f}', 
                transform=ax.transAxes, verticalalignment='top',
                bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.8),
                fontsize=8)
    
    def _dict_to_namespace(self, config_dict):
        """
        å°†å­—å…¸è½¬æ¢ä¸ºå…·æœ‰å±æ€§è®¿é—®çš„å¯¹è±¡
        Args:
            config_dict: é…ç½®å­—å…¸
        Returns:
            å…·æœ‰å±æ€§è®¿é—®çš„é…ç½®å¯¹è±¡
        """
        from types import SimpleNamespace
        
        if isinstance(config_dict, dict):
            namespace = SimpleNamespace()
            for key, value in config_dict.items():
                if isinstance(value, dict):
                    setattr(namespace, key, self._dict_to_namespace(value))
                else:
                    setattr(namespace, key, value)
            return namespace
        else:
            return config_dict
    def _build_dataset_config(self, dataset_config):
        """
        æ„å»ºæ•°æ®é›†é…ç½®ï¼Œå¤„ç†_base_å¼•ç”¨ï¼Œå¹¶è½¬æ¢ä¸ºå­—å…¸æ ¼å¼
        Args:
            dataset_config: åŸå§‹é…ç½®
        Returns:
            å¤„ç†åçš„é…ç½®å¯¹è±¡ï¼ˆå…·æœ‰å±æ€§è®¿é—®ï¼‰
        """
        if hasattr(dataset_config, '_base_'):
            # å¤„ç†_base_å¼•ç”¨
            base_config = dataset_config._base_
            
            # åˆ›å»ºå­—å…¸é…ç½®
            config_dict = {}
            
            # å¤åˆ¶åŸºç¡€é…ç½®
            if hasattr(base_config, '__dict__'):
                config_dict.update(base_config.__dict__)
            else:
                config_dict.update(base_config)
            
            # æ·»åŠ å…¶ä»–é…ç½®
            if hasattr(dataset_config, 'others'):
                if hasattr(dataset_config.others, '__dict__'):
                    config_dict.update(dataset_config.others.__dict__)
                else:
                    config_dict.update(dataset_config.others)
            
            # è½¬æ¢ä¸ºå…·æœ‰å±æ€§è®¿é—®çš„å¯¹è±¡
            return self._dict_to_namespace(config_dict)
        else:
            # å¦‚æœå·²ç»æ˜¯æ­£ç¡®çš„æ ¼å¼ï¼Œç›´æ¥è¿”å›
            return dataset_config
    
    def visualize_dataset_samples(self, num_samples=5, save_dir=None):
        """
        å¯è§†åŒ–æ•°æ®é›†ä¸­çš„å¤šä¸ªæ ·æœ¬
        Args:
            num_samples: æ ·æœ¬æ•°é‡
            save_dir: ä¿å­˜ç›®å½•
        """
        # ç›´æ¥åˆ›å»ºMMFIæ•°æ®é›†å®ä¾‹ï¼Œç»•è¿‡æ³¨å†Œç³»ç»Ÿ
        try:
            # æ„å»ºæ•°æ®é›†é…ç½®
            if hasattr(self.config, 'dataset') and hasattr(self.config.dataset, 'train'):
                dataset_config = self.config.dataset.train
                # å¤„ç†_base_å¼•ç”¨
                dataset_config = self._build_dataset_config(dataset_config)
            else:
                # ä½¿ç”¨é»˜è®¤çš„MMFIé…ç½®
                print("âš ï¸ No dataset config found, using default MMFI config")
                dataset_config = self._dict_to_namespace({
                    'NAME': 'MMFI',
                    'DATA_PATH': 'data/MMFI',
                    'N_POINTS': 650,
                    'subset': 'train'
                })
            
            # ç›´æ¥åˆ›å»ºMMFIæ•°æ®é›†å®ä¾‹
            from datasets.MMFIDataset import MMFIDataset
            dataset = MMFIDataset(dataset_config)
            print(f"ğŸ“¦ Dataset size: {len(dataset)}")
            
        except Exception as e:
            print(f"âŒ Failed to create dataset: {e}")
            return []
        
        if save_dir:
            os.makedirs(save_dir, exist_ok=True)
        
        results = []
        for i in range(min(num_samples, len(dataset))):
            print(f"\nğŸ” Processing sample {i+1}/{num_samples}")
            
            # è·å–æ•°æ®æ ·æœ¬
            data = dataset[i]
            if isinstance(data, dict):
                points = data['points'] if 'points' in data else data['pos']
            elif isinstance(data, (list, tuple)) and len(data) >= 2:
                # MMFIDataset è¿”å› (taxonomy_id, model_id, data)
                points = data[2] if len(data) > 2 else data[1]
            else:
                points = data
            
            print(f"ğŸ“ Sample shape: {points.shape}")
            
            # å¯è§†åŒ–
            save_path = os.path.join(save_dir, f'sample_{i+1}_reconstruction.png') if save_dir else None
            result = self.visualize_single_sample(points, save_path)
            results.append(result)
        
        # ç»Ÿè®¡ä¿¡æ¯
        mse_values = [r['mse_fine'] for r in results]
        print(f"\nğŸ“ˆ Reconstruction MSE Statistics:")
        print(f"   Mean: {np.mean(mse_values):.6f}")
        print(f"   Std:  {np.std(mse_values):.6f}")
        print(f"   Min:  {np.min(mse_values):.6f}")
        print(f"   Max:  {np.max(mse_values):.6f}")
        
        return results
    
    def visualize_codebook_usage(self, num_samples=100, save_path=None):
        """
        åˆ†æç æœ¬ä½¿ç”¨æƒ…å†µ
        Args:
            num_samples: åˆ†æçš„æ ·æœ¬æ•°é‡
            save_path: ä¿å­˜è·¯å¾„
        """
        # ç›´æ¥åˆ›å»ºMMFIæ•°æ®é›†å®ä¾‹
        try:
            # æ„å»ºæ•°æ®é›†é…ç½®
            if hasattr(self.config, 'dataset') and hasattr(self.config.dataset, 'train'):
                dataset_config = self.config.dataset.train
                # å¤„ç†_base_å¼•ç”¨
                dataset_config = self._build_dataset_config(dataset_config)
            else:
                # ä½¿ç”¨é»˜è®¤çš„MMFIé…ç½®
                print("âš ï¸ No dataset config found, using default MMFI config")
                dataset_config = self._dict_to_namespace({
                    'NAME': 'MMFI',
                    'DATA_PATH': 'data/MMFI',
                    'N_POINTS': 650,
                    'subset': 'train'
                })
            
            # ç›´æ¥åˆ›å»ºMMFIæ•°æ®é›†å®ä¾‹
            from datasets.MMFIDataset import MMFIDataset
            dataset = MMFIDataset(dataset_config)
            
        except Exception as e:
            print(f"âŒ Failed to create dataset: {e}")
            return {}
        
        vq_indices = []
        print(f"ğŸ”¢ Analyzing codebook usage with {num_samples} samples...")
        
        for i in range(min(num_samples, len(dataset))):
            data = dataset[i]
            if isinstance(data, dict):
                points = data['points'] if 'points' in data else data['pos']
            elif isinstance(data, (list, tuple)) and len(data) >= 2:
                # MMFIDataset è¿”å› (taxonomy_id, model_id, data)
                points = data[2] if len(data) > 2 else data[1]
            else:
                points = data
            
            if isinstance(points, np.ndarray):
                points = torch.from_numpy(points).float()
            
            if len(points.shape) == 2:
                points = points.unsqueeze(0)
            
            points = points.to(self.device)
            
            with torch.no_grad():
                _, _, encoding_indices = self.model(points)
                vq_indices.append(encoding_indices[0].item())
        
        # ç»Ÿè®¡åˆ†æ
        vq_indices = np.array(vq_indices)
        unique_indices = np.unique(vq_indices)
        
        print(f"ğŸ“Š Codebook Usage Statistics:")
        print(f"   Total codes used: {len(unique_indices)}/{self.model.codebook_size}")
        print(f"   Usage rate: {len(unique_indices)/self.model.codebook_size*100:.2f}%")
        print(f"   Most frequent code: {np.bincount(vq_indices).argmax()}")
        
        # ç»˜åˆ¶ç›´æ–¹å›¾
        plt.figure(figsize=(12, 6))
        plt.hist(vq_indices, bins=min(50, len(unique_indices)), alpha=0.7, edgecolor='black')
        plt.title(f'VQ Code Usage Distribution ({num_samples} samples)')
        plt.xlabel('VQ Code Index')
        plt.ylabel('Frequency')
        plt.grid(True, alpha=0.3)
        
        if save_path:
            os.makedirs(os.path.dirname(save_path), exist_ok=True)
            plt.savefig(save_path, dpi=150, bbox_inches='tight')
            print(f"ğŸ’¾ Saved codebook analysis to {save_path}")
        
        plt.show()
        
        return {
            'indices': vq_indices,
            'unique_count': len(unique_indices),
            'usage_rate': len(unique_indices)/self.model.codebook_size
        }
    
    def visualize_skeleton_augmentation_strategy(self, data_sample, save_path=None):
        """
        ä¸“é—¨å¯è§†åŒ–éª¨æ¶å¢å¼ºç­–ç•¥çš„è¯¦ç»†è¿‡ç¨‹
        Args:
            data_sample: (650, 3) ç‚¹äº‘æ•°æ®
            save_path: ä¿å­˜è·¯å¾„
        """
        if isinstance(data_sample, np.ndarray):
            points = data_sample
        else:
            points = data_sample.cpu().numpy() if hasattr(data_sample, 'cpu') else data_sample
        
        # åˆ›å»ºè¯¦ç»†çš„éª¨æ¶å¢å¼ºç­–ç•¥å¯è§†åŒ–
        fig = plt.figure(figsize=(20, 15))
        
        # 1. åŸå§‹17å…³èŠ‚éª¨æ¶
        ax1 = fig.add_subplot(231, projection='3d')
        skeleton_joints = self._extract_skeleton_joints(points)
        self._plot_skeleton_only(ax1, skeleton_joints, 'Step 1: Original 17 Joints\n(Human Skeleton Structure)')
        
        # 2. éª¨æ¶è¿æ¥çº¿
        ax2 = fig.add_subplot(232, projection='3d')
        self._plot_skeleton_with_connections(ax2, skeleton_joints, 'Step 2: Skeleton Connections\n(16 bone connections)')
        
        # 3. æ’å€¼ç‚¹ç”Ÿæˆç¤ºä¾‹ï¼ˆæ˜¾ç¤ºå‡ æ¡è¿æ¥çš„æ’å€¼ï¼‰
        ax3 = fig.add_subplot(233, projection='3d')
        self._plot_interpolation_demo(ax3, skeleton_joints, 'Step 3: Interpolation Strategy\n(Adding points along connections)')
        
        # 4. å®Œæ•´çš„å¢å¼ºç‚¹äº‘
        ax4 = fig.add_subplot(234, projection='3d')
        self._plot_skeleton_structure(ax4, points, 'Step 4: Complete Augmented Cloud\n(650 points total)',
                                     show_skeleton=True, show_augmented=True)
        
        # 5. å¯†åº¦åˆ†æ
        ax5 = fig.add_subplot(235, projection='3d')
        self._plot_density_analysis(ax5, points, skeleton_joints, 'Step 5: Point Density Analysis\n(Distribution along skeleton)')
        
        # 6. ç»Ÿè®¡åˆ†æ
        ax6 = fig.add_subplot(236)
        self._plot_augmentation_statistics(ax6, points, skeleton_joints)
        
        plt.tight_layout()
        
        if save_path:
            os.makedirs(os.path.dirname(save_path), exist_ok=True)
            plt.savefig(save_path, dpi=150, bbox_inches='tight')
            print(f"ğŸ’¾ Saved skeleton augmentation analysis to {save_path}")
        
        plt.show()
        
        # è¯¦ç»†ç»Ÿè®¡ä¿¡æ¯
        print(f"ğŸ¦´ Skeleton Augmentation Analysis:")
        print(f"   Original joints: 17")
        print(f"   Total points after augmentation: {len(points)}")
        print(f"   Interpolated points: {len(points) - 17}")
        print(f"   Augmentation ratio: {(len(points) - 17) / 17:.1f}x")
        print(f"   Average points per connection: {(len(points) - 17) / 16:.1f}")
        
        return {
            'original_joints': skeleton_joints,
            'total_points': len(points),
            'augmented_points': len(points) - 17,
            'augmentation_ratio': (len(points) - 17) / 17
        }
    
    def _plot_skeleton_only(self, ax, skeleton_joints, title):
        """åªç»˜åˆ¶éª¨æ¶å…³èŠ‚ç‚¹"""
        joint_colors = self._get_joint_colors()
        
        ax.scatter(skeleton_joints[:, 0], skeleton_joints[:, 1], skeleton_joints[:, 2],
                  c=joint_colors, s=120, alpha=0.9, edgecolors='black', linewidths=2)
        
        # æ·»åŠ å…³èŠ‚ç¼–å·
        for i, joint in enumerate(skeleton_joints):
            ax.text(joint[0], joint[1], joint[2], f'J{i}', 
                   fontsize=8, fontweight='bold', ha='center')
        
        ax.set_title(title, fontsize=12, fontweight='bold')
        ax.set_xlabel('X')
        ax.set_ylabel('Y')
        ax.set_zlabel('Z')
        self._set_axis_limits(ax, skeleton_joints)
    
    def _plot_skeleton_with_connections(self, ax, skeleton_joints, title):
        """ç»˜åˆ¶å¸¦è¿æ¥çº¿çš„éª¨æ¶"""
        joint_colors = self._get_joint_colors()
        connections = self._get_mmfi_skeleton_connections()
        
        # ç»˜åˆ¶å…³èŠ‚ç‚¹
        ax.scatter(skeleton_joints[:, 0], skeleton_joints[:, 1], skeleton_joints[:, 2],
                  c=joint_colors, s=100, alpha=0.9, edgecolors='black', linewidths=1)
        
        # ç»˜åˆ¶è¿æ¥çº¿ï¼Œç”¨ä¸åŒé¢œè‰²è¡¨ç¤ºä¸åŒçš„è¿æ¥
        connection_colors = ['#E74C3C', '#3498DB', '#27AE60', '#F39C12', '#9B59B6', 
                           '#E67E22', '#1ABC9C', '#34495E']
        
        for i, connection in enumerate(connections):
            if connection[0] < len(skeleton_joints) and connection[1] < len(skeleton_joints):
                start_joint = skeleton_joints[connection[0]]
                end_joint = skeleton_joints[connection[1]]
                color = connection_colors[i % len(connection_colors)]
                
                ax.plot([start_joint[0], end_joint[0]], 
                       [start_joint[1], end_joint[1]], 
                       [start_joint[2], end_joint[2]], 
                       color=color, alpha=0.8, linewidth=3, solid_capstyle='round')
        
        ax.set_title(title, fontsize=12, fontweight='bold')
        ax.set_xlabel('X')
        ax.set_ylabel('Y')
        ax.set_zlabel('Z')
        self._set_axis_limits(ax, skeleton_joints)
        
        # æ·»åŠ è¿æ¥æ•°é‡ä¿¡æ¯
        ax.text2D(0.02, 0.98, f'{len(connections)} connections', 
                 transform=ax.transAxes, fontsize=10, fontweight='bold',
                 bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.8))
    
    def _plot_interpolation_demo(self, ax, skeleton_joints, title):
        """æ¼”ç¤ºæ’å€¼è¿‡ç¨‹"""
        connections = self._get_mmfi_skeleton_connections()
        joint_colors = self._get_joint_colors()
        
        # ç»˜åˆ¶å…³èŠ‚ç‚¹
        ax.scatter(skeleton_joints[:, 0], skeleton_joints[:, 1], skeleton_joints[:, 2],
                  c=joint_colors, s=80, alpha=0.9, edgecolors='black', linewidths=1)
        
        # é€‰æ‹©å‡ æ¡ä»£è¡¨æ€§è¿æ¥è¿›è¡Œæ’å€¼æ¼”ç¤º
        demo_connections = connections[:5]  # å‰5ä¸ªè¿æ¥
        interpolation_points_per_connection = 37  # å¹³å‡æ¯ä¸ªè¿æ¥çš„æ’å€¼ç‚¹æ•°
        
        for i, connection in enumerate(demo_connections):
            start_joint = skeleton_joints[connection[0]]
            end_joint = skeleton_joints[connection[1]]
            
            # åœ¨è¿æ¥çº¿ä¸Šç”Ÿæˆæ’å€¼ç‚¹
            t_values = np.linspace(0, 1, interpolation_points_per_connection)
            interpolated_points = np.array([
                start_joint + t * (end_joint - start_joint) for t in t_values
            ])
            
            # ç»˜åˆ¶è¿æ¥çº¿
            ax.plot([start_joint[0], end_joint[0]], 
                   [start_joint[1], end_joint[1]], 
                   [start_joint[2], end_joint[2]], 
                   color='gray', alpha=0.6, linewidth=2)
            
            # ç»˜åˆ¶æ’å€¼ç‚¹
            ax.scatter(interpolated_points[:, 0], interpolated_points[:, 1], interpolated_points[:, 2],
                      c='red', s=20, alpha=0.7, label=f'Interpolated' if i == 0 else "")
        
        ax.set_title(title, fontsize=12, fontweight='bold')
        ax.set_xlabel('X')
        ax.set_ylabel('Y')
        ax.set_zlabel('Z')
        self._set_axis_limits(ax, skeleton_joints)
        
        if len(demo_connections) > 0:
            ax.legend(loc='upper right', fontsize=10)
    
    def _plot_density_analysis(self, ax, all_points, skeleton_joints, title):
        """ç»˜åˆ¶ç‚¹å¯†åº¦åˆ†æ"""
        # è®¡ç®—æ¯ä¸ªç‚¹åˆ°æœ€è¿‘éª¨æ¶å…³èŠ‚çš„è·ç¦»
        distances_to_skeleton = []
        for point in all_points:
            min_dist = min([np.linalg.norm(point - joint) for joint in skeleton_joints])
            distances_to_skeleton.append(min_dist)
        
        distances_to_skeleton = np.array(distances_to_skeleton)
        
        # ç”¨é¢œè‰²ç¼–ç è¡¨ç¤ºå¯†åº¦ï¼ˆè·ç¦»éª¨æ¶çš„è¿œè¿‘ï¼‰
        scatter = ax.scatter(all_points[:, 0], all_points[:, 1], all_points[:, 2],
                           c=distances_to_skeleton, s=15, alpha=0.6, cmap='viridis')
        
        # é«˜äº®åŸå§‹éª¨æ¶å…³èŠ‚
        ax.scatter(skeleton_joints[:, 0], skeleton_joints[:, 1], skeleton_joints[:, 2],
                  c='red', s=80, alpha=1.0, edgecolors='white', linewidths=2,
                  label='Original Joints')
        
        ax.set_title(title, fontsize=12, fontweight='bold')
        ax.set_xlabel('X')
        ax.set_ylabel('Y')
        ax.set_zlabel('Z')
        ax.legend()
        self._set_axis_limits(ax, all_points)
        
        # æ·»åŠ é¢œè‰²æ¡
        plt.colorbar(scatter, ax=ax, shrink=0.6, label='Distance to Skeleton')
    
    def _plot_augmentation_statistics(self, ax, all_points, skeleton_joints):
        """ç»˜åˆ¶å¢å¼ºç­–ç•¥ç»Ÿè®¡ä¿¡æ¯"""
        # è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
        total_points = len(all_points)
        original_joints = len(skeleton_joints)
        augmented_points = total_points - original_joints
        connections = len(self._get_mmfi_skeleton_connections())
        avg_points_per_connection = augmented_points / connections
        
        # åˆ›å»ºç»Ÿè®¡å›¾è¡¨
        categories = ['Original\nJoints', 'Augmented\nPoints', 'Total\nPoints']
        values = [original_joints, augmented_points, total_points]
        colors = ['#E74C3C', '#3498DB', '#27AE60']
        
        bars = ax.bar(categories, values, color=colors, alpha=0.7, edgecolor='black')
        
        # æ·»åŠ æ•°å€¼æ ‡ç­¾
        for bar, value in zip(bars, values):
            ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + max(values)*0.01,
                   str(value), ha='center', va='bottom', fontweight='bold')
        
        ax.set_title('Skeleton Augmentation Statistics', fontsize=12, fontweight='bold')
        ax.set_ylabel('Number of Points')
        ax.grid(True, alpha=0.3, axis='y')
        
        # æ·»åŠ è¯¦ç»†ç»Ÿè®¡ä¿¡æ¯
        stats_text = f"""Augmentation Details:
â€¢ Connections: {connections}
â€¢ Avg points/connection: {avg_points_per_connection:.1f}
â€¢ Augmentation ratio: {augmented_points/original_joints:.1f}x
â€¢ Density increase: {total_points/original_joints:.1f}x"""
        
        ax.text(0.02, 0.98, stats_text, transform=ax.transAxes, 
               verticalalignment='top', fontfamily='monospace', fontsize=9,
               bbox=dict(boxstyle='round', facecolor='lightgray', alpha=0.8))
    
    def _set_axis_limits(self, ax, points):
        """è®¾ç½®è½´é™åˆ¶"""
        max_range = np.max(np.abs(points)) * 1.1
        ax.set_xlim([-max_range, max_range])
        ax.set_ylim([-max_range, max_range])
        ax.set_zlim([-max_range, max_range])


def main():
    parser = argparse.ArgumentParser(description='AdaptiveSkeletonDVAE Visualization Tool')
    parser.add_argument('--config', type=str, required=True,
                       help='Path to config file')
    parser.add_argument('--checkpoint', type=str,
                       help='Path to model checkpoint')
    parser.add_argument('--mode', type=str, 
                       choices=['single', 'dataset', 'codebook', 'skeleton_strategy'], 
                       default='dataset',
                       help='Visualization mode')
    parser.add_argument('--num_samples', type=int, default=5,
                       help='Number of samples to visualize')
    parser.add_argument('--save_dir', type=str, default='./visualizations/adaptive_dvae',
                       help='Save directory for outputs')
    parser.add_argument('--device', type=str, default='cuda',
                       help='Device to use')
    
    args = parser.parse_args()
    
    # åˆ›å»ºå¯è§†åŒ–å™¨
    visualizer = AdaptiveSkeletonDVAEVisualizer(
        config_path=args.config,
        checkpoint_path=args.checkpoint,
        device=args.device
    )
    
    if args.mode == 'dataset':
        # å¯è§†åŒ–æ•°æ®é›†æ ·æœ¬
        print(f"ğŸ¨ Visualizing {args.num_samples} dataset samples...")
        visualizer.visualize_dataset_samples(
            num_samples=args.num_samples,
            save_dir=args.save_dir
        )
    
    elif args.mode == 'codebook':
        # åˆ†æç æœ¬ä½¿ç”¨æƒ…å†µ
        print("ğŸ“ˆ Analyzing codebook usage...")
        save_path = os.path.join(args.save_dir, 'codebook_usage.png')
        visualizer.visualize_codebook_usage(
            num_samples=args.num_samples,
            save_path=save_path
        )
    
    elif args.mode == 'skeleton_strategy':
        # å¯è§†åŒ–éª¨æ¶å¢å¼ºç­–ç•¥
        print("ğŸ¦´ Analyzing skeleton augmentation strategy...")
        
        # è·å–ä¸€ä¸ªæ ·æœ¬è¿›è¡Œæ¼”ç¤º
        try:
            # æ„å»ºæ•°æ®é›†é…ç½®
            if hasattr(visualizer.config, 'dataset') and hasattr(visualizer.config.dataset, 'train'):
                dataset_config = visualizer.config.dataset.train
                dataset_config = visualizer._build_dataset_config(dataset_config)
            else:
                dataset_config = visualizer._dict_to_namespace({
                    'NAME': 'MMFI',
                    'DATA_PATH': 'data/MMFI',
                    'N_POINTS': 650,
                    'subset': 'train'
                })
            
            # åˆ›å»ºæ•°æ®é›†å®ä¾‹
            from datasets.MMFIDataset import MMFIDataset
            dataset = MMFIDataset(dataset_config)
            
            # è·å–ç¬¬ä¸€ä¸ªæ ·æœ¬è¿›è¡Œæ¼”ç¤º
            data = dataset[0]
            if isinstance(data, dict):
                points = data['points'] if 'points' in data else data['pos']
            elif isinstance(data, (list, tuple)) and len(data) >= 2:
                points = data[2] if len(data) > 2 else data[1]
            else:
                points = data
            
            # å¯è§†åŒ–éª¨æ¶å¢å¼ºç­–ç•¥
            save_path = os.path.join(args.save_dir, 'skeleton_augmentation_strategy.png')
            visualizer.visualize_skeleton_augmentation_strategy(points, save_path)
            
        except Exception as e:
            print(f"âŒ Failed to analyze skeleton strategy: {e}")
            print("Using synthetic skeleton data for demonstration...")
            
            # åˆ›å»ºåˆæˆçš„éª¨æ¶æ•°æ®è¿›è¡Œæ¼”ç¤º
            synthetic_skeleton = np.random.randn(650, 3) * 0.5
            save_path = os.path.join(args.save_dir, 'skeleton_augmentation_demo.png')
            visualizer.visualize_skeleton_augmentation_strategy(synthetic_skeleton, save_path)
    
    elif args.mode == 'single':
        # å•æ ·æœ¬å¯è§†åŒ– - éœ€è¦æ‰‹åŠ¨æä¾›æ•°æ®
        print("âš ï¸ Single mode requires manual data input")
        print("   Use visualizer.visualize_single_sample(your_data) directly")
        print("   Or try skeleton_strategy mode to see augmentation analysis")


if __name__ == '__main__':
    main()
