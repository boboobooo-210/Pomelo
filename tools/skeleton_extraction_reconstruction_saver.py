#!/usr/bin/env python3
"""
å»é™¤å¯è§†åŒ–çš„ MARS -> NTU æå– + GCN é‡æ„æ‰¹å¤„ç†è„šæœ¬

åŠŸèƒ½ï¼š
- æ‰¹é‡è¯»å– MARS çš„é›·è¾¾ç‰¹å¾å›¾ npy æ–‡ä»¶ï¼ˆtrain/test/validateï¼‰
- ä½¿ç”¨ MARSTransformerModel æå– MARS (19 joints)
- ä½¿ç”¨ SkeletonJointMapper/EnhancedSkeletonMapper è½¬æ¢ä¸º NTU 25 å…³èŠ‚
- ä½¿ç”¨å·²è®­ç»ƒçš„ GCN é‡æ„å™¨å¾—åˆ°é‡æ„éª¨æ¶å’Œ token_sequence
- å°†é‡æ„éª¨æ¶ã€token_sequenceã€vq_lossã€ï¼ˆå¯é€‰ï¼‰åŸå§‹æå–éª¨æ¶æŒ‰ç´¢å¼•ç»‘å®šå¹¶ä¿å­˜ä¸ºå‹ç¼© npz

è¾“å‡ºæ ¼å¼ï¼ˆå•æ–‡ä»¶ï¼‰ï¼š
  <out_dir>/<split>_recon.npz åŒ…å«ï¼š
    - reconstructed: float32 array (N, 25, 3)  # å·²ç»æ˜¯ [x,y,z] æ ¼å¼ï¼ˆæ ‡å‡†åŒ–æˆ–æ¨¡å‹è¾“å‡ºï¼Œè§ä¸‹ï¼‰
    - token_sequences: int32 array (N, T) æˆ– (N,)
    - vq_losses: float32 array (N,)
    - extracted: float32 array (N, 25, 3)  # å¯é€‰ï¼šæå–å™¨çš„è¾“å‡ºï¼ˆNTUæ ¼å¼ï¼‰
    - metadata: JSON å­—ç¬¦ä¸²ï¼ˆN æ¡å…ƒä¿¡æ¯ï¼‰

æ­¤å¤–å¯é€‰ per-sample å‹ç¼©æ–‡ä»¶ï¼ˆæ¯æ ·æœ¬ä¸€ä¸ª .npzï¼‰ï¼Œå¦‚éœ€ä¸‹æ¸¸æŒ‰æ ·æœ¬åŠ è½½æ›´æ–¹ä¾¿ã€‚

ä½¿ç”¨ç¤ºä¾‹ï¼š
  python tools/skeleton_extraction_reconstruction_saver.py \
    --extractor mars_transformer_best.pth \
    --gcn_ckpt experiments/gcn_skeleton_memory_optimized/NTU_models/default/ckpt-best.pth \
    --gcn_cfg cfgs/NTU_models/gcn_skeleton_memory_optimized.yaml \
    --out_dir data/MARS_recon_tokens \
    --batch_size 32

"""

import os
import sys
import json
from pathlib import Path
import argparse
import numpy as np

# æŠŠé¡¹ç›®æ ¹åŠ å…¥è·¯å¾„ï¼Œä¾¿äºå¯¼å…¥ models ç­‰
project_root = Path(__file__).resolve().parents[1]
sys.path.append(str(project_root))

# å»¶è¿Ÿå¯¼å…¥ torch å¹¶ç»™å‡ºå‹å¥½ä¿¡æ¯
try:
    import torch
except Exception as e:
    print("âŒ éœ€è¦ PyTorch æ‰èƒ½è¿è¡Œæ­¤è„šæœ¬ï¼Œè¯·å…ˆå®‰è£… torchï¼š", e)
    sys.exit(1)

# å¯¼å…¥æ¨¡å‹ä¸æ˜ å°„å™¨
try:
    from models.skeleton_extractor import MARSTransformerModel
    from models.GCNSkeletonTokenizer import GCNSkeletonTokenizer
    from utils.config import cfg_from_yaml_file
    from models.skeleton_joint_mapper import SkeletonJointMapper, EnhancedSkeletonMapper
except Exception as e:
    print("âŒ å¯¼å…¥é¡¹ç›®å†…æ¨¡å—å¤±è´¥ï¼š", e)
    raise

# è¿›åº¦æ¡
try:
    from tqdm import tqdm
except Exception:
    tqdm = lambda x: x


class SkeletonReconstructionSaver:
    def __init__(self, extractor_ckpt, gcn_ckpt, gcn_cfg, device=None, use_enhanced_mapper=True):
        self.device = torch.device(device if device is not None else ('cuda' if torch.cuda.is_available() else 'cpu'))
        print(f"Using device: {self.device}")

        # æ˜ å°„å™¨
        if use_enhanced_mapper:
            self.joint_mapper = EnhancedSkeletonMapper().to(self.device)
        else:
            self.joint_mapper = SkeletonJointMapper().to(self.device)

        # åŠ è½½æ¨¡å‹
        self.skeleton_extractor = self._load_skeleton_extractor(extractor_ckpt)
        self.gcn_reconstructor = self._load_gcn_reconstructor(gcn_ckpt, gcn_cfg)

    def _load_skeleton_extractor(self, model_path):
        """åŠ è½½MARSéª¨æ¶æå–å™¨ - æ”¯æŒå¤šå°ºåº¦ç‰¹å¾èåˆï¼ˆè‡ªåŠ¨æ£€æµ‹ï¼‰"""
        print(f"Loading skeleton extractor: {model_path}")
        
        # å°è¯•åŠ è½½æƒé‡ä»¥æ£€æµ‹æ¨¡å‹ç±»å‹
        state_dict = torch.load(model_path, map_location=self.device)
        
        # æ£€æµ‹æ˜¯å¦ä¸ºå¤šå°ºåº¦æ¨¡å‹ï¼ˆé€šè¿‡ç¬¬ä¸€å±‚Linearçš„è¾“å…¥ç»´åº¦åˆ¤æ–­ï¼‰
        first_linear_key = 'regression_head.feature_projection.0.weight'
        
        if first_linear_key in state_dict:
            input_dim = state_dict[first_linear_key].shape[1]  # (out_features, in_features)
            is_multi_scale = (input_dim == 448)
            
            if is_multi_scale:
                print("ğŸ” æ£€æµ‹åˆ°å¤šå°ºåº¦æ¨¡å‹ (448ç»´è¾“å…¥)")
                model = MARSTransformerModel(input_channels=5, output_dim=57, multi_scale=True)
            else:
                print("ğŸ” æ£€æµ‹åˆ°å•å°ºåº¦æ¨¡å‹ (256ç»´è¾“å…¥)")
                model = MARSTransformerModel(input_channels=5, output_dim=57, multi_scale=False)
        else:
            # å¦‚æœæ‰¾ä¸åˆ°å…³é”®å±‚ï¼Œé»˜è®¤å°è¯•å¤šå°ºåº¦ï¼ˆæ–°ç‰ˆæœ¬ï¼‰
            print("âš ï¸ æ— æ³•æ£€æµ‹æ¨¡å‹ç±»å‹ï¼Œé»˜è®¤ä½¿ç”¨å¤šå°ºåº¦æ¨¡å‹")
            model = MARSTransformerModel(input_channels=5, output_dim=57, multi_scale=True)
        
        # åŠ è½½æƒé‡
        try:
            model.load_state_dict(state_dict)
            print("âœ… æƒé‡åŠ è½½æˆåŠŸï¼")
        except RuntimeError as e:
            print(f"âŒ æƒé‡åŠ è½½å¤±è´¥: {e}")
            print("âš ï¸ å¯èƒ½æ˜¯æ¨¡å‹ç‰ˆæœ¬ä¸åŒ¹é…ï¼Œè¯·æ£€æŸ¥æƒé‡æ–‡ä»¶")
            raise
        
        model.to(self.device)
        model.eval()
        
        # è¾“å‡ºæ¨¡å‹ä¿¡æ¯
        total_params = sum(p.numel() for p in model.parameters())
        print(f"ğŸ“Š æ¨¡å‹å‚æ•°: {total_params:,}")
        if hasattr(model, 'backbone'):
            print(f"ğŸ“Š Backboneè¾“å‡ºç»´åº¦: {model.backbone.output_dim}")
        
        return model

    def _load_gcn_reconstructor(self, model_path, config_path):
        print(f"Loading GCN reconstructor: {model_path}")
        try:
            cfg = cfg_from_yaml_file(config_path)
            model = GCNSkeletonTokenizer(cfg.model)

            checkpoint = torch.load(model_path, map_location=self.device)
            state_dict = checkpoint.get('base_model', checkpoint)
            # å»æ‰ possible 'module.' å‰ç¼€
            new_state = {}
            for k, v in state_dict.items():
                if k.startswith('module.'):
                    new_state[k[7:]] = v
                else:
                    new_state[k] = v
            model.load_state_dict(new_state)
            model.to(self.device)
            model.eval()
            return model
        except Exception as e:
            print("âŒ åŠ è½½ GCN é‡æ„å™¨å¤±è´¥ï¼š", e)
            return None

    def _extract_batch(self, radar_batch):
        """ radar_batch: np.array (B, H, W, C) or (B, C, H, W) """
        # è½¬ä¸º tensor (B, C, H, W)
        if len(radar_batch.shape) == 4 and radar_batch.shape[-1] in (1,3,5):
            tensor = torch.from_numpy(radar_batch.transpose(0, 3, 1, 2)).float().to(self.device)
        elif len(radar_batch.shape) == 4 and radar_batch.shape[1] in (1,3,5):
            tensor = torch.from_numpy(radar_batch).float().to(self.device)
        else:
            # å°è¯•å¤„ç† (H,W,C) å•æ ·æœ¬
            if len(radar_batch.shape) == 3:
                tensor = torch.from_numpy(radar_batch.transpose(2,0,1)).unsqueeze(0).float().to(self.device)
            else:
                raise ValueError(f"Unexpected radar batch shape: {radar_batch.shape}")

        with torch.no_grad():
            out = self.skeleton_extractor(tensor)
            # è¾“å‡ºå¯èƒ½æ˜¯ (B,57) æˆ– (B,19,3)
            if len(out.shape) == 2 and out.shape[1] == 57:
                # ä¿ç•™åŸå§‹ flat 57 è¾“å‡ºä¾›æ˜ å°„å™¨ä½¿ç”¨
                flat = out
                # ä½†ä¹Ÿæ„é€  (B,19,3)
                x = out[:, 0:19]
                y = out[:, 19:38]
                z = out[:, 38:57]
                mars_skel = torch.stack([x, y, z], dim=-1)  # (B,19,3)
            else:
                # å‡è®¾æ˜¯ (B,19,3)
                mars_skel = out
                flat = out.view(out.shape[0], -1)

            # è½¬æ¢ä¸º NTU 25 joints via mapper (joint_mapper æ¥å— flat 57 æˆ– tensor)
            try:
                ntu = self.joint_mapper(flat)
            except Exception:
                # æœ‰äº›æ˜ å°„å™¨å¯èƒ½éœ€è¦ CPU tensor
                ntu = self.joint_mapper(flat.to(self.device))

            return {
                'mars_flat': flat.cpu(),            # (B,57)
                'mars_skel': mars_skel.cpu(),      # (B,19,3)
                'ntu_skel': ntu.cpu()              # (B,25,3)
            }

    def _reconstruct_batch(self, ntu_batch):
        """ntubatch: np.array or torch tensor shape (B,25,3) - the method will normalize and feed GCN"""
        if self.gcn_reconstructor is None:
            raise RuntimeError("GCN reconstructor is not loaded")

        # normalize according to training expectation: convert (x,y,z)->(x,z,y) then normalize
        if isinstance(ntu_batch, torch.Tensor):
            ntu_np = ntu_batch.cpu().numpy()
        else:
            ntu_np = np.array(ntu_batch)

        # apply same normalization as pipeline: expects (B,25,3) in x,y,z -> convert to x,z,y then standardize per sample
        xzy = ntu_np[:, :, [0, 2, 1]]  # [x,y,z] -> [x,z,y]

        # normalization per-sample
        normalized = []
        for i in range(xzy.shape[0]):
            s = xzy[i]
            centroid = np.mean(s, axis=0)
            centered = s - centroid
            dists = np.sqrt(np.sum(centered**2, axis=1))
            maxd = np.max(dists)
            if maxd > 0:
                normalized.append(centered / maxd)
            else:
                normalized.append(centered)
        normalized = np.array(normalized).astype(np.float32)

        tensor_in = torch.from_numpy(normalized).to(self.device)
        with torch.no_grad():
            out = self.gcn_reconstructor(tensor_in, return_recon=True)

        # out expects keys: 'reconstructed' (B,25,3?) (in x,z,y or x,y,z depending model) and 'token_sequence' etc.
        recon = out['reconstructed'].cpu().numpy()
        tokens = out['token_sequence'].cpu().numpy()
        vq_loss = float(out.get('vq_loss', 0.0)) if not isinstance(out.get('vq_loss', 0.0), torch.Tensor) else out['vq_loss'].item()
        
        # æ–°å¢ï¼šè·å– base_reconstructed å’Œ residual_scaleï¼ˆç”¨äºè¯Šæ–­ï¼‰
        base_recon = out.get('base_reconstructed', None)
        if base_recon is not None:
            base_recon = base_recon.cpu().numpy()
        residual_scale = out.get('residual_scale', None)
        if residual_scale is not None and isinstance(residual_scale, torch.Tensor):
            residual_scale = float(residual_scale.cpu().numpy())

        # pipeline code used reconstructed_xzy then converted to x,y,z via [:,:, [0,2,1]]
        # if model returned xzy we convert to xyz
        try:
            # assume recon is in xzy -> convert
            recon_xyz = recon[:, :, [0, 2, 1]]
        except Exception:
            recon_xyz = recon
        
        # è½¬æ¢ base_reconï¼ˆå¦‚æœå­˜åœ¨ï¼‰
        if base_recon is not None:
            try:
                base_recon_xyz = base_recon[:, :, [0, 2, 1]]
            except Exception:
                base_recon_xyz = base_recon
        else:
            base_recon_xyz = None

        return {
            'reconstructed': recon_xyz,
            'tokens': tokens,
            'vq_loss': vq_loss,
            'base_reconstructed': base_recon_xyz,
            'residual_scale': residual_scale
        }

    def analyze_token_diversity(self, tokens_array, num_groups=5, tokens_per_group=128):
        """åˆ†æ token åºåˆ—çš„å¤šæ ·æ€§
        
        Args:
            tokens_array: (N, num_groups) token åºåˆ—æ•°ç»„
            num_groups: è¯­ä¹‰åˆ†ç»„æ•°ï¼ˆé»˜è®¤5ï¼‰
            tokens_per_group: æ¯ç»„tokenæ•°é‡ï¼ˆé»˜è®¤128ï¼‰
        
        Returns:
            dict: åŒ…å«æ¯ç»„çš„å”¯ä¸€tokenæ•°ã€é¢‘ç‡åˆ†å¸ƒç­‰ç»Ÿè®¡ä¿¡æ¯
        """
        group_names = ['head_spine', 'left_arm', 'right_arm', 'left_leg', 'right_leg']
        
        analysis = {}
        
        print("\n" + "="*80)
        print("TOKEN DIVERSITY ANALYSIS")
        print("="*80)
        
        for group_idx in range(min(num_groups, tokens_array.shape[1])):
            group_tokens = tokens_array[:, group_idx]
            
            # è®¡ç®—è¯¥ç»„tokençš„offsetï¼ˆç”¨äºè¿˜åŸåŸå§‹token IDï¼‰
            group_offset = group_idx * tokens_per_group
            
            # è¿˜åŸä¸ºåŸå§‹token IDï¼ˆç§»é™¤offsetï¼‰
            original_tokens = group_tokens - group_offset
            
            # ç»Ÿè®¡å”¯ä¸€token
            unique_tokens, counts = np.unique(original_tokens, return_counts=True)
            
            # è®¡ç®—ç†µï¼ˆè¡¡é‡åˆ†å¸ƒå‡åŒ€æ€§ï¼‰
            probs = counts / counts.sum()
            entropy = -np.sum(probs * np.log2(probs + 1e-10))
            max_entropy = np.log2(tokens_per_group)  # å®Œå…¨å‡åŒ€åˆ†å¸ƒçš„æœ€å¤§ç†µ
            
            # Top-10 æœ€å¸¸ç”¨çš„token
            top_indices = np.argsort(-counts)[:10]
            top_tokens = unique_tokens[top_indices]
            top_counts = counts[top_indices]
            
            group_name = group_names[group_idx] if group_idx < len(group_names) else f'group_{group_idx}'
            
            analysis[group_name] = {
                'unique_tokens': len(unique_tokens),
                'total_tokens': len(original_tokens),
                'coverage': len(unique_tokens) / tokens_per_group * 100,  # ä½¿ç”¨äº†å¤šå°‘æ¯”ä¾‹çš„ç æœ¬
                'entropy': entropy,
                'normalized_entropy': entropy / max_entropy if max_entropy > 0 else 0,  # 0-1ä¹‹é—´
                'top_10_tokens': top_tokens.tolist(),
                'top_10_counts': top_counts.tolist(),
                'top_10_freq': (top_counts / counts.sum() * 100).tolist()
            }
            
            # æ‰“å°è¯¥ç»„ä¿¡æ¯
            print(f"\n{group_name.upper()} (Group {group_idx}):")
            print(f"  Unique tokens: {len(unique_tokens)}/{tokens_per_group} ({len(unique_tokens)/tokens_per_group*100:.1f}% coverage)")
            print(f"  Entropy: {entropy:.3f} / {max_entropy:.3f} (normalized: {entropy/max_entropy:.3f})")
            print(f"  Top-10 most used tokens:")
            for i, (tok, cnt, freq) in enumerate(zip(top_tokens, top_counts, top_counts/counts.sum()*100)):
                print(f"    #{i+1}: Token {tok:3d} - {cnt:5d} times ({freq:5.2f}%)")
        
        # å…¨å±€ç»Ÿè®¡
        print(f"\n{'='*80}")
        print("GLOBAL SUMMARY:")
        print(f"{'='*80}")
        
        total_unique = sum(a['unique_tokens'] for a in analysis.values())
        total_possible = num_groups * tokens_per_group
        avg_coverage = np.mean([a['coverage'] for a in analysis.values()])
        avg_entropy = np.mean([a['normalized_entropy'] for a in analysis.values()])
        
        print(f"Total unique tokens used: {total_unique}/{total_possible} ({total_unique/total_possible*100:.1f}%)")
        print(f"Average coverage per group: {avg_coverage:.1f}%")
        print(f"Average normalized entropy: {avg_entropy:.3f}")
        
        # è¯Šæ–­å»ºè®®
        print(f"\n{'='*80}")
        print("DIAGNOSTIC:")
        print(f"{'='*80}")
        
        if avg_coverage < 10:
            print("âŒ SEVERE COLLAPSE: Less than 10% of codebook used!")
            print("   â†’ Model is NOT learning meaningful discrete representations")
        elif avg_coverage < 30:
            print("âš ï¸  MODERATE COLLAPSE: 10-30% of codebook used")
            print("   â†’ Codebook learning is weak, consider increasing VQ loss weight")
        elif avg_coverage < 60:
            print("âœ“  FAIR: 30-60% of codebook used")
            print("   â†’ Codebook is learning, but still has room for improvement")
        else:
            print("âœ… GOOD: >60% of codebook used")
            print("   â†’ Codebook is learning diverse representations")
        
        if avg_entropy < 0.3:
            print("âŒ HIGHLY SKEWED: Token distribution is extremely imbalanced")
        elif avg_entropy < 0.6:
            print("âš ï¸  SKEWED: Token distribution is somewhat imbalanced")
        else:
            print("âœ… BALANCED: Token distribution is relatively uniform")
        
        return analysis
    
    def process_and_save(self, input_npy, out_dir, split_name='split', batch_size=32, per_sample=False, analyze_tokens=True):
        os.makedirs(out_dir, exist_ok=True)
        print(f"Processing {input_npy} -> {out_dir} (batch_size={batch_size}, per_sample={per_sample})")

        data = np.load(input_npy)
        N = len(data)
        print(f"Loaded {N} samples from {input_npy}")

        # Prepare containers
        recon_list = []
        tokens_list = []
        vq_list = []
        extracted_list = []
        base_recon_list = []  # æ–°å¢ï¼šçº¯ç æœ¬é‡å»º
        residual_scales = []  # æ–°å¢ï¼šæ®‹å·®ç³»æ•°
        metadata = []

        # iterate in batches
        for start in tqdm(range(0, N, batch_size)):
            end = min(start + batch_size, N)
            batch = data[start:end]
            # ensure shape (B,H,W,C)
            if batch.ndim == 3:
                # (H,W,C) single sample -> expand
                batch = np.expand_dims(batch, 0)
            if batch.ndim == 4 and batch.shape[-1] not in (1,3,5):
                # maybe (B,C,H,W), convert to (B,H,W,C)
                batch = batch.transpose(0, 2, 3, 1)

            # extraction
            extracted = self._extract_batch(batch)
            ntu = extracted['ntu_skel'].numpy()  # (B,25,3)

            # reconstructionï¼ˆè¿”å›å€¼å¢å¼ºï¼‰
            try:
                recon_result = self._reconstruct_batch(ntu)
                recon_xyz = recon_result['reconstructed']
                tokens = recon_result['tokens']
                vq_loss = recon_result['vq_loss']
                base_recon_xyz = recon_result['base_reconstructed']
                residual_scale = recon_result['residual_scale']
            except Exception as e:
                print(f"âŒ Reconstruction failed for batch {start}-{end}: {e}")
                # fill with zeros to keep alignment
                B = ntu.shape[0]
                recon_xyz = np.zeros_like(ntu)
                tokens = np.zeros((B, 5), dtype=np.int32) if isinstance(tokens, np.ndarray) else np.zeros((B,), dtype=np.int32)
                vq_loss = 0.0
                base_recon_xyz = None
                residual_scale = None

            # accumulate
            recon_list.append(recon_xyz.astype(np.float32))
            tokens_list.append(np.array(tokens))
            
            if base_recon_xyz is not None:
                base_recon_list.append(base_recon_xyz.astype(np.float32))
            
            if residual_scale is not None:
                residual_scales.append(residual_scale)
            
            # vq_loss may be scalar; replicate per sample if so
            if np.isscalar(vq_loss):
                vq_list.append(np.full((ntu.shape[0],), float(vq_loss), dtype=np.float32))
            else:
                vq_list.append(np.array(vq_loss, dtype=np.float32))

            extracted_list.append(ntu.astype(np.float32))

            # metadata per sample
            for i in range(ntu.shape[0]):
                metadata.append({'index': start + i, 'src_file': str(input_npy)})

            # optional per-sample saving
            if per_sample:
                for i in range(ntu.shape[0]):
                    idx = start + i
                    per_path = os.path.join(out_dir, f'{split_name}_sample_{idx:06d}.npz')
                    save_dict = {
                        'reconstructed': recon_xyz[i].astype(np.float32),
                        'tokens': np.array(tokens[i]).astype(np.int32) if np.ndim(tokens) > 1 else np.array([int(tokens[i])], dtype=np.int32),
                        'vq_loss': float(vq_loss) if np.isscalar(vq_loss) else float(vq_loss[i]),
                        'extracted': ntu[i].astype(np.float32)
                    }
                    if base_recon_xyz is not None:
                        save_dict['base_reconstructed'] = base_recon_xyz[i].astype(np.float32)
                    np.savez_compressed(per_path, **save_dict)

        # concat
        reconstructed = np.concatenate(recon_list, axis=0)
        try:
            token_sequences = np.concatenate(tokens_list, axis=0)
        except Exception:
            # tokens may be ragged; fallback to object array (save as list)
            token_sequences = np.array(tokens_list, dtype=object)
        vq_losses = np.concatenate(vq_list, axis=0)
        extracted_all = np.concatenate(extracted_list, axis=0)
        
        # åˆå¹¶ base_recon å’Œ residual_scale
        if base_recon_list:
            base_reconstructed = np.concatenate(base_recon_list, axis=0)
        else:
            base_reconstructed = None
        
        avg_residual_scale = np.mean(residual_scales) if residual_scales else None

        # æ–°å¢ï¼šToken å¤šæ ·æ€§åˆ†æ
        if analyze_tokens and token_sequences.dtype != object:
            try:
                token_analysis = self.analyze_token_diversity(token_sequences)
                
                # ä¿å­˜åˆ†æç»“æœ
                analysis_path = os.path.join(out_dir, f'{split_name}_token_analysis.json')
                with open(analysis_path, 'w') as f:
                    json.dump(token_analysis, f, indent=2)
                print(f"âœ… Token analysis saved to: {analysis_path}")
            except Exception as e:
                print(f"âš ï¸ Token diversity analysis failed: {e}")
        
        # æ‰“å°é‡æ„è´¨é‡ç»Ÿè®¡
        print(f"\n{'='*80}")
        print("RECONSTRUCTION QUALITY:")
        print(f"{'='*80}")
        print(f"Average VQ Loss: {vq_losses.mean():.6f}")
        if avg_residual_scale is not None:
            print(f"Average Residual Scale: {avg_residual_scale:.6f}")
            if avg_residual_scale > 0.5:
                print("âš ï¸  High residual contribution - model may be bypassing codebook!")
            else:
                print("âœ“  Reasonable residual contribution")
        
        if base_reconstructed is not None:
            # è®¡ç®—çº¯ç æœ¬é‡å»ºçš„è¯¯å·® vs æœ€ç»ˆé‡å»ºçš„è¯¯å·®
            base_error = np.mean((extracted_all - base_reconstructed) ** 2)
            final_error = np.mean((extracted_all - reconstructed) ** 2)
            improvement = (base_error - final_error) / base_error * 100 if base_error > 0 else 0
            
            print(f"Codebook-only MSE: {base_error:.6f}")
            print(f"Final MSE (w/ residual): {final_error:.6f}")
            print(f"Residual improvement: {improvement:.1f}%")
            
            if improvement > 50:
                print("âŒ Residual provides >50% improvement - codebook is weak!")
            elif improvement > 20:
                print("âš ï¸  Residual provides 20-50% improvement - codebook needs strengthening")
            else:
                print("âœ… Residual provides <20% improvement - codebook is doing most of the work")

        # save single compressed file
        out_path = os.path.join(out_dir, f'{split_name}_recon.npz')
        print(f"\nSaving aggregated results to {out_path}")

        # metadata to json string
        meta_json = json.dumps(metadata)

        # å¦‚æœ token_sequences æ˜¯ objectï¼ˆé•¿åº¦ä¸ä¸€ï¼‰ï¼Œæˆ‘ä»¬æ— æ³•ç›´æ¥ä¿å­˜ä¸ºæ•°ç»„ï¼Œæ”¹ä¸ºä¿å­˜ä¸º np.savez çš„ list
        save_dict = {
            'reconstructed': reconstructed,
            'vq_losses': vq_losses,
            'extracted': extracted_all,
            'metadata': meta_json
        }
        
        if base_reconstructed is not None:
            save_dict['base_reconstructed'] = base_reconstructed
        
        if token_sequences.dtype == object:
            # ä¿å­˜ tokens å•ç‹¬ä¸º .npy via pickle-friendly method
            tokens_path = os.path.join(out_dir, f'{split_name}_tokens.npy')
            print(f"Tokens are ragged; saving tokens as list to {tokens_path}")
            np.save(tokens_path, token_sequences, allow_pickle=True)
        else:
            save_dict['token_sequences'] = token_sequences.astype(np.int32)
        
        np.savez_compressed(out_path, **save_dict)

        print(f"âœ… Saved: {out_path} (N={reconstructed.shape[0]})")
        
        # ç”ŸæˆCSVç´¢å¼•æ–‡ä»¶
        if per_sample and token_sequences.dtype != object:
            csv_path = os.path.join(out_dir, f'{split_name}_index.csv')
            print(f"Generating CSV index: {csv_path}")
            
            import csv
            with open(csv_path, 'w', newline='') as f:
                writer = csv.writer(f)
                writer.writerow(['index', 'split', 'file_path', 'tokens_str', 'token_first', 'vq_loss'])
                
                for i in range(len(token_sequences)):
                    sample_path = os.path.join(out_dir, f'{split_name}_sample_{i:06d}.npz')
                    tokens_str = str(token_sequences[i].tolist())
                    token_first = int(token_sequences[i][0])
                    vq_loss_val = float(vq_losses[i])
                    
                    writer.writerow([i, split_name, sample_path, tokens_str, token_first, vq_loss_val])
            
            print(f"âœ… CSV index saved: {csv_path} ({len(token_sequences)} rows)")
        
        return out_path


def find_model_files():
    """è‡ªåŠ¨æŸ¥æ‰¾å¯ç”¨çš„æ¨¡å‹æ–‡ä»¶"""
    import glob
    
    # æŸ¥æ‰¾æå–å™¨æ¨¡å‹
    extractor_patterns = [
        'mars_transformer_best*.pth',
        'mars_transformer*.pth',
        'models/mars_transformer*.pth',
        'checkpoints/mars_transformer*.pth'
    ]
    
    extractor_files = []
    for pattern in extractor_patterns:
        extractor_files.extend(glob.glob(pattern))
    
    # æŸ¥æ‰¾GCNæ¨¡å‹
    gcn_patterns = [
        'experiments/gcn_skeleton_memory_optimized/NTU_models/*/ckpt-best.pth',
        'experiments/*/ckpt-best.pth',
        'checkpoints/gcn*.pth'
    ]
    
    gcn_files = []
    for pattern in gcn_patterns:
        gcn_files.extend(glob.glob(pattern))
    
    # æŸ¥æ‰¾GCNé…ç½®
    cfg_patterns = [
        'cfgs/NTU_models/gcn_skeleton_memory_optimized.yaml',
        'cfgs/NTU_models/*.yaml',
        'configs/*.yaml'
    ]
    
    cfg_files = []
    for pattern in cfg_patterns:
        cfg_files.extend(glob.glob(pattern))
    
    return extractor_files, gcn_files, cfg_files


def main():
    parser = argparse.ArgumentParser(
        description='Batch reconstruct MARS datasets and save recon+tokens (no visualization).',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ç¤ºä¾‹ç”¨æ³•:
  # è‡ªåŠ¨æŸ¥æ‰¾æ¨¡å‹ï¼ˆäº¤äº’å¼ï¼‰
  python %(prog)s
  
  # æŒ‡å®šæ‰€æœ‰å‚æ•°
  python %(prog)s --extractor mars_transformer_best_150.pth \\
                  --gcn_ckpt experiments/.../ckpt-best.pth \\
                  --gcn_cfg cfgs/NTU_models/gcn_skeleton_memory_optimized.yaml
                  
  # ç”Ÿæˆå•æ ·æœ¬æ–‡ä»¶ï¼ˆç”¨äºç²¾ç»†æ‰¹æ³¨ï¼‰
  python %(prog)s --per_sample
        """
    )
    
    # é»˜è®¤è·¯å¾„
    default_extractor = 'mars_transformer_best_150.pth'
    default_gcn_ckpt = 'experiments/gcn_skeleton_memory_optimized/NTU_models/default/ckpt-best.pth'
    default_gcn_cfg = 'cfgs/NTU_models/gcn_skeleton_memory_optimized.yaml'
    
    parser.add_argument('--extractor', default=None, help=f'è·¯å¾„åˆ° MARSTransformerModel æƒé‡ (é»˜è®¤: {default_extractor})')
    parser.add_argument('--gcn_ckpt', default=None, help=f'è·¯å¾„åˆ° GCN é‡æ„å™¨æƒé‡ (é»˜è®¤: {default_gcn_ckpt})')
    parser.add_argument('--gcn_cfg', default=None, help=f'è·¯å¾„åˆ° GCN é…ç½® yaml (é»˜è®¤: {default_gcn_cfg})')
    parser.add_argument('--out_dir', default='data/MARS_recon_tokens', help='è¾“å‡ºç›®å½•')
    parser.add_argument('--batch_size', type=int, default=32, help='æ‰¹æ¬¡å¤§å°')
    parser.add_argument('--per_sample', action='store_true', help='æ˜¯å¦ä¸ºæ¯ä¸ªæ ·æœ¬ä¿å­˜å•ç‹¬çš„ npz æ–‡ä»¶')
    parser.add_argument('--use_enhanced_mapper', action='store_true', help='ä½¿ç”¨ EnhancedSkeletonMapper (é»˜è®¤ False)')
    parser.add_argument('--device', default=None, help='è®¾å¤‡ï¼Œå¦‚ cuda:0 æˆ– cpu')
    parser.add_argument('--train', default='/home/uo/myProject/HumanPoint-BERT/data/MARS/featuremap_train.npy')
    parser.add_argument('--test', default='/home/uo/myProject/HumanPoint-BERT/data/MARS/featuremap_test.npy')
    parser.add_argument('--validate', default='/home/uo/myProject/HumanPoint-BERT/data/MARS/featuremap_validate.npy')
    parser.add_argument('--auto', action='store_true', help='è‡ªåŠ¨æ¨¡å¼ï¼šä½¿ç”¨é»˜è®¤å€¼ï¼Œä¸è¿›è¡Œäº¤äº’å¼ç¡®è®¤')

    args = parser.parse_args()
    
    print("="*80)
    print("MARS Token Generation Pipeline")
    print("="*80)
    print()
    
    # å¦‚æœæ²¡æœ‰æŒ‡å®šæ¨¡å‹è·¯å¾„ï¼Œå°è¯•è‡ªåŠ¨æŸ¥æ‰¾æˆ–ä½¿ç”¨é»˜è®¤å€¼
    if args.extractor is None:
        if os.path.exists(default_extractor):
            args.extractor = default_extractor
            print(f"âœ“ ä½¿ç”¨é»˜è®¤æå–å™¨: {args.extractor}")
        else:
            print("ğŸ” è‡ªåŠ¨æŸ¥æ‰¾æå–å™¨æ¨¡å‹...")
            extractor_files, _, _ = find_model_files()
            if extractor_files:
                print(f"æ‰¾åˆ° {len(extractor_files)} ä¸ªæå–å™¨æ¨¡å‹:")
                for i, f in enumerate(extractor_files, 1):
                    print(f"  [{i}] {f}")
                
                if not args.auto:
                    choice = input(f"\nè¯·é€‰æ‹©æ¨¡å‹ [1-{len(extractor_files)}] (å›è½¦ä½¿ç”¨ç¬¬1ä¸ª): ").strip()
                    idx = int(choice) - 1 if choice else 0
                    args.extractor = extractor_files[idx]
                else:
                    args.extractor = extractor_files[0]
                print(f"âœ“ é€‰æ‹©: {args.extractor}")
            else:
                print(f"âŒ æœªæ‰¾åˆ°æå–å™¨æ¨¡å‹ï¼Œè¯·ä½¿ç”¨ --extractor å‚æ•°æŒ‡å®š")
                return
    
    if args.gcn_ckpt is None:
        if os.path.exists(default_gcn_ckpt):
            args.gcn_ckpt = default_gcn_ckpt
            print(f"âœ“ ä½¿ç”¨é»˜è®¤GCNæ¨¡å‹: {args.gcn_ckpt}")
        else:
            print("ğŸ” è‡ªåŠ¨æŸ¥æ‰¾GCNæ¨¡å‹...")
            _, gcn_files, _ = find_model_files()
            if gcn_files:
                print(f"æ‰¾åˆ° {len(gcn_files)} ä¸ªGCNæ¨¡å‹:")
                for i, f in enumerate(gcn_files, 1):
                    print(f"  [{i}] {f}")
                
                if not args.auto:
                    choice = input(f"\nè¯·é€‰æ‹©æ¨¡å‹ [1-{len(gcn_files)}] (å›è½¦ä½¿ç”¨ç¬¬1ä¸ª): ").strip()
                    idx = int(choice) - 1 if choice else 0
                    args.gcn_ckpt = gcn_files[idx]
                else:
                    args.gcn_ckpt = gcn_files[0]
                print(f"âœ“ é€‰æ‹©: {args.gcn_ckpt}")
            else:
                print(f"âŒ æœªæ‰¾åˆ°GCNæ¨¡å‹ï¼Œè¯·ä½¿ç”¨ --gcn_ckpt å‚æ•°æŒ‡å®š")
                return
    
    if args.gcn_cfg is None:
        if os.path.exists(default_gcn_cfg):
            args.gcn_cfg = default_gcn_cfg
            print(f"âœ“ ä½¿ç”¨é»˜è®¤GCNé…ç½®: {args.gcn_cfg}")
        else:
            print("ğŸ” è‡ªåŠ¨æŸ¥æ‰¾GCNé…ç½®...")
            _, _, cfg_files = find_model_files()
            if cfg_files:
                print(f"æ‰¾åˆ° {len(cfg_files)} ä¸ªé…ç½®æ–‡ä»¶:")
                for i, f in enumerate(cfg_files, 1):
                    print(f"  [{i}] {f}")
                
                if not args.auto:
                    choice = input(f"\nè¯·é€‰æ‹©é…ç½® [1-{len(cfg_files)}] (å›è½¦ä½¿ç”¨ç¬¬1ä¸ª): ").strip()
                    idx = int(choice) - 1 if choice else 0
                    args.gcn_cfg = cfg_files[idx]
                else:
                    args.gcn_cfg = cfg_files[0]
                print(f"âœ“ é€‰æ‹©: {args.gcn_cfg}")
            else:
                print(f"âŒ æœªæ‰¾åˆ°GCNé…ç½®ï¼Œè¯·ä½¿ç”¨ --gcn_cfg å‚æ•°æŒ‡å®š")
                return
    
    # éªŒè¯æ–‡ä»¶æ˜¯å¦å­˜åœ¨
    print()
    print("="*80)
    print("éªŒè¯æ–‡ä»¶...")
    print("="*80)
    
    required_files = {
        'æå–å™¨æ¨¡å‹': args.extractor,
        'GCNæ¨¡å‹': args.gcn_ckpt,
        'GCNé…ç½®': args.gcn_cfg
    }
    
    all_exist = True
    for name, path in required_files.items():
        if os.path.exists(path):
            size = os.path.getsize(path)
            size_str = f"{size/1024/1024:.1f}MB" if size > 1024*1024 else f"{size/1024:.1f}KB"
            print(f"âœ“ {name}: {path} ({size_str})")
        else:
            print(f"âŒ {name}: {path} (ä¸å­˜åœ¨)")
            all_exist = False
    
    if not all_exist:
        print("\nâŒ æŸäº›å¿…éœ€æ–‡ä»¶ä¸å­˜åœ¨ï¼Œæ— æ³•ç»§ç»­")
        return
    
    # æ£€æŸ¥æ•°æ®æ–‡ä»¶
    print()
    print("="*80)
    print("æ£€æŸ¥æ•°æ®æ–‡ä»¶...")
    print("="*80)
    
    data_files = {
        'train': args.train,
        'test': args.test,
        'validate': args.validate
    }
    
    available_splits = []
    for split, path in data_files.items():
        if os.path.exists(path):
            size = os.path.getsize(path)
            size_str = f"{size/1024/1024:.1f}MB"
            num_samples = len(np.load(path, mmap_mode='r'))
            print(f"âœ“ {split}: {path} ({size_str}, {num_samples} samples)")
            available_splits.append(split)
        else:
            print(f"âš ï¸ {split}: {path} (ä¸å­˜åœ¨ï¼Œå°†è·³è¿‡)")
    
    if not available_splits:
        print("\nâŒ æœªæ‰¾åˆ°ä»»ä½•æ•°æ®æ–‡ä»¶")
        return
    
    # æ˜¾ç¤ºé…ç½®
    print()
    print("="*80)
    print("å¤„ç†é…ç½®:")
    print("="*80)
    print(f"è¾“å‡ºç›®å½•: {args.out_dir}")
    print(f"æ‰¹æ¬¡å¤§å°: {args.batch_size}")
    print(f"ç”Ÿæˆå•æ ·æœ¬æ–‡ä»¶: {'æ˜¯' if args.per_sample else 'å¦'}")
    print(f"ä½¿ç”¨å¢å¼ºæ˜ å°„å™¨: {'æ˜¯' if args.use_enhanced_mapper else 'å¦'}")
    print(f"è®¾å¤‡: {args.device if args.device else 'è‡ªåŠ¨æ£€æµ‹'}")
    print(f"å°†å¤„ç†çš„æ•°æ®é›†: {', '.join(available_splits)}")
    
    # ç¡®è®¤
    if not args.auto:
        print()
        confirm = input("æŒ‰å›è½¦é”®å¼€å§‹å¤„ç†ï¼Œæˆ–è¾“å…¥ 'n' å–æ¶ˆ: ").strip().lower()
        if confirm == 'n':
            print("å·²å–æ¶ˆ")
            return
    
    print()
    print("="*80)
    print("å¼€å§‹å¤„ç†...")
    print("="*80)
    print()

    saver = SkeletonReconstructionSaver(args.extractor, args.gcn_ckpt, args.gcn_cfg, device=args.device, use_enhanced_mapper=args.use_enhanced_mapper)

    out_dir = args.out_dir
    os.makedirs(out_dir, exist_ok=True)

    # ä¾æ¬¡å¤„ç† train/test/validateï¼ˆå¦‚æœå­˜åœ¨ï¼‰
    import time
    start_time = time.time()
    
    processed_splits = []
    for split_name, path in [('train', args.train), ('test', args.test), ('validate', args.validate)]:
        if path and os.path.exists(path):
            print(f"\n{'='*80}")
            print(f"Processing split: {split_name}")
            print(f"{'='*80}")
            saver.process_and_save(path, out_dir, split_name=split_name, batch_size=args.batch_size, 
                                  per_sample=args.per_sample, analyze_tokens=True)
            processed_splits.append(split_name)
        else:
            if path:
                print(f"\nâš ï¸ è·³è¿‡ {split_name}ï¼šæ–‡ä»¶ä¸å­˜åœ¨ {path}")
    
    # å¦‚æœç”Ÿæˆäº†å•æ ·æœ¬æ–‡ä»¶ï¼Œåˆå¹¶æ‰€æœ‰splitçš„CSVç´¢å¼•
    if args.per_sample and processed_splits:
        print()
        print("="*80)
        print("ç”Ÿæˆåˆå¹¶CSVç´¢å¼•...")
        print("="*80)
        
        combined_csv_path = os.path.join(out_dir, 'index.csv')
        import csv
        
        with open(combined_csv_path, 'w', newline='') as f_out:
            writer = csv.writer(f_out)
            writer.writerow(['index', 'split', 'file_path', 'tokens_str', 'token_first', 'vq_loss'])
            
            for split_name in processed_splits:
                split_csv = os.path.join(out_dir, f'{split_name}_index.csv')
                if os.path.exists(split_csv):
                    with open(split_csv, 'r') as f_in:
                        reader = csv.reader(f_in)
                        next(reader)  # è·³è¿‡è¡¨å¤´
                        for row in reader:
                            writer.writerow(row)
                    print(f"  âœ“ åˆå¹¶ {split_name}")
        
        print(f"âœ… åˆå¹¶CSVç´¢å¼•å·²ä¿å­˜: {combined_csv_path}")

    elapsed = time.time() - start_time
    minutes = int(elapsed // 60)
    seconds = int(elapsed % 60)
    
    print()
    print("="*80)
    print(f"âœ… å¤„ç†å®Œæˆï¼è€—æ—¶: {minutes}åˆ†{seconds}ç§’")
    print("="*80)
    print()
    print("ç”Ÿæˆçš„æ–‡ä»¶:")
    for f in sorted(os.listdir(out_dir)):
        fpath = os.path.join(out_dir, f)
        if os.path.isfile(fpath):
            size = os.path.getsize(fpath)
            size_str = f"{size/1024/1024:.1f}MB" if size > 1024*1024 else f"{size/1024:.1f}KB"
            print(f"  {f} ({size_str})")
    
    print()
    print("ä¸‹ä¸€æ­¥:")
    print("  1. æŸ¥çœ‹Tokenåˆ†æ: cat", os.path.join(out_dir, "*_token_analysis.json"))
    print("  2. åŠ è½½æ•°æ®è¿›è¡Œæ‰¹æ³¨:")
    print(f"     python tools/example_load_mars_tokens.py")
    print()


if __name__ == '__main__':
    main()
