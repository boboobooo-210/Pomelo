#!/usr/bin/env python3
"""
éª¨æ¶æå– + GCNé‡æ„ ç®€åŒ–å®ç°
ç»“åˆskeleton_extractorå’ŒGCNSkeletonTokenizerçš„æœ€å°åŒ–å¯æ‰§è¡Œç‰ˆæœ¬
"""

import os
import sys
import json
from pathlib import Path

# æ·»åŠ é¡¹ç›®è·¯å¾„
project_root = Path(__file__).parent.parent
sys.path.append(str(project_root))

def check_dependencies():
    """æ£€æŸ¥å¿…éœ€çš„ä¾èµ–å’Œæ–‡ä»¶"""
    print("ğŸ” æ£€æŸ¥è¿è¡Œç¯å¢ƒ...")
    
    # æ£€æŸ¥æƒé‡æ–‡ä»¶
    required_files = {
        'extractor_weights': 'mars_transformer_best.pth',
        'gcn_weights': 'experiments/gcn_skeleton_memory_optimized/NTU_models/default/ckpt-best.pth',
        'gcn_config': 'cfgs/NTU_models/gcn_skeleton_memory_optimized.yaml',
        'extractor_model': 'models/skeleton_extractor.py',
        'gcn_model': 'models/GCNSkeletonTokenizer.py'
    }
    
    missing_files = []
    for name, path in required_files.items():
        if os.path.exists(path):
            print(f"  âœ… {name}: {path}")
        else:
            print(f"  âŒ {name}: {path} - æ–‡ä»¶ä¸å­˜åœ¨")
            missing_files.append(path)
    
    # æ£€æŸ¥PythonåŒ…
    required_packages = ['torch', 'numpy', 'matplotlib']
    available_packages = []
    
    for pkg in required_packages:
        try:
            __import__(pkg)
            print(f"  âœ… {pkg}: å·²å®‰è£…")
            available_packages.append(pkg)
        except ImportError:
            print(f"  âŒ {pkg}: æœªå®‰è£…")
    
    return len(missing_files) == 0 and len(available_packages) == len(required_packages)

def create_mock_pipeline_demo():
    """åˆ›å»ºæ¨¡æ‹Ÿæµæ°´çº¿æ¼”ç¤º"""
    print("\nğŸ­ åˆ›å»ºæ¨¡æ‹Ÿæµæ°´çº¿æ¼”ç¤º...")
    
    # æ¨¡æ‹Ÿæ•°æ®
    import numpy as np
    
    # 1. æ¨¡æ‹Ÿé›·è¾¾ç‰¹å¾å›¾
    print("  1ï¸âƒ£ æ¨¡æ‹Ÿé›·è¾¾ç‰¹å¾å›¾ (8x8x5)")
    radar_data = np.random.rand(8, 8, 5).astype(np.float32)
    
    # 2. æ¨¡æ‹Ÿéª¨æ¶æå–è¿‡ç¨‹
    print("  2ï¸âƒ£ æ¨¡æ‹ŸMARSéª¨æ¶æå–...")
    # å‡è®¾MARSæ¨¡å‹è¾“å‡º57ç»´ç‰¹å¾ï¼Œè½¬æ¢ä¸º25å…³èŠ‚ç‚¹
    skeleton_57d = np.random.rand(57).astype(np.float32) * 2 - 1  # èŒƒå›´[-1, 1]
    
    # è½¬æ¢ä¸º25å…³èŠ‚ç‚¹æ ¼å¼ (åªå–å‰75ç»´ä½œä¸º25Ã—3åæ ‡)
    if len(skeleton_57d) >= 75:
        skeleton_25joints = skeleton_57d[:75].reshape(25, 3)
    else:
        skeleton_25joints = np.zeros((25, 3))
        available_joints = min(len(skeleton_57d) // 3, 25)
        skeleton_25joints[:available_joints, :] = skeleton_57d[:available_joints*3].reshape(available_joints, 3)
    
    print(f"     æå–éª¨æ¶å½¢çŠ¶: {skeleton_25joints.shape}")
    
    # 3. æ¨¡æ‹Ÿéª¨æ¶æ ‡å‡†åŒ–
    print("  3ï¸âƒ£ éª¨æ¶æ ‡å‡†åŒ–å¤„ç†...")
    centroid = np.mean(skeleton_25joints, axis=0)
    centered = skeleton_25joints - centroid
    distances = np.sqrt(np.sum(centered**2, axis=1))
    max_distance = np.max(distances)
    
    if max_distance > 0:
        normalized_skeleton = centered / max_distance
    else:
        normalized_skeleton = centered
    
    print(f"     æ ‡å‡†åŒ–åèŒƒå›´: [{normalized_skeleton.min():.3f}, {normalized_skeleton.max():.3f}]")
    
    # 4. æ¨¡æ‹ŸGCNé‡æ„è¿‡ç¨‹
    print("  4ï¸âƒ£ æ¨¡æ‹ŸGCNé‡æ„...")
    
    # æ¨¡æ‹Ÿé‡æ„ç»“æœï¼ˆæ·»åŠ å°é‡å™ªå£°ï¼‰
    reconstruction_noise = np.random.normal(0, 0.05, skeleton_25joints.shape)
    reconstructed_skeleton = normalized_skeleton + reconstruction_noise
    
    # æ¨¡æ‹Ÿtokenåºåˆ—ï¼ˆ5ä¸ªè¯­ä¹‰ç»„ï¼‰
    token_sequence = np.random.randint(0, 128, 5)
    # æ·»åŠ ç»„åç§»
    group_offsets = [0, 128, 256, 384, 512]
    for i in range(5):
        token_sequence[i] += group_offsets[i]
    
    # è®¡ç®—æ¨¡æ‹ŸæŒ‡æ ‡
    mse_error = np.mean((normalized_skeleton - reconstructed_skeleton)**2)
    vq_loss = np.random.uniform(0.001, 0.01)  # æ¨¡æ‹ŸVQæŸå¤±
    
    print(f"     é‡æ„MSEè¯¯å·®: {mse_error:.6f}")
    print(f"     æ¨¡æ‹ŸVQæŸå¤±: {vq_loss:.6f}")
    print(f"     Tokenåºåˆ—: {token_sequence}")
    
    # 5. åˆ†æTokenåºåˆ—
    print("  5ï¸âƒ£ Tokenåºåˆ—åˆ†æ...")
    group_names = ['head_spine', 'left_arm', 'right_arm', 'left_leg', 'right_leg']
    for i, (token_id, group_name) in enumerate(zip(token_sequence, group_names)):
        expected_range = f"{group_offsets[i]}-{group_offsets[i]+127}"
        print(f"     {group_name}: Token {token_id} (èŒƒå›´: {expected_range})")
    
    return {
        'radar_data': radar_data,
        'extracted_skeleton': skeleton_25joints,
        'normalized_skeleton': normalized_skeleton,
        'reconstructed_skeleton': reconstructed_skeleton,
        'token_sequence': token_sequence,
        'mse_error': mse_error,
        'vq_loss': vq_loss
    }

def create_text_visualization(demo_result):
    """åˆ›å»ºæ–‡æœ¬å½¢å¼çš„å¯è§†åŒ–"""
    print("\nğŸ“Š ç”Ÿæˆæ–‡æœ¬å¯è§†åŒ–...")
    
    import numpy as np
    
    # åˆ›å»ºç®€å•çš„ASCIIå›¾è¡¨
    mse_error = demo_result['mse_error']
    vq_loss = demo_result['vq_loss']
    token_sequence = demo_result['token_sequence']
    
    print("=" * 60)
    print("ğŸ“ˆ éª¨æ¶æå–+é‡æ„æµç¨‹ç»“æœ")
    print("=" * 60)
    
    # è´¨é‡è¯„ä¼°
    print("ğŸ¯ è´¨é‡æŒ‡æ ‡:")
    if mse_error < 0.01:
        quality = "ä¼˜ç§€ âœ…"
    elif mse_error < 0.05:
        quality = "è‰¯å¥½ âš¡"
    else:
        quality = "éœ€æ”¹è¿› âš ï¸"
    
    print(f"  MSEé‡æ„è¯¯å·®: {mse_error:.6f} - {quality}")
    print(f"  VQé‡åŒ–æŸå¤±: {vq_loss:.6f}")
    
    # Tokenåˆ†å¸ƒ
    print("\nğŸ­ è¯­ä¹‰ç»„Tokenåˆ†æ:")
    group_names = ['å¤´è„ŠæŸ±', 'å·¦è‡‚', 'å³è‡‚', 'å·¦è…¿', 'å³è…¿']
    group_ranges = [(0,127), (128,255), (256,383), (384,511), (512,639)]
    
    for i, (token_id, group_name, (min_id, max_id)) in enumerate(zip(token_sequence, group_names, group_ranges)):
        usage_percent = ((token_id - min_id) / (max_id - min_id)) * 100
        bar_length = int(usage_percent / 5)  # æ¯5%ä¸€ä¸ªå­—ç¬¦
        bar = "â–ˆ" * bar_length + "â–‘" * (20 - bar_length)
        print(f"  {group_name:>4}: {token_id:3d} |{bar}| {usage_percent:5.1f}%")
    
    # éª¨æ¶ç»Ÿè®¡
    print(f"\nğŸ¦´ éª¨æ¶ç»Ÿè®¡:")
    original = demo_result['normalized_skeleton']
    reconstructed = demo_result['reconstructed_skeleton']
    
    joint_errors = np.sqrt(np.sum((original - reconstructed)**2, axis=1))
    max_error_joint = np.argmax(joint_errors)
    min_error_joint = np.argmin(joint_errors)
    
    print(f"  å…³èŠ‚ç‚¹æ•°é‡: {len(original)}")
    print(f"  æœ€å¤§è¯¯å·®å…³èŠ‚: #{max_error_joint} (è¯¯å·®: {joint_errors[max_error_joint]:.4f})")
    print(f"  æœ€å°è¯¯å·®å…³èŠ‚: #{min_error_joint} (è¯¯å·®: {joint_errors[min_error_joint]:.4f})")
    print(f"  å¹³å‡å…³èŠ‚è¯¯å·®: {np.mean(joint_errors):.4f}")
    
    # æµç¨‹æ€»ç»“
    print(f"\nğŸ”„ æµç¨‹æ€»ç»“:")
    print(f"  é›·è¾¾ç‰¹å¾å›¾: {demo_result['radar_data'].shape} â†’ 57ç»´ç‰¹å¾ â†’ 25å…³èŠ‚ç‚¹")
    print(f"  æ ‡å‡†åŒ–å¤„ç†: è´¨å¿ƒå¯¹é½ + è·ç¦»å½’ä¸€åŒ–") 
    print(f"  GCNç¼–ç : 5ä¸ªè¯­ä¹‰ç»„ â†’ 5ä¸ªç¦»æ•£Token")
    print(f"  é‡æ„è§£ç : Token â†’ 25å…³èŠ‚ç‚¹åæ ‡")
    
    print("=" * 60)

def create_real_implementation_guide():
    """åˆ›å»ºçœŸå®å®ç°æŒ‡å—"""
    print("\nğŸ“– çœŸå®å®ç°æŒ‡å—:")
    print("=" * 60)
    
    guide_content = """
ğŸš€ å¦‚ä½•è¿è¡ŒçœŸå®çš„éª¨æ¶æå–+é‡æ„æµç¨‹:

1ï¸âƒ£ å‡†å¤‡æƒé‡æ–‡ä»¶:
   â€¢ mars_transformer_best.pth (MARSéª¨æ¶æå–å™¨)
   â€¢ experiments/.../ckpt-best.pth (GCNé‡æ„å™¨)

2ï¸âƒ£ å®‰è£…ä¾èµ–:
   pip install torch torchvision numpy matplotlib

3ï¸âƒ£ è¿è¡Œå®Œæ•´æµç¨‹:
   python tools/skeleton_extraction_reconstruction_pipeline.py

4ï¸âƒ£ æˆ–åˆ†æ­¥æ‰§è¡Œ:
   # åŠ è½½MARSæ¨¡å‹
   from models.skeleton_extractor import MARSTransformerModel
   extractor = MARSTransformerModel(5, 57)
   extractor.load_state_dict(torch.load('mars_transformer_best.pth'))
   
   # åŠ è½½GCNæ¨¡å‹  
   from models.GCNSkeletonTokenizer import GCNSkeletonTokenizer
   gcn_model = GCNSkeletonTokenizer(config)
   gcn_model.load_state_dict(torch.load('ckpt-best.pth'))
   
   # å¤„ç†æ•°æ®
   radar_data = load_radar_data()  # (B, 5, 8, 8)
   skeleton_57d = extractor(radar_data)  # (B, 57)
   skeleton_25 = skeleton_57d[:, :75].reshape(-1, 25, 3)
   reconstruction = gcn_model(skeleton_25)

5ï¸âƒ£ è¾“å‡ºç»“æœ:
   â€¢ å¯è§†åŒ–å›¾åƒ: visualizations/skeleton_extraction_reconstruction/
   â€¢ æ•°å€¼ç»“æœ: pipeline_results.json
   â€¢ è´¨é‡æŒ‡æ ‡: MSEè¯¯å·®, VQæŸå¤±, Tokenåºåˆ—

ğŸ“Š é¢„æœŸæ€§èƒ½:
   â€¢ ä¼˜ç§€é‡æ„: MSE < 0.01
   â€¢ è‰¯å¥½é‡æ„: MSE < 0.05  
   â€¢ TokenèŒƒå›´: æ¯ç»„0-127 (åŠ åç§»å0-639)
   â€¢ å¤„ç†é€Ÿåº¦: ~100ms/æ ·æœ¬ (GPU)
"""
    
    print(guide_content)

def save_demo_results(demo_result, output_dir="demo_output"):
    """ä¿å­˜æ¼”ç¤ºç»“æœ"""
    print(f"\nğŸ’¾ ä¿å­˜æ¼”ç¤ºç»“æœåˆ°: {output_dir}/")
    
    os.makedirs(output_dir, exist_ok=True)
    
    # ä¿å­˜æ•°å€¼ç»“æœ
    results = {
        'pipeline_summary': {
            'mse_error': float(demo_result['mse_error']),
            'vq_loss': float(demo_result['vq_loss']),
            'token_sequence': demo_result['token_sequence'].tolist()
        },
        'quality_assessment': {
            'reconstruction_quality': 'excellent' if demo_result['mse_error'] < 0.01 else 'good' if demo_result['mse_error'] < 0.05 else 'needs_improvement',
            'token_distribution': {
                'head_spine': int(demo_result['token_sequence'][0]),
                'left_arm': int(demo_result['token_sequence'][1]), 
                'right_arm': int(demo_result['token_sequence'][2]),
                'left_leg': int(demo_result['token_sequence'][3]),
                'right_leg': int(demo_result['token_sequence'][4])
            }
        },
        'data_shapes': {
            'radar_input': demo_result['radar_data'].shape,
            'extracted_skeleton': demo_result['extracted_skeleton'].shape,
            'normalized_skeleton': demo_result['normalized_skeleton'].shape,
            'reconstructed_skeleton': demo_result['reconstructed_skeleton'].shape
        }
    }
    
    # ä¿å­˜JSONç»“æœ
    with open(f"{output_dir}/demo_results.json", 'w') as f:
        json.dump(results, f, indent=2)
    
    # ä¿å­˜numpyæ•°ç»„
    import numpy as np
    np.save(f"{output_dir}/radar_data.npy", demo_result['radar_data'])
    np.save(f"{output_dir}/extracted_skeleton.npy", demo_result['extracted_skeleton'])
    np.save(f"{output_dir}/reconstructed_skeleton.npy", demo_result['reconstructed_skeleton'])
    
    print(f"  âœ… demo_results.json - æ•°å€¼ç»“æœ")
    print(f"  âœ… *.npy - éª¨æ¶æ•°æ®æ–‡ä»¶")

def main():
    """ä¸»å‡½æ•°"""
    print("=" * 80)
    print("ğŸ¦´ éª¨æ¶æå– + GCNé‡æ„æµæ°´çº¿æ¼”ç¤º")
    print("=" * 80)
    
    # æ£€æŸ¥ç¯å¢ƒ
    if not check_dependencies():
        print("\nâš ï¸ ç¯å¢ƒæ£€æŸ¥æœªå®Œå…¨é€šè¿‡ï¼Œå°†è¿è¡Œæ¨¡æ‹Ÿæ¼”ç¤º")
    else:
        print("\nâœ… ç¯å¢ƒæ£€æŸ¥é€šè¿‡ï¼Œå¯è¿è¡ŒçœŸå®æµç¨‹")
    
    try:
        # è¿è¡Œæ¨¡æ‹Ÿæ¼”ç¤º
        demo_result = create_mock_pipeline_demo()
        
        # ç”Ÿæˆå¯è§†åŒ–
        create_text_visualization(demo_result)
        
        # ä¿å­˜ç»“æœ
        save_demo_results(demo_result)
        
        # æ˜¾ç¤ºå®ç°æŒ‡å—
        create_real_implementation_guide()
        
        print("\nğŸ‰ æ¼”ç¤ºå®Œæˆï¼")
        
    except Exception as e:
        print(f"\nâŒ æ¼”ç¤ºæ‰§è¡Œå¤±è´¥: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()