"""
åˆ†æNTUæ•°æ®é›†åœ¨è®­ç»ƒå’Œæµ‹è¯•ä¸­çš„å®é™…ä½¿ç”¨æƒ…å†µ
ç»Ÿè®¡æ–‡ä»¶æ•°é‡ã€æ•°æ®åˆ†å¸ƒã€è®­ç»ƒ/éªŒè¯/æµ‹è¯•é›†åˆ’åˆ†ç­‰
"""

import os
import sys
import numpy as np
import re
from collections import defaultdict

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from datasets.NTUDatasetAugmented import NTUAugmented


def analyze_ntu_data_directory():
    """åˆ†æNTUæ•°æ®ç›®å½•çš„æ–‡ä»¶æƒ…å†µ"""
    print("ğŸ“ åˆ†æNTUæ•°æ®ç›®å½•")
    print("=" * 50)
    
    data_path = '../data/NTU-RGB+D'
    
    if not os.path.exists(data_path):
        print(f"âŒ æ•°æ®ç›®å½•ä¸å­˜åœ¨: {data_path}")
        return None
    
    # ç»Ÿè®¡.skeletonæ–‡ä»¶
    skeleton_files = []
    for file in os.listdir(data_path):
        if file.endswith('.skeleton'):
            skeleton_files.append(file)
    
    print(f"ğŸ“Š æ€».skeletonæ–‡ä»¶æ•°: {len(skeleton_files)}")
    
    # åˆ†ææ–‡ä»¶åæ¨¡å¼
    action_counts = defaultdict(int)
    subject_counts = defaultdict(int)
    camera_counts = defaultdict(int)
    setup_counts = defaultdict(int)
    
    for file in skeleton_files:
        # NTUæ–‡ä»¶åæ ¼å¼: SsssCcccPpppRrrrAaaa.skeleton
        # S: setup, C: camera, P: performer, R: replication, A: action
        match = re.match(r'S(\d{3})C(\d{3})P(\d{3})R(\d{3})A(\d{3})\.skeleton', file)
        if match:
            setup, camera, performer, replication, action = match.groups()
            action_counts[int(action)] += 1
            subject_counts[int(performer)] += 1
            camera_counts[int(camera)] += 1
            setup_counts[int(setup)] += 1
    
    print(f"ğŸ“ˆ åŠ¨ä½œç±»åˆ«æ•°: {len(action_counts)} (èŒƒå›´: {min(action_counts.keys())}-{max(action_counts.keys())})")
    print(f"ğŸ“ˆ å—è¯•è€…æ•°: {len(subject_counts)} (èŒƒå›´: {min(subject_counts.keys())}-{max(subject_counts.keys())})")
    print(f"ğŸ“ˆ æ‘„åƒå¤´æ•°: {len(camera_counts)} (èŒƒå›´: {min(camera_counts.keys())}-{max(camera_counts.keys())})")
    print(f"ğŸ“ˆ è®¾ç½®æ•°: {len(setup_counts)} (èŒƒå›´: {min(setup_counts.keys())}-{max(setup_counts.keys())})")
    
    return {
        'total_files': len(skeleton_files),
        'action_counts': dict(action_counts),
        'subject_counts': dict(subject_counts),
        'camera_counts': dict(camera_counts),
        'setup_counts': dict(setup_counts),
        'skeleton_files': skeleton_files
    }


def analyze_dataset_splits():
    """åˆ†ææ•°æ®é›†çš„è®­ç»ƒ/éªŒè¯/æµ‹è¯•åˆ’åˆ†"""
    print(f"\nğŸ“Š åˆ†ææ•°æ®é›†åˆ’åˆ†")
    print("=" * 50)
    
    splits = ['train', 'val', 'test']
    split_info = {}
    
    for split in splits:
        try:
            # åˆ›å»ºé…ç½®
            class Config:
                def __init__(self, subset):
                    self.DATA_PATH = '../data/NTU-RGB+D'
                    self.subset = subset
                    self.N_POINTS = 25
                    self.npoints = 720
                    self.density_uniform = True
                    self.min_points_per_bone = 3
                    self.action_filter = 'dvae'
                    self.augment = False
                    self.whole = False
                
                def get(self, key, default=None):
                    return getattr(self, key, default)
            
            config = Config(split)
            dataset = NTUAugmented(config)
            
            split_info[split] = {
                'size': len(dataset),
                'config': config
            }
            
            print(f"âœ… {split.upper()}é›†: {len(dataset)} ä¸ªæ ·æœ¬")
            
        except Exception as e:
            print(f"âŒ {split.upper()}é›†åŠ è½½å¤±è´¥: {e}")
            split_info[split] = {'size': 0, 'error': str(e)}
    
    return split_info


def analyze_action_filter():
    """åˆ†æåŠ¨ä½œè¿‡æ»¤çš„æ•ˆæœ"""
    print(f"\nğŸ¯ åˆ†æåŠ¨ä½œè¿‡æ»¤æ•ˆæœ")
    print("=" * 50)
    
    # åˆ†æä¸åŒaction_filterçš„æ•ˆæœ
    filters = ['dvae', 'daily', 'rehab', 'all']
    filter_results = {}
    
    for filter_type in filters:
        try:
            class Config:
                def __init__(self, action_filter):
                    self.DATA_PATH = '../data/NTU-RGB+D'
                    self.subset = 'train'
                    self.N_POINTS = 25
                    self.npoints = 720
                    self.density_uniform = True
                    self.min_points_per_bone = 3
                    self.action_filter = action_filter
                    self.augment = False
                    self.whole = False
                
                def get(self, key, default=None):
                    return getattr(self, key, default)
            
            config = Config(filter_type)
            dataset = NTUAugmented(config)
            
            filter_results[filter_type] = len(dataset)
            print(f"ğŸ“Š {filter_type.upper()}è¿‡æ»¤: {len(dataset)} ä¸ªæ ·æœ¬")
            
        except Exception as e:
            print(f"âŒ {filter_type.upper()}è¿‡æ»¤å¤±è´¥: {e}")
            filter_results[filter_type] = 0
    
    return filter_results


def analyze_training_logs():
    """åˆ†æè®­ç»ƒæ—¥å¿—ä¸­çš„æ•°æ®ä½¿ç”¨æƒ…å†µ"""
    print(f"\nğŸ“‹ åˆ†æè®­ç»ƒæ—¥å¿—")
    print("=" * 50)
    
    log_dir = '../experiments/skeleton_dvae_pretrain/NTU_models/ntu_skeleton_tokenizer_720pts'
    
    if not os.path.exists(log_dir):
        print(f"âŒ æ—¥å¿—ç›®å½•ä¸å­˜åœ¨: {log_dir}")
        return None
    
    # æŸ¥æ‰¾æœ€æ–°çš„æ—¥å¿—æ–‡ä»¶
    log_files = [f for f in os.listdir(log_dir) if f.endswith('.log')]
    if not log_files:
        print(f"âŒ æœªæ‰¾åˆ°æ—¥å¿—æ–‡ä»¶")
        return None
    
    latest_log = sorted(log_files)[-1]
    log_path = os.path.join(log_dir, latest_log)
    
    print(f"ğŸ“„ åˆ†ææ—¥å¿—æ–‡ä»¶: {latest_log}")
    
    training_info = {
        'train_batches': 0,
        'val_samples': 0,
        'epochs_completed': 0
    }
    
    try:
        with open(log_path, 'r', encoding='utf-8') as f:
            for line in f:
                # æŸ¥æ‰¾è®­ç»ƒæ‰¹æ¬¡ä¿¡æ¯
                if '[Batch' in line and '/31]' in line:
                    training_info['train_batches'] = 31  # æ¯ä¸ªepochæœ‰31ä¸ªbatch
                
                # æŸ¥æ‰¾éªŒè¯æ ·æœ¬ä¿¡æ¯
                if 'human_skeleton' in line and 'Sample' in line:
                    match = re.search(r'human_skeleton\s+(\d+)', line)
                    if match:
                        training_info['val_samples'] = int(match.group(1))
                
                # æŸ¥æ‰¾å®Œæˆçš„epochæ•°
                if '[Training] EPOCH:' in line:
                    match = re.search(r'EPOCH:\s+(\d+)', line)
                    if match:
                        training_info['epochs_completed'] = max(training_info['epochs_completed'], int(match.group(1)) + 1)
    
    except Exception as e:
        print(f"âŒ è¯»å–æ—¥å¿—å¤±è´¥: {e}")
        return None
    
    print(f"ğŸ“Š è®­ç»ƒä¿¡æ¯:")
    print(f"  æ¯ä¸ªepochçš„batchæ•°: {training_info['train_batches']}")
    print(f"  éªŒè¯é›†æ ·æœ¬æ•°: {training_info['val_samples']}")
    print(f"  å·²å®Œæˆepochæ•°: {training_info['epochs_completed']}")
    
    # è®¡ç®—è®­ç»ƒé›†å¤§å°
    if training_info['train_batches'] > 0:
        batch_size = 32  # ä»é…ç½®æ–‡ä»¶å¾—çŸ¥
        train_samples = training_info['train_batches'] * batch_size
        print(f"  è®­ç»ƒé›†æ ·æœ¬æ•°: ~{train_samples} (31 batches Ã— 32 batch_size)")
    
    return training_info


def calculate_data_usage_summary():
    """è®¡ç®—æ•°æ®ä½¿ç”¨æ€»ç»“"""
    print(f"\nğŸ“Š æ•°æ®ä½¿ç”¨æ€»ç»“")
    print("=" * 50)
    
    # åˆ†æç›®å½•ä¿¡æ¯
    dir_info = analyze_ntu_data_directory()
    
    # åˆ†ææ•°æ®é›†åˆ’åˆ†
    split_info = analyze_dataset_splits()
    
    # åˆ†æåŠ¨ä½œè¿‡æ»¤
    filter_info = analyze_action_filter()
    
    # åˆ†æè®­ç»ƒæ—¥å¿—
    log_info = analyze_training_logs()
    
    print(f"\nğŸ¯ æ€»ç»“:")
    print(f"=" * 30)
    
    if dir_info:
        print(f"ğŸ“ æ•°æ®ç›®å½•:")
        print(f"  æ€».skeletonæ–‡ä»¶: {dir_info['total_files']}")
        print(f"  åŠ¨ä½œç±»åˆ«æ•°: {len(dir_info['action_counts'])}")
        print(f"  å—è¯•è€…æ•°: {len(dir_info['subject_counts'])}")
    
    if split_info:
        total_used = sum(info['size'] for info in split_info.values() if 'size' in info)
        print(f"\nğŸ“Š æ•°æ®é›†ä½¿ç”¨:")
        for split, info in split_info.items():
            if 'size' in info:
                print(f"  {split.upper()}é›†: {info['size']} æ ·æœ¬")
        print(f"  æ€»ä½¿ç”¨æ ·æœ¬: {total_used}")
    
    if filter_info:
        print(f"\nğŸ¯ åŠ¨ä½œè¿‡æ»¤æ•ˆæœ:")
        for filter_type, count in filter_info.items():
            print(f"  {filter_type.upper()}: {count} æ ·æœ¬")
    
    if log_info:
        print(f"\nğŸƒ å®é™…è®­ç»ƒä½¿ç”¨:")
        print(f"  è®­ç»ƒé›†: ~{log_info['train_batches'] * 32} æ ·æœ¬")
        print(f"  éªŒè¯é›†: {log_info['val_samples']} æ ·æœ¬")
        print(f"  å·²è®­ç»ƒ: {log_info['epochs_completed']} epochs")
    
    # è®¡ç®—ä½¿ç”¨ç‡
    if dir_info and split_info:
        total_files = dir_info['total_files']
        total_used = sum(info['size'] for info in split_info.values() if 'size' in info)
        usage_rate = (total_used / total_files) * 100 if total_files > 0 else 0
        
        print(f"\nğŸ“ˆ æ•°æ®ä½¿ç”¨ç‡:")
        print(f"  æ€»æ–‡ä»¶æ•°: {total_files}")
        print(f"  å®é™…ä½¿ç”¨: {total_used}")
        print(f"  ä½¿ç”¨ç‡: {usage_rate:.1f}%")


def main():
    """ä¸»åˆ†æå‡½æ•°"""
    print("ğŸ” NTUæ•°æ®é›†ä½¿ç”¨æƒ…å†µåˆ†æ")
    print("=" * 60)
    
    # æ‰§è¡Œå®Œæ•´åˆ†æ
    calculate_data_usage_summary()
    
    print(f"\nğŸ‰ åˆ†æå®Œæˆï¼")


if __name__ == '__main__':
    main()
