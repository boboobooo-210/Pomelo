#!/usr/bin/env python3
"""
ç æœ¬åˆ°æ–‡æœ¬æ˜ å°„ç³»ç»Ÿ
å®ç°ä»Tokenåºåˆ—åˆ°è‡ªç„¶è¯­è¨€æè¿°çš„å®Œæ•´æ˜ å°„
"""

import json
import os
import numpy as np
from datetime import datetime
from typing import Dict, List, Tuple, Optional, Union

class CodebookTextMapper:
    """ç æœ¬åˆ°æ–‡æœ¬çš„æ˜ å°„å™¨"""
    
    def __init__(self, mapping_file: str = "codebook_action_mappings.json"):
        self.mapping_file = mapping_file
        self.part_mappings = {}
        self.global_mappings = {}
        self.statistics = {}
        
        # èº«ä½“éƒ¨ä½ä¿¡æ¯
        self.part_names = ['head_spine', 'left_arm', 'right_arm', 'left_leg', 'right_leg']
        self.part_display_names = ['å¤´éƒ¨è„ŠæŸ±', 'å·¦è‡‚', 'å³è‡‚', 'å·¦è…¿', 'å³è…¿']
        
        # é»˜è®¤åŠ¨ä½œæ¨¡æ¿ï¼ˆç”¨äºæ–°æ˜ å°„çš„åˆå§‹åŒ–ï¼‰
        self.default_action_templates = {
            'head_spine': [
                "ä¸­æ€§å§¿æ€", "æŠ¬å¤´å‘ä¸Š", "ä½å¤´å‘ä¸‹", "å·¦è½¬å¤´éƒ¨", "å³è½¬å¤´éƒ¨",
                "æŒºç›´è„ŠæŸ±", "å‰å€¾èº«ä½“", "åä»°èº«ä½“", "å·¦ä¾§å¼¯æ›²", "å³ä¾§å¼¯æ›²"
            ],
            'left_arm': [
                "è‡ªç„¶ä¸‹å‚", "ä¸Šä¸¾è¿‡å¤´", "å‰ä¼¸æŒ‡å‘", "ä¾§å¹³ä¸¾", "å¼¯æ›²æ’‘è…°",
                "äº¤å‰èƒ¸å‰", "æŒ¥æ‰‹åŠ¨ä½œ", "èƒŒåä¼¸å±•", "æ¡æ‹³å‡†å¤‡", "æ”¾æ¾æ‘†åŠ¨"
            ],
            'right_arm': [
                "è‡ªç„¶ä¸‹å‚", "ä¸Šä¸¾è¿‡å¤´", "å‰ä¼¸æŒ‡å‘", "ä¾§å¹³ä¸¾", "å¼¯æ›²æ’‘è…°",
                "äº¤å‰èƒ¸å‰", "æŒ¥æ‰‹åŠ¨ä½œ", "èƒŒåä¼¸å±•", "æ¡æ‹³å‡†å¤‡", "æ”¾æ¾æ‘†åŠ¨"
            ],
            'left_leg': [
                "ç›´ç«‹æ”¯æ’‘", "å¾®å¼¯å‡†å¤‡", "æŠ¬èµ·å‰è¸", "ä¾§å‘è¿ˆæ­¥", "è¹²å§¿å¼¯æ›²",
                "åé€€å‡†å¤‡", "è¸¢è…¿åŠ¨ä½œ", "ç«™ç«‹å¹³è¡¡", "äº¤å‰ç«™ç«‹", "è·³è·ƒå‡†å¤‡"
            ],
            'right_leg': [
                "ç›´ç«‹æ”¯æ’‘", "å¾®å¼¯å‡†å¤‡", "æŠ¬èµ·å‰è¸", "ä¾§å‘è¿ˆæ­¥", "è¹²å§¿å¼¯æ›²", 
                "åé€€å‡†å¤‡", "è¸¢è…¿åŠ¨ä½œ", "ç«™ç«‹å¹³è¡¡", "äº¤å‰ç«™ç«‹", "è·³è·ƒå‡†å¤‡"
            ]
        }
        
        # åŠ è½½ç°æœ‰æ˜ å°„
        self.load_mappings()
        
    def load_mappings(self) -> bool:
        """åŠ è½½ç æœ¬æ˜ å°„è¡¨"""
        if os.path.exists(self.mapping_file):
            try:
                with open(self.mapping_file, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                    
                self.part_mappings = data.get('part_mappings', {})
                
                # å¤„ç†å…¨å±€æ˜ å°„çš„é”®è½¬æ¢
                global_mappings_raw = data.get('global_mappings', {})
                self.global_mappings = {}
                for key_str, value in global_mappings_raw.items():
                    try:
                        # å°†å­—ç¬¦ä¸²é”®è½¬æ¢å›tuple
                        if key_str.startswith('(') and key_str.endswith(')'):
                            key_tuple = eval(key_str)
                            self.global_mappings[key_tuple] = value
                    except:
                        continue
                        
                self.statistics = data.get('statistics', {})
                
                print(f"âœ… æˆåŠŸåŠ è½½ç æœ¬æ˜ å°„è¡¨: {len(self.part_mappings)} ä¸ªéƒ¨ä½æ˜ å°„")
                return True
                
            except Exception as e:
                print(f"âš ï¸ åŠ è½½æ˜ å°„è¡¨å¤±è´¥: {e}")
                
        print("ğŸ“ åˆ›å»ºæ–°çš„æ˜ å°„è¡¨...")
        self._initialize_default_mappings()
        return False
        
    def _initialize_default_mappings(self):
        """åˆå§‹åŒ–é»˜è®¤æ˜ å°„è¡¨"""
        for part_name in self.part_names:
            self.part_mappings[part_name] = {}
            
        # å¯ä»¥é¢„è®¾ä¸€äº›å¸¸è§çš„æ˜ å°„
        self._create_sample_mappings()
        
    def _create_sample_mappings(self):
        """åˆ›å»ºç¤ºä¾‹æ˜ å°„ï¼ˆç”¨äºæ¼”ç¤ºï¼‰"""
        sample_mappings = {
            'head_spine': {
                15: {'semantic': 'å¤´éƒ¨ä¸­æ€§', 'confidence': 0.95, 'frequency': 45},
                28: {'semantic': 'æŠ¬å¤´å‘ä¸Š', 'confidence': 0.92, 'frequency': 32},
                45: {'semantic': 'ä½å¤´å‘ä¸‹', 'confidence': 0.88, 'frequency': 28}
            },
            'left_arm': {
                32: {'semantic': 'è‡ªç„¶ä¸‹å‚', 'confidence': 0.94, 'frequency': 55},
                58: {'semantic': 'ä¸Šä¸¾è¿‡å¤´', 'confidence': 0.97, 'frequency': 38},
                76: {'semantic': 'å‰ä¼¸æŒ‡å‘', 'confidence': 0.89, 'frequency': 25}
            },
            'right_arm': {
                41: {'semantic': 'è‡ªç„¶ä¸‹å‚', 'confidence': 0.95, 'frequency': 52},
                65: {'semantic': 'ä¸Šä¸¾è¿‡å¤´', 'confidence': 0.96, 'frequency': 36},
                119: {'semantic': 'æŒ¥æ‰‹åŠ¨ä½œ', 'confidence': 0.88, 'frequency': 22}
            },
            'left_leg': {
                18: {'semantic': 'ç›´ç«‹æ”¯æ’‘', 'confidence': 0.98, 'frequency': 68},
                72: {'semantic': 'æŠ¬èµ·å‰è¸', 'confidence': 0.89, 'frequency': 31},
                113: {'semantic': 'è¹²å§¿å¼¯æ›²', 'confidence': 0.82, 'frequency': 18}
            },
            'right_leg': {
                23: {'semantic': 'ç›´ç«‹æ”¯æ’‘', 'confidence': 0.97, 'frequency': 65},
                78: {'semantic': 'æŠ¬èµ·å‰è¸', 'confidence': 0.88, 'frequency': 29},
                126: {'semantic': 'è¹²å§¿å¼¯æ›²', 'confidence': 0.81, 'frequency': 16}
            }
        }
        
        for part, mappings in sample_mappings.items():
            self.part_mappings[part].update(mappings)
            
        # ç¤ºä¾‹å…¨å±€æ˜ å°„
        self.global_mappings = {
            (28, 58, 65, 18, 23): {
                'action': 'åŒæ‰‹ä¸¾é«˜åº†ç¥åŠ¨ä½œ',
                'confidence': 0.94,
                'category': 'celebration',
                'frequency': 15
            },
            (15, 76, 119, 72, 23): {
                'action': 'æŒ‡å‘å¹¶æŒ¥æ‰‹é—®å€™',
                'confidence': 0.91,
                'category': 'greeting', 
                'frequency': 12
            }
        }
        
    def save_mappings(self):
        """ä¿å­˜æ˜ å°„è¡¨åˆ°æ–‡ä»¶"""
        mapping_data = {
            'part_mappings': self.part_mappings,
            'global_mappings': {str(k): v for k, v in self.global_mappings.items()},
            'statistics': self.statistics,
            'metadata': {
                'creation_date': datetime.now().isoformat(),
                'total_parts': len(self.part_names),
                'version': '1.0'
            }
        }
        
        with open(self.mapping_file, 'w', encoding='utf-8') as f:
            json.dump(mapping_data, f, indent=2, ensure_ascii=False)
            
        print(f"ğŸ’¾ æ˜ å°„è¡¨å·²ä¿å­˜åˆ°: {self.mapping_file}")
        
    def map_tokens_to_text(self, token_sequence: List[int]) -> Dict:
        """å°†Tokenåºåˆ—æ˜ å°„ä¸ºæ–‡æœ¬æè¿°"""
        if len(token_sequence) != 5:
            return {
                "error": "Tokenåºåˆ—é•¿åº¦å¿…é¡»ä¸º5",
                "token_sequence": token_sequence
            }
            
        result = {
            'token_sequence': token_sequence,
            'part_descriptions': {},
            'detailed_descriptions': {},
            'overall_action': None,
            'confidence_scores': {},
            'natural_language': '',
            'timestamp': datetime.now().isoformat()
        }
        
        # 1. è§£æå„éƒ¨ä½æè¿°
        total_confidence = 0
        valid_parts = 0
        
        for i, (part_name, display_name) in enumerate(zip(self.part_names, self.part_display_names)):
            token_id = token_sequence[i]
            
            if str(token_id) in self.part_mappings[part_name]:
                mapping = self.part_mappings[part_name][str(token_id)]
                semantic = mapping['semantic']
                confidence = mapping['confidence']
                
                result['part_descriptions'][display_name] = semantic
                result['confidence_scores'][display_name] = confidence
                result['detailed_descriptions'][display_name] = {
                    'token_id': token_id,
                    'action': semantic,
                    'confidence': confidence,
                    'reliability': self._get_reliability_level(confidence),
                    'frequency': mapping.get('frequency', 0)
                }
                
                total_confidence += confidence
                valid_parts += 1
                
            else:
                # æœªçŸ¥Tokençš„å¤„ç†
                result['part_descriptions'][display_name] = f'æœªè¯†åˆ«åŠ¨ä½œ (Token: {token_id})'
                result['confidence_scores'][display_name] = 0.0
                result['detailed_descriptions'][display_name] = {
                    'token_id': token_id,
                    'action': 'æœªè¯†åˆ«åŠ¨ä½œ',
                    'confidence': 0.0,
                    'reliability': 'ä½',
                    'frequency': 0
                }
                
        # 2. æŸ¥æ‰¾å…¨å±€åŠ¨ä½œåŒ¹é…
        token_tuple = tuple(token_sequence)
        if token_tuple in self.global_mappings:
            global_mapping = self.global_mappings[token_tuple]
            result['overall_action'] = {
                'name': global_mapping['action'],
                'confidence': global_mapping['confidence'],
                'category': global_mapping['category'],
                'type': 'exact_match',
                'frequency': global_mapping.get('frequency', 0)
            }
        else:
            # ç”Ÿæˆç»„åˆåŠ¨ä½œæè¿°
            result['overall_action'] = self._generate_composite_action(result['part_descriptions'])
            
        # 3. è®¡ç®—æ•´ä½“ç½®ä¿¡åº¦
        avg_confidence = total_confidence / valid_parts if valid_parts > 0 else 0.0
        result['average_confidence'] = avg_confidence
        
        # 4. ç”Ÿæˆè‡ªç„¶è¯­è¨€æè¿°
        result['natural_language'] = self._generate_natural_language(result)
        
        return result
        
    def _get_reliability_level(self, confidence: float) -> str:
        """æ ¹æ®ç½®ä¿¡åº¦è¿”å›å¯é æ€§ç­‰çº§"""
        if confidence >= 0.9:
            return 'é«˜'
        elif confidence >= 0.8:
            return 'ä¸­'
        elif confidence >= 0.6:
            return 'ä¸­ä½'
        else:
            return 'ä½'
            
    def _generate_composite_action(self, part_descriptions: Dict) -> Dict:
        """åŸºäºå±€éƒ¨æè¿°ç”Ÿæˆç»„åˆåŠ¨ä½œ"""
        descriptions = list(part_descriptions.values())
        description_text = ' '.join(descriptions)
        
        # æ£€æµ‹åŠ¨ä½œæ¨¡å¼
        if 'ä¸Šä¸¾' in description_text and description_text.count('ä¸Šä¸¾') >= 2:
            return {
                'name': 'åŒè‡‚ä¸Šä¸¾åŠ¨ä½œ',
                'confidence': 0.85,
                'category': 'arm_movement',
                'type': 'pattern_match'
            }
        elif 'æŒ¥æ‰‹' in description_text:
            return {
                'name': 'æŒ¥æ‰‹é—®å€™åŠ¨ä½œ',
                'confidence': 0.82,
                'category': 'greeting',
                'type': 'pattern_match'
            }
        elif 'è¹²' in description_text and description_text.count('è¹²') >= 2:
            return {
                'name': 'è¹²å§¿ç›¸å…³åŠ¨ä½œ',
                'confidence': 0.79,
                'category': 'posture_change',
                'type': 'pattern_match'
            }
        elif 'è¿ˆæ­¥' in description_text or 'å‰è¸' in description_text:
            return {
                'name': 'æ­¥è¡Œç§»åŠ¨åŠ¨ä½œ',
                'confidence': 0.77,
                'category': 'locomotion',
                'type': 'pattern_match'
            }
        elif 'ä½å¤´' in description_text and ('ä¸‹å‚' in description_text or 'å¼¯æ›²' in description_text):
            return {
                'name': 'æ£€æŸ¥è§‚å¯ŸåŠ¨ä½œ',
                'confidence': 0.74,
                'category': 'examination',
                'type': 'pattern_match'
            }
        else:
            return {
                'name': 'å¤åˆåŠ¨ä½œç»„åˆ',
                'confidence': 0.70,
                'category': 'complex',
                'type': 'composite'
            }
            
    def _generate_natural_language(self, result: Dict) -> str:
        """ç”Ÿæˆè‡ªç„¶è¯­è¨€æè¿°"""
        parts = result['part_descriptions']
        overall = result['overall_action']
        avg_confidence = result.get('average_confidence', 0.0)
        
        # æ„å»ºæœ‰æ•ˆçš„éƒ¨ä½æè¿°
        valid_part_texts = []
        for part_name, action in parts.items():
            if 'æœªè¯†åˆ«' not in action:
                valid_part_texts.append(f"{part_name}å‘ˆç°{action}")
                
        # æ„å»ºå®Œæ•´æè¿°
        if overall and overall['confidence'] > 0.85:
            # é«˜ç½®ä¿¡åº¦çš„æ•´ä½“åŠ¨ä½œ
            description = f"è¯†åˆ«ä¸ºã€{overall['name']}ã€‘"
            if valid_part_texts:
                description += f"ï¼Œå…·ä½“è¡¨ç°ä¸ºï¼š{', '.join(valid_part_texts)}"
        else:
            # åŸºäºå±€éƒ¨æè¿°çš„ç»„åˆ
            if valid_part_texts:
                description = f"æ£€æµ‹åˆ°åŠ¨ä½œç»„åˆï¼š{', '.join(valid_part_texts)}"
                if overall:
                    description += f"ï¼Œæ•´ä½“åˆ¤æ–­ä¸º{overall['name']}"
            else:
                description = "åŠ¨ä½œè¯†åˆ«ç½®ä¿¡åº¦è¾ƒä½ï¼Œå»ºè®®äººå·¥ç¡®è®¤"
                
        # æ·»åŠ ç½®ä¿¡åº¦ä¿¡æ¯
        if avg_confidence > 0:
            confidence_text = f"(å¹³å‡ç½®ä¿¡åº¦: {avg_confidence:.2f})"
            description += f" {confidence_text}"
            
        return description
        
    def add_token_mapping(self, part_name: str, token_id: int, semantic: str, 
                         confidence: float = 0.8, frequency: int = 1):
        """æ·»åŠ æ–°çš„Tokenæ˜ å°„"""
        if part_name not in self.part_names:
            raise ValueError(f"æ— æ•ˆçš„éƒ¨ä½åç§°: {part_name}")
            
        if part_name not in self.part_mappings:
            self.part_mappings[part_name] = {}
            
        self.part_mappings[part_name][str(token_id)] = {
            'semantic': semantic,
            'confidence': confidence,
            'frequency': frequency
        }
        
        print(f"âœ… æ·»åŠ æ˜ å°„: {part_name} Token {token_id} -> {semantic}")
        
    def add_global_mapping(self, token_sequence: List[int], action_name: str,
                          confidence: float = 0.8, category: str = 'custom'):
        """æ·»åŠ æ–°çš„å…¨å±€åŠ¨ä½œæ˜ å°„"""
        if len(token_sequence) != 5:
            raise ValueError("Tokenåºåˆ—é•¿åº¦å¿…é¡»ä¸º5")
            
        token_tuple = tuple(token_sequence)
        self.global_mappings[token_tuple] = {
            'action': action_name,
            'confidence': confidence,
            'category': category,
            'frequency': 1
        }
        
        print(f"âœ… æ·»åŠ å…¨å±€æ˜ å°„: {token_sequence} -> {action_name}")
        
    def get_mapping_statistics(self) -> Dict:
        """è·å–æ˜ å°„ç»Ÿè®¡ä¿¡æ¯"""
        stats = {
            'part_coverage': {},
            'total_mappings': 0,
            'global_mappings_count': len(self.global_mappings),
            'average_confidence_by_part': {}
        }
        
        for part_name in self.part_names:
            if part_name in self.part_mappings:
                mappings = self.part_mappings[part_name]
                stats['part_coverage'][part_name] = len(mappings)
                stats['total_mappings'] += len(mappings)
                
                # è®¡ç®—å¹³å‡ç½®ä¿¡åº¦
                if mappings:
                    confidences = [m['confidence'] for m in mappings.values()]
                    stats['average_confidence_by_part'][part_name] = sum(confidences) / len(confidences)
                else:
                    stats['average_confidence_by_part'][part_name] = 0.0
            else:
                stats['part_coverage'][part_name] = 0
                stats['average_confidence_by_part'][part_name] = 0.0
                
        return stats
        
    def export_mapping_report(self, output_file: str = "mapping_report.txt"):
        """å¯¼å‡ºæ˜ å°„æŠ¥å‘Š"""
        stats = self.get_mapping_statistics()
        
        with open(output_file, 'w', encoding='utf-8') as f:
            f.write("ç æœ¬æ˜ å°„ç³»ç»ŸæŠ¥å‘Š\n")
            f.write("=" * 50 + "\n\n")
            f.write(f"ç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n\n")
            
            f.write("æ˜ å°„è¦†ç›–æƒ…å†µ:\n")
            for part_name, count in stats['part_coverage'].items():
                display_name = dict(zip(self.part_names, self.part_display_names))[part_name]
                confidence = stats['average_confidence_by_part'][part_name]
                f.write(f"  {display_name}: {count} ä¸ªæ˜ å°„ (å¹³å‡ç½®ä¿¡åº¦: {confidence:.3f})\n")
                
            f.write(f"\næ€»è®¡å±€éƒ¨æ˜ å°„: {stats['total_mappings']} ä¸ª\n")
            f.write(f"å…¨å±€åŠ¨ä½œæ˜ å°„: {stats['global_mappings_count']} ä¸ª\n\n")
            
            # è¯¦ç»†æ˜ å°„åˆ—è¡¨
            f.write("è¯¦ç»†æ˜ å°„åˆ—è¡¨:\n")
            f.write("-" * 30 + "\n")
            
            for part_name, display_name in zip(self.part_names, self.part_display_names):
                f.write(f"\n{display_name}:\n")
                if part_name in self.part_mappings:
                    for token_id, mapping in sorted(self.part_mappings[part_name].items()):
                        f.write(f"  Token {token_id}: {mapping['semantic']} "
                               f"(ç½®ä¿¡åº¦: {mapping['confidence']:.2f}, "
                               f"é¢‘æ¬¡: {mapping.get('frequency', 0)})\n")
                else:
                    f.write("  æš‚æ— æ˜ å°„\n")
                    
        print(f"ğŸ“Š æ˜ å°„æŠ¥å‘Šå·²å¯¼å‡ºåˆ°: {output_file}")

if __name__ == "__main__":
    # æµ‹è¯•æ˜ å°„å™¨
    mapper = CodebookTextMapper()
    
    # æµ‹è¯•Tokenæ˜ å°„
    test_tokens = [28, 58, 65, 18, 23]
    result = mapper.map_tokens_to_text(test_tokens)
    
    print("ğŸ§ª æµ‹è¯•ç æœ¬æ˜ å°„:")
    print(f"Tokenåºåˆ—: {test_tokens}")
    print(f"è‡ªç„¶è¯­è¨€: {result['natural_language']}")
    
    # ä¿å­˜æ˜ å°„
    mapper.save_mappings()