"""
NTU RGB+D æ•°æ®é¢„å¤„ç†è„šæœ¬
æ ¹æ®åŠ¨ä½œç±»å‹è¿›è¡Œæ•°æ®åˆ’åˆ†å’Œé¢„å¤„ç†
"""

import os
import sys
import numpy as np
import argparse
from collections import defaultdict, Counter
import yaml
import random

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

# åŠ¨ä½œåˆ†ç±»å®šä¹‰
SINGLE_DAILY_ACTIONS = list(range(1, 41)) + list(range(61, 103))  # A1-A40, A61-A102 (é™¤åº·å¤åŠ¨ä½œ)
REHABILITATION_ACTIONS = list(range(41, 50)) + list(range(103, 106))  # A41-A49, A103-A105
INTERACTION_ACTIONS = list(range(50, 61)) + list(range(107, 121))  # A50-A60, A107-A120

# ç§»é™¤åº·å¤åŠ¨ä½œä»å•äººæ—¥å¸¸åŠ¨ä½œä¸­
for action in REHABILITATION_ACTIONS:
    if action in SINGLE_DAILY_ACTIONS:
        SINGLE_DAILY_ACTIONS.remove(action)

# DVAEè®­ç»ƒä½¿ç”¨çš„åŠ¨ä½œ
DVAE_ACTIONS = SINGLE_DAILY_ACTIONS + REHABILITATION_ACTIONS


def parse_filename(filename):
    """è§£æNTUæ–‡ä»¶åè·å–ä¿¡æ¯"""
    # æ–‡ä»¶åæ ¼å¼: S001C001P001R001A001.skeleton
    parts = filename.replace('.skeleton', '').split('C')[0].split('P')
    
    setup = int(filename[1:4])  # S001 -> 1
    camera = int(filename[5:8])  # C001 -> 1
    person = int(filename[9:12])  # P001 -> 1
    replication = int(filename[13:16])  # R001 -> 1
    action = int(filename[17:20])  # A001 -> 1
    
    return {
        'setup': setup,
        'camera': camera, 
        'person': person,
        'replication': replication,
        'action': action,
        'filename': filename
    }


def analyze_dataset(data_path):
    """åˆ†ææ•°æ®é›†ç»Ÿè®¡ä¿¡æ¯"""
    print("ğŸ” åˆ†æNTU RGB+Dæ•°æ®é›†...")
    
    skeleton_files = [f for f in os.listdir(data_path) if f.endswith('.skeleton')]
    print(f"æ€»æ–‡ä»¶æ•°: {len(skeleton_files)}")
    
    # è§£ææ‰€æœ‰æ–‡ä»¶ä¿¡æ¯
    file_info = []
    for file in skeleton_files:
        try:
            info = parse_filename(file)
            file_info.append(info)
        except:
            print(f"âš ï¸ æ— æ³•è§£ææ–‡ä»¶å: {file}")
            continue
    
    # ç»Ÿè®¡ä¿¡æ¯
    actions = [info['action'] for info in file_info]
    action_counts = Counter(actions)
    
    # æŒ‰ç±»åˆ«åˆ†ç±»
    daily_files = [info for info in file_info if info['action'] in SINGLE_DAILY_ACTIONS]
    rehab_files = [info for info in file_info if info['action'] in REHABILITATION_ACTIONS]
    interaction_files = [info for info in file_info if info['action'] in INTERACTION_ACTIONS]
    dvae_files = [info for info in file_info if info['action'] in DVAE_ACTIONS]
    
    print(f"\nğŸ“Š åŠ¨ä½œç±»åˆ«ç»Ÿè®¡:")
    print(f"å•äººæ—¥å¸¸åŠ¨ä½œ: {len(daily_files)} ä¸ªæ–‡ä»¶")
    print(f"åº·å¤åŠ¨ä½œ: {len(rehab_files)} ä¸ªæ–‡ä»¶")
    print(f"åŒäººäº’åŠ¨åŠ¨ä½œ: {len(interaction_files)} ä¸ªæ–‡ä»¶")
    print(f"DVAEè®­ç»ƒæ•°æ®: {len(dvae_files)} ä¸ªæ–‡ä»¶")
    
    print(f"\nğŸ¯ DVAEè®­ç»ƒåŠ¨ä½œåˆ†å¸ƒ:")
    dvae_actions = [info['action'] for info in dvae_files]
    dvae_action_counts = Counter(dvae_actions)
    
    for action_id in sorted(dvae_action_counts.keys()):
        count = dvae_action_counts[action_id]
        category = "åº·å¤" if action_id in REHABILITATION_ACTIONS else "æ—¥å¸¸"
        print(f"A{action_id:03d}: {count} ä¸ªæ ·æœ¬ ({category})")
    
    return {
        'total_files': len(skeleton_files),
        'valid_files': len(file_info),
        'daily_files': daily_files,
        'rehab_files': rehab_files,
        'interaction_files': interaction_files,
        'dvae_files': dvae_files,
        'action_counts': action_counts,
        'dvae_action_counts': dvae_action_counts
    }


def create_data_splits(dvae_files, train_ratio=0.7, val_ratio=0.15, test_ratio=0.15):
    """åˆ›å»ºè®­ç»ƒ/éªŒè¯/æµ‹è¯•æ•°æ®åˆ’åˆ†"""
    print(f"\nğŸ“‚ åˆ›å»ºæ•°æ®åˆ’åˆ† (è®­ç»ƒ:{train_ratio}, éªŒè¯:{val_ratio}, æµ‹è¯•:{test_ratio})")
    
    # æŒ‰åŠ¨ä½œç±»åˆ«åˆ†ç»„
    action_groups = defaultdict(list)
    for file_info in dvae_files:
        action_groups[file_info['action']].append(file_info)
    
    train_files = []
    val_files = []
    test_files = []
    
    # å¯¹æ¯ä¸ªåŠ¨ä½œç±»åˆ«è¿›è¡Œåˆ’åˆ†
    for action_id, files in action_groups.items():
        random.shuffle(files)
        
        n_total = len(files)
        n_train = int(n_total * train_ratio)
        n_val = int(n_total * val_ratio)
        n_test = n_total - n_train - n_val
        
        train_files.extend(files[:n_train])
        val_files.extend(files[n_train:n_train+n_val])
        test_files.extend(files[n_train+n_val:])
        
        print(f"A{action_id:03d}: {n_total} -> è®­ç»ƒ:{n_train}, éªŒè¯:{n_val}, æµ‹è¯•:{n_test}")
    
    print(f"\nâœ… æ•°æ®åˆ’åˆ†å®Œæˆ:")
    print(f"è®­ç»ƒé›†: {len(train_files)} ä¸ªæ–‡ä»¶")
    print(f"éªŒè¯é›†: {len(val_files)} ä¸ªæ–‡ä»¶") 
    print(f"æµ‹è¯•é›†: {len(test_files)} ä¸ªæ–‡ä»¶")
    
    return train_files, val_files, test_files


def save_data_splits(train_files, val_files, test_files, output_dir):
    """ä¿å­˜æ•°æ®åˆ’åˆ†ä¿¡æ¯"""
    os.makedirs(output_dir, exist_ok=True)
    
    # ä¿å­˜æ–‡ä»¶åˆ—è¡¨
    splits = {
        'train': [f['filename'] for f in train_files],
        'val': [f['filename'] for f in val_files],
        'test': [f['filename'] for f in test_files]
    }
    
    for split_name, filenames in splits.items():
        output_file = os.path.join(output_dir, f'{split_name}_files.txt')
        with open(output_file, 'w') as f:
            for filename in sorted(filenames):
                f.write(filename + '\n')
        print(f"ğŸ’¾ ä¿å­˜ {split_name} æ–‡ä»¶åˆ—è¡¨: {output_file}")
    
    # ä¿å­˜ç»Ÿè®¡ä¿¡æ¯
    stats = {
        'total_files': len(train_files) + len(val_files) + len(test_files),
        'train_files': len(train_files),
        'val_files': len(val_files),
        'test_files': len(test_files),
        'action_categories': {
            'single_daily': SINGLE_DAILY_ACTIONS,
            'rehabilitation': REHABILITATION_ACTIONS,
            'interaction': INTERACTION_ACTIONS,
            'dvae_actions': DVAE_ACTIONS
        }
    }
    
    stats_file = os.path.join(output_dir, 'dataset_stats.yaml')
    with open(stats_file, 'w') as f:
        yaml.dump(stats, f, default_flow_style=False)
    print(f"ğŸ“Š ä¿å­˜ç»Ÿè®¡ä¿¡æ¯: {stats_file}")


def create_dataset_config(output_dir):
    """åˆ›å»ºæ•°æ®é›†é…ç½®æ–‡ä»¶"""
    config = {
        'dataset_name': 'NTU_RGB+D',
        'description': 'NTU RGB+D éª¨æ¶æ•°æ®é›†ï¼Œç”¨äºDVAEè®­ç»ƒæ„å»ºäººä½“éª¨æ¶ç‚¹äº‘ç æœ¬',
        'data_path': './data/NTU-RGB+D',
        'splits': {
            'train': 'train_files.txt',
            'val': 'val_files.txt', 
            'test': 'test_files.txt'
        },
        'action_filter': 'dvae',
        'num_joints': 25,
        'num_points': 1024,
        'action_categories': {
            'single_daily': {
                'actions': SINGLE_DAILY_ACTIONS,
                'description': 'å•äººæ—¥å¸¸åŠ¨ä½œ',
                'count': len(SINGLE_DAILY_ACTIONS)
            },
            'rehabilitation': {
                'actions': REHABILITATION_ACTIONS,
                'description': 'åº·å¤åŠ¨ä½œ',
                'count': len(REHABILITATION_ACTIONS)
            },
            'interaction': {
                'actions': INTERACTION_ACTIONS,
                'description': 'åŒäººäº’åŠ¨åŠ¨ä½œï¼ˆä¸ç”¨äºDVAEè®­ç»ƒï¼‰',
                'count': len(INTERACTION_ACTIONS)
            }
        },
        'dvae_training': {
            'use_actions': DVAE_ACTIONS,
            'total_actions': len(DVAE_ACTIONS),
            'description': 'ç”¨äºDVAEè®­ç»ƒçš„åŠ¨ä½œç±»åˆ«ï¼ˆå•äººæ—¥å¸¸ + åº·å¤ï¼‰'
        }
    }
    
    config_file = os.path.join(output_dir, 'ntu_dataset_config.yaml')
    with open(config_file, 'w', encoding='utf-8') as f:
        yaml.dump(config, f, default_flow_style=False, allow_unicode=True)
    print(f"âš™ï¸ ä¿å­˜æ•°æ®é›†é…ç½®: {config_file}")


def main():
    parser = argparse.ArgumentParser(description='NTU RGB+D æ•°æ®é¢„å¤„ç†')
    parser.add_argument('--data_path', type=str, default='./data/NTU-RGB+D',
                       help='NTU RGB+D æ•°æ®è·¯å¾„')
    parser.add_argument('--output_dir', type=str, default='./data/NTU-RGB+D/splits',
                       help='è¾“å‡ºç›®å½•')
    parser.add_argument('--train_ratio', type=float, default=0.7,
                       help='è®­ç»ƒé›†æ¯”ä¾‹')
    parser.add_argument('--val_ratio', type=float, default=0.15,
                       help='éªŒè¯é›†æ¯”ä¾‹')
    parser.add_argument('--test_ratio', type=float, default=0.15,
                       help='æµ‹è¯•é›†æ¯”ä¾‹')
    parser.add_argument('--seed', type=int, default=42,
                       help='éšæœºç§å­')
    
    args = parser.parse_args()
    
    # è®¾ç½®éšæœºç§å­
    random.seed(args.seed)
    np.random.seed(args.seed)
    
    print("ğŸ¯ NTU RGB+D æ•°æ®é¢„å¤„ç†")
    print("=" * 50)
    print(f"æ•°æ®è·¯å¾„: {args.data_path}")
    print(f"è¾“å‡ºç›®å½•: {args.output_dir}")
    print(f"æ•°æ®åˆ’åˆ†: è®­ç»ƒ{args.train_ratio}, éªŒè¯{args.val_ratio}, æµ‹è¯•{args.test_ratio}")
    
    # æ£€æŸ¥æ•°æ®è·¯å¾„
    if not os.path.exists(args.data_path):
        print(f"âŒ æ•°æ®è·¯å¾„ä¸å­˜åœ¨: {args.data_path}")
        return
    
    # åˆ†ææ•°æ®é›†
    stats = analyze_dataset(args.data_path)
    
    # åˆ›å»ºæ•°æ®åˆ’åˆ†
    train_files, val_files, test_files = create_data_splits(
        stats['dvae_files'], 
        args.train_ratio, 
        args.val_ratio, 
        args.test_ratio
    )
    
    # ä¿å­˜æ•°æ®åˆ’åˆ†
    save_data_splits(train_files, val_files, test_files, args.output_dir)
    
    # åˆ›å»ºé…ç½®æ–‡ä»¶
    create_dataset_config(args.output_dir)
    
    print(f"\nğŸ‰ NTU RGB+D æ•°æ®é¢„å¤„ç†å®Œæˆï¼")
    print(f"ğŸ“ è¾“å‡ºç›®å½•: {args.output_dir}")
    print(f"ğŸ¯ DVAEè®­ç»ƒæ•°æ®: {len(stats['dvae_files'])} ä¸ªæ–‡ä»¶")
    print(f"ğŸ“Š åŠ¨ä½œç±»åˆ«: {len(DVAE_ACTIONS)} ä¸ªåŠ¨ä½œ")


if __name__ == '__main__':
    main()
