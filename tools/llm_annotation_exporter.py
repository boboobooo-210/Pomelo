#!/usr/bin/env python3
"""
LLM Token Annotation Exporter
================================
å°†æ ‡æ³¨å¥½çš„tokenè¯­ä¹‰å¯¼å‡ºä¸ºLLMå‹å¥½çš„æ ¼å¼,æ”¯æŒ:
1. é™æ€Tokenæè¿° (å•å¸§å§¿æ€è¯­ä¹‰)
2. Tokenåºåˆ— â†’ åŠ¨ä½œè¯­ä¹‰ (å¤šå¸§åŠ¨ä½œç†è§£)
3. å±‚çº§åŒ–æè¿° (éƒ¨ä½ + æ•´ä½“)

ä½œè€…: Skeleton Tokenizer Team
æ—¥æœŸ: 2025-11-07
"""

import json
import numpy as np
from pathlib import Path
from typing import Dict, List, Tuple
from collections import defaultdict


class LLMAnnotationExporter:
    """å°†ç æœ¬æ ‡æ³¨å¯¼å‡ºä¸ºLLMå¯ç†è§£çš„çŸ¥è¯†åº“"""
    
    def __init__(self, project_root: str = "/home/uo/myProject/CRSkeleton"):
        self.project_root = Path(project_root)
        self.token_analysis_dir = self.project_root / "token_analysis"
        self.annotation_path = self.token_analysis_dir / "codebook_annotations.json"
        
        # è¯­ä¹‰åˆ†ç»„å®šä¹‰
        self.semantic_groups = {
            'head_spine': [0, 1, 2, 3, 20],  # å¤´ã€é¢ˆæ¤ã€è„ŠæŸ±
            'left_arm': [4, 5, 6, 7, 21, 22],
            'right_arm': [8, 9, 10, 11, 23, 24],
            'left_leg': [12, 13, 14, 15],
            'right_leg': [16, 17, 18, 19]
        }
        
        # ä¸­æ–‡éƒ¨ä½åç§°æ˜ å°„
        self.part_names_zh = {
            'head_spine': 'å¤´éƒ¨èº¯å¹²',
            'left_arm': 'å·¦è‡‚',
            'right_arm': 'å³è‡‚',
            'left_leg': 'å·¦è…¿',
            'right_leg': 'å³è…¿'
        }
        
        self.annotations = {}
        self.load_annotations()
    
    def load_annotations(self):
        """åŠ è½½ç°æœ‰æ ‡æ³¨"""
        if not self.annotation_path.exists():
            raise FileNotFoundError(f"æ ‡æ³¨æ–‡ä»¶ä¸å­˜åœ¨: {self.annotation_path}")
        
        with open(self.annotation_path, 'r', encoding='utf-8') as f:
            data = json.load(f)
        
        self.annotations = data.get('codebook_annotation', {})
        self.metadata = data.get('metadata', {})
        
        print(f"âœ… å·²åŠ è½½æ ‡æ³¨:")
        print(f"   - æ€»Tokenæ•°: {self.metadata.get('total_unique_tokens', 0)}")
        print(f"   - å·²æ ‡æ³¨: {self.metadata.get('annotated_tokens', 0)}")
        print(f"   - æœ€åæ›´æ–°: {self.metadata.get('last_updated', 'Unknown')}")
    
    def export_llm_knowledge_base(self, output_path: str = None) -> Dict:
        """
        å¯¼å‡ºLLMçŸ¥è¯†åº“ (é™æ€Tokenè¯­ä¹‰)
        
        æ ¼å¼:
        {
            "token_semantics": {
                "35": {
                    "body_part": "å¤´éƒ¨èº¯å¹²",
                    "description": "å·¦å€¾æ–œ",
                    "token_id": 35,
                    "group": "head_spine",
                    "joints_involved": [0, 1, 2, 3, 20]
                },
                ...
            },
            "body_part_vocabulary": {
                "å¤´éƒ¨èº¯å¹²": ["å·¦å€¾æ–œ", "å³å€¾æ–œ", "å‰å€¾", ...],
                ...
            },
            "metadata": {...}
        }
        """
        if output_path is None:
            output_path = self.token_analysis_dir / "llm_token_knowledge_base.json"
        
        knowledge_base = {
            "token_semantics": {},
            "body_part_vocabulary": defaultdict(set),
            "metadata": {
                "source": "MARS Dataset Token Annotations",
                "total_tokens": self.metadata.get('total_unique_tokens', 0),
                "annotation_date": self.metadata.get('last_updated', ''),
                "format_version": "1.0",
                "description": "éª¨æ¶å§¿æ€ç æœ¬çš„è¯­ä¹‰çŸ¥è¯†åº“,ç”¨äºLLMç†è§£éª¨æ¶token"
            }
        }
        
        # å¤„ç†æ¯ä¸ªéƒ¨ä½çš„token
        for part_name, token_dict in self.annotations.items():
            part_name_zh = self.part_names_zh.get(part_name, part_name)
            joints = self.semantic_groups.get(part_name, [])
            
            for token_id, description in token_dict.items():
                token_id_int = int(token_id)
                
                # æ„å»ºtokenè¯­ä¹‰æ¡ç›®
                knowledge_base["token_semantics"][token_id] = {
                    "token_id": token_id_int,
                    "body_part": part_name_zh,
                    "body_part_en": part_name,
                    "description": description,
                    "joints_involved": joints,
                    "example_usage": f"å½“{part_name_zh}å¤„äº'{description}'çŠ¶æ€æ—¶,ä½¿ç”¨token_{token_id}"
                }
                
                # æ„å»ºéƒ¨ä½è¯æ±‡è¡¨
                knowledge_base["body_part_vocabulary"][part_name_zh].add(description)
        
        # è½¬æ¢setä¸ºlist (JSONåºåˆ—åŒ–éœ€è¦)
        knowledge_base["body_part_vocabulary"] = {
            k: sorted(list(v)) for k, v in knowledge_base["body_part_vocabulary"].items()
        }
        
        # ä¿å­˜
        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(knowledge_base, f, ensure_ascii=False, indent=2)
        
        print(f"\nâœ… LLMçŸ¥è¯†åº“å·²å¯¼å‡º: {output_path}")
        print(f"   - Tokenæ¡ç›®æ•°: {len(knowledge_base['token_semantics'])}")
        print(f"   - éƒ¨ä½è¯æ±‡ç±»åˆ«: {len(knowledge_base['body_part_vocabulary'])}")
        
        return knowledge_base
    
    def export_prompt_templates(self, output_path: str = None) -> Dict:
        """
        å¯¼å‡ºLLM Promptæ¨¡æ¿
        
        åŒ…å«:
        1. Token â†’ æ–‡æœ¬æè¿° (skeleton tokenè§£ç ä¸ºè‡ªç„¶è¯­è¨€)
        2. æ–‡æœ¬æè¿° â†’ Token (è‡ªç„¶è¯­è¨€ç¼–ç ä¸ºskeleton token)
        3. åºåˆ—ç†è§£ (å¤šå¸§tokenåºåˆ—ç†è§£åŠ¨ä½œ)
        """
        if output_path is None:
            output_path = self.token_analysis_dir / "llm_prompt_templates.json"
        
        # æ„å»ºToken ID â†’ æè¿°çš„å¿«é€ŸæŸ¥è¯¢è¡¨
        token_to_desc = {}
        for part_name, token_dict in self.annotations.items():
            part_zh = self.part_names_zh.get(part_name, part_name)
            for token_id, desc in token_dict.items():
                token_to_desc[int(token_id)] = {
                    "part": part_zh,
                    "desc": desc
                }
        
        templates = {
            "task_1_decode_single_frame": {
                "name": "å•å¸§å§¿æ€è§£ç  (Token â†’ æ–‡æœ¬)",
                "description": "å°†ä¸€å¸§éª¨æ¶çš„5ä¸ªéƒ¨ä½tokenè½¬æ¢ä¸ºè‡ªç„¶è¯­è¨€æè¿°",
                "system_prompt": """ä½ æ˜¯ä¸€ä¸ªéª¨æ¶å§¿æ€ç†è§£ä¸“å®¶ã€‚ç”¨æˆ·ä¼šç»™ä½ ä¸€ç»„token ID,ä»£è¡¨äººä½“éª¨æ¶çš„5ä¸ªéƒ¨ä½çŠ¶æ€ã€‚
ä½ éœ€è¦å°†è¿™äº›token IDè½¬æ¢ä¸ºè‡ªç„¶è¯­è¨€æè¿°ã€‚

å¯ç”¨çš„Tokenè¯­ä¹‰çŸ¥è¯†åº“:
{token_knowledge_base}

æ³¨æ„:
- æ¯ä¸ªtokenä»£è¡¨ä¸€ä¸ªèº«ä½“éƒ¨ä½çš„å§¿æ€çŠ¶æ€
- æè¿°è¦ç®€æ´å‡†ç¡®,é¿å…å†—ä½™
- æŒ‰ç…§"å¤´éƒ¨èº¯å¹²-å·¦è‡‚-å³è‡‚-å·¦è…¿-å³è…¿"é¡ºåºç»„ç»‡æè¿°
""",
                "user_prompt_template": """è¯·æè¿°ä»¥ä¸‹éª¨æ¶å§¿æ€:

Tokenåºåˆ—: [{head_spine}, {left_arm}, {right_arm}, {left_leg}, {right_leg}]

è¦æ±‚:
1. å…ˆåˆ†åˆ«æè¿°5ä¸ªéƒ¨ä½çš„å§¿æ€
2. å†æ€»ç»“æ•´ä½“å§¿æ€/åŠ¨ä½œ
3. è¾“å‡ºæ ¼å¼:
   éƒ¨ä½æè¿°:
   - å¤´éƒ¨èº¯å¹²: xxx
   - å·¦è‡‚: xxx
   - å³è‡‚: xxx
   - å·¦è…¿: xxx
   - å³è…¿: xxx
   
   æ•´ä½“å§¿æ€: xxx
""",
                "example_input": {
                    "head_spine": 117,
                    "left_arm": 178,
                    "right_arm": 375,
                    "left_leg": 489,
                    "right_leg": 608
                },
                "example_output": """éƒ¨ä½æè¿°:
- å¤´éƒ¨èº¯å¹²: æ­£å¸¸å§¿æ€
- å·¦è‡‚: è‡ªç„¶å‚è½
- å³è‡‚: è‡ªç„¶å‚è½
- å·¦è…¿: ç«™ç«‹(ç›´ç«‹)
- å³è…¿: ç«™ç«‹

æ•´ä½“å§¿æ€: æ ‡å‡†ç«™ç«‹å§¿åŠ¿,èº«ä½“ä¿æŒç›´ç«‹,åŒè‡‚è‡ªç„¶å‚äºèº«ä½“ä¸¤ä¾§,åŒè…¿å¹¶æ‹¢æ”¯æ’‘èº«ä½“ã€‚"""
            },
            
            "task_2_encode_description": {
                "name": "æ–‡æœ¬æè¿°ç¼–ç  (æ–‡æœ¬ â†’ Token)",
                "description": "å°†è‡ªç„¶è¯­è¨€å§¿æ€æè¿°è½¬æ¢ä¸ºå¯¹åº”çš„token ID",
                "system_prompt": """ä½ æ˜¯ä¸€ä¸ªéª¨æ¶å§¿æ€ç¼–ç ä¸“å®¶ã€‚ç”¨æˆ·ä¼šç»™ä½ ä¸€æ®µè‡ªç„¶è¯­è¨€æè¿°,ä½ éœ€è¦é€‰æ‹©æœ€åŒ¹é…çš„token IDã€‚

å¯ç”¨çš„Tokenè¯æ±‡è¡¨:
{body_part_vocabulary}

Tokenè¯­ä¹‰çŸ¥è¯†åº“:
{token_knowledge_base}

æ³¨æ„:
- é€‰æ‹©æœ€æ¥è¿‘æè¿°çš„token
- å¦‚æœæ²¡æœ‰å®Œå…¨åŒ¹é…,é€‰æ‹©è¯­ä¹‰æœ€ç›¸è¿‘çš„token
- å¿…é¡»è¿”å›5ä¸ªéƒ¨ä½çš„token ID
""",
                "user_prompt_template": """è¯·å°†ä»¥ä¸‹å§¿æ€æè¿°ç¼–ç ä¸ºtokenåºåˆ—:

æè¿°: {action_description}

è¦æ±‚:
1. åˆ†ææè¿°ä¸­å„ä¸ªèº«ä½“éƒ¨ä½çš„çŠ¶æ€
2. ä»çŸ¥è¯†åº“ä¸­é€‰æ‹©æœ€åŒ¹é…çš„token
3. è¾“å‡ºæ ¼å¼:
   {{
     "head_spine": <token_id>,
     "left_arm": <token_id>,
     "right_arm": <token_id>,
     "left_leg": <token_id>,
     "right_leg": <token_id>,
     "confidence": <0-1ä¹‹é—´çš„ç½®ä¿¡åº¦>,
     "reasoning": "é€‰æ‹©ç†ç”±"
   }}
""",
                "example_input": "ä¸€ä¸ªäººç«™ç«‹,å·¦æ‰‹å‘ä¾§é¢æŠ¬èµ·,å³æ‰‹è‡ªç„¶ä¸‹å‚,èº«ä½“å¾®å¾®å·¦å€¾",
                "example_output": {
                    "head_spine": 105,  # å·¦å€¾æ–œ
                    "left_arm": 159,    # å·¦ä¾§æŠ¬èµ·
                    "right_arm": 375,   # è‡ªç„¶å‚è½
                    "left_leg": 489,    # ç«™ç«‹(ç›´ç«‹)
                    "right_leg": 608,   # ç«™ç«‹
                    "confidence": 0.85,
                    "reasoning": "æè¿°æ˜ç¡®æåˆ°å·¦å€¾ã€å·¦æ‰‹ä¾§æŠ¬ã€å³æ‰‹ä¸‹å‚å’Œç«™ç«‹,ä¸å¯¹åº”tokenè¯­ä¹‰é«˜åº¦åŒ¹é…"
                }
            },
            
            "task_3_sequence_understanding": {
                "name": "åºåˆ—åŠ¨ä½œç†è§£ (å¤šå¸§Token â†’ åŠ¨ä½œè¯­ä¹‰)",
                "description": "åˆ†æè¿ç»­å¤šå¸§çš„tokenåºåˆ—,ç†è§£æ•´ä½“åŠ¨ä½œæ„å›¾",
                "system_prompt": """ä½ æ˜¯ä¸€ä¸ªåŠ¨ä½œåºåˆ—åˆ†æä¸“å®¶ã€‚ç”¨æˆ·ä¼šç»™ä½ è¿ç»­å¤šå¸§çš„éª¨æ¶tokenåºåˆ—,ä½ éœ€è¦:
1. ç†è§£æ¯ä¸€å¸§çš„å§¿æ€
2. åˆ†æå¸§é—´çš„å˜åŒ–è¶‹åŠ¿
3. æ¨æ–­æ•´ä½“åŠ¨ä½œçš„è¯­ä¹‰(å¦‚"æŒ¥æ‰‹"ã€"ä¸‹è¹²"ã€"è¡Œèµ°"ç­‰)

å¯ç”¨çš„TokençŸ¥è¯†åº“:
{token_knowledge_base}

åˆ†æç»´åº¦:
- æ—¶åºå˜åŒ–: å“ªäº›éƒ¨ä½åœ¨åŠ¨,å¦‚ä½•åŠ¨
- ååŒæ¨¡å¼: å¤šä¸ªéƒ¨ä½å¦‚ä½•é…åˆ
- åŠ¨ä½œå‘¨æœŸ: æ˜¯å¦æœ‰é‡å¤/å‘¨æœŸæ€§
- æ„å›¾æ¨æ–­: è¿™ä¸ªåŠ¨ä½œå¯èƒ½åœ¨åšä»€ä¹ˆ
""",
                "user_prompt_template": """è¯·åˆ†æä»¥ä¸‹éª¨æ¶åŠ¨ä½œåºåˆ—:

å¸§æ•°: {num_frames}
Tokenåºåˆ—:
{token_sequence}

è¦æ±‚:
1. é€å¸§æè¿°å§¿æ€å˜åŒ–
2. è¯†åˆ«å…³é”®åŠ¨ä½œé˜¶æ®µ
3. æ¨æ–­æ•´ä½“åŠ¨ä½œç±»å‹
4. è¾“å‡ºæ ¼å¼:
   {{
     "frame_analysis": [
       {{"frame": 0, "pose": "...", "key_changes": "..."}},
       ...
     ],
     "action_phases": ["å‡†å¤‡é˜¶æ®µ", "æ‰§è¡Œé˜¶æ®µ", "æ¢å¤é˜¶æ®µ"],
     "overall_action": "åŠ¨ä½œåç§°",
     "confidence": 0.x,
     "reasoning": "åˆ¤æ–­ä¾æ®"
   }}
""",
                "example_input": {
                    "num_frames": 5,
                    "token_sequence": [
                        {"frame": 0, "tokens": [117, 178, 375, 489, 608]},  # ç«™ç«‹
                        {"frame": 1, "tokens": [117, 159, 375, 489, 608]},  # å·¦æ‰‹å¼€å§‹æŠ¬èµ·
                        {"frame": 2, "tokens": [117, 218, 375, 489, 608]},  # å·¦æ‰‹å®Œå…¨ä¾§æŠ¬
                        {"frame": 3, "tokens": [117, 159, 375, 489, 608]},  # å·¦æ‰‹å¼€å§‹æ”¾ä¸‹
                        {"frame": 4, "tokens": [117, 178, 375, 489, 608]}   # æ¢å¤ç«™ç«‹
                    ]
                },
                "example_output": {
                    "frame_analysis": [
                        {"frame": 0, "pose": "æ ‡å‡†ç«™ç«‹", "key_changes": "åˆå§‹å§¿æ€"},
                        {"frame": 1, "pose": "å·¦æ‰‹å¼€å§‹æŠ¬èµ·", "key_changes": "å·¦è‡‚ä»è‡ªç„¶å‚è½(178)å˜ä¸ºä¾§æŠ¬(159)"},
                        {"frame": 2, "pose": "å·¦æ‰‹å®Œå…¨ä¾§æŠ¬", "key_changes": "å·¦è‡‚è¾¾åˆ°æœ€é«˜ç‚¹(218)"},
                        {"frame": 3, "pose": "å·¦æ‰‹å¼€å§‹ä¸‹é™", "key_changes": "å·¦è‡‚ä»æœ€é«˜ç‚¹å›è½(159)"},
                        {"frame": 4, "pose": "æ¢å¤ç«™ç«‹", "key_changes": "å·¦è‡‚å›åˆ°è‡ªç„¶å‚è½(178)"}
                    ],
                    "action_phases": ["å‡†å¤‡(F0)", "æŠ¬æ‰‹(F1-F2)", "æ”¾ä¸‹(F3-F4)"],
                    "overall_action": "å·¦æ‰‹æŒ¥æ‰‹/æ‹›æ‰‹åŠ¨ä½œ",
                    "confidence": 0.92,
                    "reasoning": "å·¦è‡‚å®Œæˆäº†ä¸€ä¸ªå®Œæ•´çš„ä¸ŠæŠ¬-ä¸‹é™å‘¨æœŸ,å…¶ä»–éƒ¨ä½ä¿æŒé™æ­¢,ç¬¦åˆå…¸å‹çš„æŒ¥æ‰‹åŠ¨ä½œæ¨¡å¼"
                }
            }
        }
        
        # ä¿å­˜
        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(templates, f, ensure_ascii=False, indent=2)
        
        print(f"\nâœ… LLM Promptæ¨¡æ¿å·²å¯¼å‡º: {output_path}")
        print(f"   - ä»»åŠ¡æ¨¡æ¿æ•°: {len(templates)}")
        
        return templates
    
    def generate_sample_sequences_from_dataset(self, 
                                              reconstructed_dir: str = None,
                                              num_samples: int = 10,
                                              min_frames: int = 5,
                                              max_frames: int = 10,
                                              output_path: str = None) -> List[Dict]:
        """
        ä»MARSé‡æ„æ•°æ®ä¸­æå–çœŸå®çš„tokenåºåˆ—æ ·æœ¬
        
        ç”¨äº:
        1. ç”ŸæˆçœŸå®çš„å¤šå¸§tokenåºåˆ—
        2. ä¸ºLLMæä¾›å®é™…çš„åŠ¨ä½œæ¡ˆä¾‹
        3. æ”¯æŒåç»­çš„GIFå¯è§†åŒ–æ ‡æ³¨
        
        å‚æ•°:
            reconstructed_dir: é‡æ„æ•°æ®ç›®å½•
            num_samples: æå–æ ·æœ¬æ•°é‡
            min_frames/max_frames: åºåˆ—å¸§æ•°èŒƒå›´
            output_path: è¾“å‡ºè·¯å¾„
        
        è¿”å›:
            æ ·æœ¬åˆ—è¡¨,æ¯ä¸ªæ ·æœ¬åŒ…å«tokenåºåˆ—å’Œå…ƒä¿¡æ¯
        """
        if reconstructed_dir is None:
            reconstructed_dir = self.project_root / "data" / "MARS" / "reconstructed"
        else:
            reconstructed_dir = Path(reconstructed_dir)
        
        if output_path is None:
            output_path = self.token_analysis_dir / "sample_token_sequences.json"
        
        if not reconstructed_dir.exists():
            print(f"âŒ é‡æ„æ•°æ®ç›®å½•ä¸å­˜åœ¨: {reconstructed_dir}")
            print("   æç¤º: éœ€è¦å…ˆè¿è¡Œ skeleton_extraction_reconstruction_saver.py")
            return []
        
        # è·å–æ‰€æœ‰.npzæ–‡ä»¶
        npz_files = sorted(list(reconstructed_dir.glob("*.npz")))
        if len(npz_files) == 0:
            print(f"âŒ æœªæ‰¾åˆ°é‡æ„æ•°æ®æ–‡ä»¶ (.npz)")
            return []
        
        print(f"ğŸ“‚ æ‰¾åˆ° {len(npz_files)} ä¸ªé‡æ„æ•°æ®æ–‡ä»¶")
        
        # éšæœºé‡‡æ ·æ–‡ä»¶
        sample_indices = np.random.choice(len(npz_files), 
                                         min(num_samples, len(npz_files)), 
                                         replace=False)
        
        samples = []
        for idx in sample_indices:
            npz_file = npz_files[idx]
            try:
                data = np.load(npz_file)
                tokens = data['tokens']  # Shape: (T, 5) - Tå¸§,5ä¸ªéƒ¨ä½
                
                # éšæœºé€‰æ‹©è¿ç»­å¸§ç‰‡æ®µ
                total_frames = tokens.shape[0]
                seq_len = np.random.randint(min_frames, min(max_frames, total_frames) + 1)
                start_frame = np.random.randint(0, max(1, total_frames - seq_len + 1))
                token_seq = tokens[start_frame:start_frame + seq_len]
                
                # æ„å»ºæ ·æœ¬
                sample = {
                    "sample_id": npz_file.stem,
                    "source_file": str(npz_file.name),
                    "frame_range": [start_frame, start_frame + seq_len - 1],
                    "num_frames": seq_len,
                    "token_sequence": [
                        {
                            "frame": i,
                            "head_spine": int(token_seq[i][0]),
                            "left_arm": int(token_seq[i][1]),
                            "right_arm": int(token_seq[i][2]),
                            "left_leg": int(token_seq[i][3]),
                            "right_leg": int(token_seq[i][4])
                        }
                        for i in range(seq_len)
                    ],
                    # é¢„ç•™å­—æ®µç”¨äºåç»­äººå·¥æ ‡æ³¨
                    "action_annotation": {
                        "overall_action": "[å¾…æ ‡æ³¨]",
                        "body_part_actions": {
                            "head_spine": "[å¾…æ ‡æ³¨]",
                            "left_arm": "[å¾…æ ‡æ³¨]",
                            "right_arm": "[å¾…æ ‡æ³¨]",
                            "left_leg": "[å¾…æ ‡æ³¨]",
                            "right_leg": "[å¾…æ ‡æ³¨]"
                        },
                        "action_phases": [],
                        "notes": ""
                    }
                }
                
                samples.append(sample)
                
            except Exception as e:
                print(f"âš ï¸  å¤„ç†æ–‡ä»¶å¤±è´¥ {npz_file.name}: {e}")
                continue
        
        # ä¿å­˜
        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump({
                "samples": samples,
                "metadata": {
                    "total_samples": len(samples),
                    "source_dataset": "MARS",
                    "frame_range": [min_frames, max_frames],
                    "generation_date": self.metadata.get('last_updated', ''),
                    "annotation_status": "å¾…æ ‡æ³¨"
                }
            }, f, ensure_ascii=False, indent=2)
        
        print(f"\nâœ… æ ·æœ¬åºåˆ—å·²å¯¼å‡º: {output_path}")
        print(f"   - æ ·æœ¬æ•°: {len(samples)}")
        print(f"   - å¹³å‡å¸§æ•°: {np.mean([s['num_frames'] for s in samples]):.1f}")
        
        return samples


def main():
    """ä¸»å‡½æ•°: æ¼”ç¤ºå¦‚ä½•ä½¿ç”¨LLMå¯¹æ¥å·¥å…·"""
    
    print("="*60)
    print("LLM Token Annotation Exporter")
    print("="*60)
    
    # åˆå§‹åŒ–å¯¼å‡ºå™¨
    exporter = LLMAnnotationExporter()
    
    # 1. å¯¼å‡ºé™æ€TokençŸ¥è¯†åº“
    print("\n" + "="*60)
    print("æ­¥éª¤1: å¯¼å‡ºTokenè¯­ä¹‰çŸ¥è¯†åº“")
    print("="*60)
    knowledge_base = exporter.export_llm_knowledge_base()
    
    # æ˜¾ç¤ºç»Ÿè®¡
    print("\nğŸ“Š çŸ¥è¯†åº“ç»Ÿè®¡:")
    for part, vocab in knowledge_base["body_part_vocabulary"].items():
        print(f"   - {part}: {len(vocab)} ç§å§¿æ€")
    
    # 2. å¯¼å‡ºPromptæ¨¡æ¿
    print("\n" + "="*60)
    print("æ­¥éª¤2: å¯¼å‡ºLLM Promptæ¨¡æ¿")
    print("="*60)
    templates = exporter.export_prompt_templates()
    
    print("\nğŸ“ å¯ç”¨ä»»åŠ¡æ¨¡æ¿:")
    for task_id, task_info in templates.items():
        print(f"   - {task_info['name']}")
        print(f"     {task_info['description']}")
    
    # 3. ç”Ÿæˆæ ·æœ¬åºåˆ—(å¦‚æœæœ‰é‡æ„æ•°æ®)
    print("\n" + "="*60)
    print("æ­¥éª¤3: æå–æ ·æœ¬Tokenåºåˆ— (å¯é€‰)")
    print("="*60)
    samples = exporter.generate_sample_sequences_from_dataset(num_samples=10)
    
    if len(samples) > 0:
        print(f"\nâœ… å·²æå– {len(samples)} ä¸ªæ ·æœ¬åºåˆ—")
        print("   è¿™äº›åºåˆ—å¯ç”¨äº:")
        print("   - ç”ŸæˆGIFåŠ¨ç”»è¿›è¡Œäººå·¥æ ‡æ³¨")
        print("   - è®­ç»ƒLLMç†è§£åŠ¨ä½œåºåˆ—")
        print("   - éªŒè¯Token â†’ æ–‡æœ¬è½¬æ¢è´¨é‡")
    
    # 4. ä½¿ç”¨å»ºè®®
    print("\n" + "="*60)
    print("ä¸‹ä¸€æ­¥æ“ä½œå»ºè®®")
    print("="*60)
    print("""
1. LLMé›†æˆæ–¹å¼:
   æ–¹å¼A - APIè°ƒç”¨ (æ¨è):
     - ä½¿ç”¨OpenAI API / Claude API / æ–‡å¿ƒä¸€è¨€ ç­‰
     - å°†çŸ¥è¯†åº“æ³¨å…¥System Prompt
     - å®æ—¶è°ƒç”¨LLMè¿›è¡ŒToken â†” æ–‡æœ¬è½¬æ¢
   
   æ–¹å¼B - Fine-tuning:
     - ä½¿ç”¨å¯¼å‡ºçš„çŸ¥è¯†åº“æ„å»ºè®­ç»ƒæ•°æ®
     - Fine-tuneå°å‹è¯­è¨€æ¨¡å‹ (å¦‚LLaMA 7B)
     - æœ¬åœ°éƒ¨ç½²æ¨ç†

2. GIFåŠ¨ç”»æ ‡æ³¨æµç¨‹:
   a) ä½¿ç”¨ sample_token_sequences.json ä¸­çš„åºåˆ—
   b) ä¸ºæ¯ä¸ªåºåˆ—ç”Ÿæˆéª¨æ¶åŠ¨ç”»GIF
   c) äººå·¥æ ‡æ³¨æ•´ä½“åŠ¨ä½œè¯­ä¹‰
   d) å›å¡«åˆ° action_annotation å­—æ®µ
   e) æ„å»º Tokenåºåˆ— â†’ åŠ¨ä½œè¯­ä¹‰ çš„è®­ç»ƒæ•°æ®

3. æµ‹è¯•LLMç†è§£èƒ½åŠ›:
   - ä½¿ç”¨ llm_prompt_templates.json ä¸­çš„example
   - æµ‹è¯•LLMæ˜¯å¦èƒ½æ­£ç¡®è§£ç token
   - æµ‹è¯•LLMæ˜¯å¦èƒ½æ­£ç¡®ç¼–ç æè¿°
   - æµ‹è¯•LLMæ˜¯å¦èƒ½ç†è§£åŠ¨ä½œåºåˆ—

4. åç»­æ”¹è¿›æ–¹å‘:
   - å¢åŠ æ›´å¤šæ ·åŒ–çš„åŠ¨ä½œåºåˆ—æ ·æœ¬
   - æ ‡æ³¨åŠ¨ä½œçš„æ—¶é—´ç²’åº¦ (å¿«/æ…¢/æ€¥ä¿ƒç­‰)
   - æ ‡æ³¨åŠ¨ä½œçš„æƒ…æ„Ÿ/æ„å›¾ (æ„¤æ€’æŒ¥æ‰‹/å‹å¥½æŒ¥æ‰‹ç­‰)
   - å»ºç«‹åŠ¨ä½œè¯­ä¹‰çš„å±‚çº§åˆ†ç±»ä½“ç³»
""")
    
    print("\n" + "="*60)
    print("âœ… å¯¼å‡ºå®Œæˆ!")
    print("="*60)
    print(f"\nç”Ÿæˆçš„æ–‡ä»¶:")
    print(f"1. {exporter.token_analysis_dir}/llm_token_knowledge_base.json")
    print(f"2. {exporter.token_analysis_dir}/llm_prompt_templates.json")
    if len(samples) > 0:
        print(f"3. {exporter.token_analysis_dir}/sample_token_sequences.json")
    print(f"\nå¯ä»¥ç›´æ¥ä½¿ç”¨è¿™äº›æ–‡ä»¶ä¸LLMè¿›è¡Œäº¤äº’!")


if __name__ == "__main__":
    main()
