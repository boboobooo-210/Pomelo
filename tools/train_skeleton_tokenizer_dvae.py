#!/usr/bin/env python3
"""
DVAEé£æ ¼SkeletonTokenizerè®­ç»ƒè„šæœ¬
ä½¿ç”¨æ”¹è¿›çš„åˆ†ç»„ç­–ç•¥å’ŒæŸå¤±å‡½æ•°ï¼Œ50è½®è®­ç»ƒ
"""

import os
import sys
import argparse
import datetime
import torch
import torch.distributed as dist
from pathlib import Path

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from tools.runner import run_net
from utils import misc, dist_utils
import time
import json
from tensorboardX import SummaryWriter


def get_args():
    parser = argparse.ArgumentParser()
    parser.add_argument('--config', type=str, 
                       default='experiments/skeleton_dvae_pretrain/NTU_models/ntu_skeleton_tokenizer_dvae_50epochs/config.yaml',
                       help='yaml config file')
    parser.add_argument('--launcher', choices=['none', 'pytorch'], default='none', help='job launcher')
    parser.add_argument('--local_rank', type=int, default=0)
    parser.add_argument('--num_workers', type=int, default=4)
    parser.add_argument('--seed', type=int, default=2021, help='random seed')
    parser.add_argument('--deterministic', action='store_true', help='whether to set deterministic options for CUDNN backend.')
    parser.add_argument('--sync_bn', action='store_true', default=False, help='whether to use sync bn')
    parser.add_argument('--fix_random_seed', action='store_true', default=False, help='')
    parser.add_argument('--exp_name', type=str, default='ntu_skeleton_tokenizer_dvae_50epochs', help='experiment name')
    parser.add_argument('--start_ckpts', type=str, default=None, help='reload used ckpt path')
    parser.add_argument('--ckpts', type=str, default=None, help='test used ckpt path')
    parser.add_argument('--val_freq', type=int, default=5, help='test freq')
    parser.add_argument('--resume', action='store_true', default=False, help='autoresume training (interrupted by accident)')
    parser.add_argument('--test', action='store_true', default=False, help='test mode for certain ckpt')
    parser.add_argument('--finetune_model', action='store_true', default=False, help='finetune modelnet with pretrained weight')
    parser.add_argument('--scratch_model', action='store_true', default=False, help='training modelnet from scratch')
    parser.add_argument('--mode', choices=['easy', 'median', 'hard'], default=None, help='difficulty mode for shapenet')
    parser.add_argument('--way', type=int, default=-1)
    parser.add_argument('--shot', type=int, default=-1)
    parser.add_argument('--fold', type=int, default=-1)
    parser.add_argument('--gpu', type=str, default='0', help='specify gpu device')

    args = parser.parse_args()
    return args


def setup_experiment_dir(args):
    """è®¾ç½®å®éªŒç›®å½•"""
    # åˆ›å»ºå®éªŒç›®å½•
    exp_dir = Path(f'experiments/skeleton_dvae_pretrain/NTU_models/{args.exp_name}')
    exp_dir.mkdir(parents=True, exist_ok=True)
    
    # åˆ›å»ºæ—¥å¿—ç›®å½•
    log_dir = exp_dir / 'logs'
    log_dir.mkdir(exist_ok=True)
    
    # åˆ›å»ºæ£€æŸ¥ç‚¹ç›®å½•
    ckpt_dir = exp_dir / 'checkpoints'
    ckpt_dir.mkdir(exist_ok=True)
    
    return exp_dir


def log_training_info(exp_dir):
    """è®°å½•è®­ç»ƒä¿¡æ¯"""
    info = {
        'experiment_name': 'DVAE Style SkeletonTokenizer Training',
        'start_time': datetime.datetime.now().isoformat(),
        'modifications': {
            'grouping_strategy': 'DVAE style FPS random sampling (8 groups)',
            'codebook_design': 'Unified codebook size (512 per group)',
            'loss_function': 'Improved with bone ratio and global shape consistency',
            'training_epochs': 50,
            'expected_improvements': [
                'Solve joint clustering problem',
                'Improve codebook utilization rate',
                'Maintain skeleton structure flexibility'
            ]
        },
        'technical_details': {
            'num_groups': 8,
            'codebook_size': 512,
            'points_per_group': 90,
            'total_points': 720,
            'loss_weights': {
                'reconstruction': 10.0,
                'structure': 1.0,
                'codebook': 0.25,
                'commitment': 0.25
            }
        }
    }
    
    with open(exp_dir / 'training_info.json', 'w') as f:
        json.dump(info, f, indent=2)
    
    print("ğŸš€ DVAEé£æ ¼SkeletonTokenizerè®­ç»ƒå¼€å§‹")
    print("=" * 60)
    print(f"ğŸ“ å®éªŒç›®å½•: {exp_dir}")
    print(f"ğŸ¯ è®­ç»ƒè½®æ•°: 50è½®")
    print(f"ğŸ”§ åˆ†ç»„ç­–ç•¥: 8ç»„DVAEé£æ ¼FPSé‡‡æ ·")
    print(f"ğŸ“š ç æœ¬è®¾è®¡: ç»Ÿä¸€512ç å­—/ç»„")
    print(f"ğŸ’¡ é¢„æœŸæ•ˆæœ: è§£å†³å›¢çŠ¶é›†ä¸­é—®é¢˜")
    print("=" * 60)


def main():
    # è§£æå‚æ•°
    args = get_args()
    
    # è®¾ç½®GPU
    os.environ["CUDA_VISIBLE_DEVICES"] = args.gpu
    
    # è®¾ç½®å®éªŒç›®å½•
    exp_dir = setup_experiment_dir(args)
    
    # è®°å½•è®­ç»ƒä¿¡æ¯
    log_training_info(exp_dir)
    
    # æ£€æŸ¥é…ç½®æ–‡ä»¶æ˜¯å¦å­˜åœ¨
    if not os.path.exists(args.config):
        print(f"âŒ é…ç½®æ–‡ä»¶ä¸å­˜åœ¨: {args.config}")
        return
    
    print(f"âœ… ä½¿ç”¨é…ç½®æ–‡ä»¶: {args.config}")
    
    # è®¾ç½®éšæœºç§å­
    if args.fix_random_seed:
        misc.set_random_seed(args.seed)
        if args.deterministic:
            misc.set_deterministic()
    
    # åˆå§‹åŒ–åˆ†å¸ƒå¼è®­ç»ƒ
    if args.launcher == 'none':
        dist_utils.setup_env(args.launcher, 0, 1, 1, args.local_rank)
    else:
        dist_utils.setup_env(args.launcher, args.local_rank, 1, 1, args.local_rank)
    
    # å¼€å§‹è®­ç»ƒ
    try:
        print(f"ğŸš€ å¼€å§‹DVAEé£æ ¼SkeletonTokenizerè®­ç»ƒ...")
        run_net(args)
        print(f"ğŸ‰ è®­ç»ƒå®Œæˆï¼")
        
        # è®°å½•å®Œæˆä¿¡æ¯
        completion_info = {
            'completion_time': datetime.datetime.now().isoformat(),
            'status': 'completed',
            'experiment_dir': str(exp_dir)
        }
        
        with open(exp_dir / 'completion_info.json', 'w') as f:
            json.dump(completion_info, f, indent=2)
            
    except Exception as e:
        print(f"âŒ è®­ç»ƒè¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
        
        # è®°å½•é”™è¯¯ä¿¡æ¯
        error_info = {
            'error_time': datetime.datetime.now().isoformat(),
            'status': 'failed',
            'error_message': str(e),
            'experiment_dir': str(exp_dir)
        }
        
        with open(exp_dir / 'error_info.json', 'w') as f:
            json.dump(error_info, f, indent=2)
        
        raise


if __name__ == '__main__':
    main()
