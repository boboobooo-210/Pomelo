#!/usr/bin/env python3
"""
Token Codebookæ ‡æ³¨å·¥å…·
ç”¨äºæ ‡æ³¨VQ-VAEç”Ÿæˆçš„Tokenï¼Œä¸ºæ¯ä¸ªTokenæä¾›è¯­ä¹‰æè¿°
"""

import os
import sys
import json
import numpy as np
import warnings
import matplotlib
matplotlib.use('TkAgg')  # ä½¿ç”¨äº¤äº’å¼åç«¯
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D
from pathlib import Path
from datetime import datetime
from typing import Dict, List, Optional

# ç¦ç”¨å­—ä½“è­¦å‘Š
warnings.filterwarnings('ignore', category=UserWarning, message='.*Glyph.*missing from font.*')

# é…ç½®matplotlibä½¿ç”¨åŸºæœ¬å­—ä½“ï¼ˆé¿å…ä¸­æ–‡å­—ä½“é—®é¢˜ï¼‰
plt.rcParams['font.family'] = 'DejaVu Sans'
plt.rcParams['axes.unicode_minus'] = False

# æ·»åŠ é¡¹ç›®è·¯å¾„
project_root = Path(__file__).parent.parent
sys.path.append(str(project_root))

# å°è¯•å¯¼å…¥å¯è§†åŒ–çª—å£ï¼ˆå¦‚æœå¯ç”¨ï¼‰
try:
    from tools.visualization_window import SkeletonVisualizationWindow
    VISUALIZATION_AVAILABLE = True
except ImportError:
    print("âš ï¸ å¯è§†åŒ–çª—å£ä¸å¯ç”¨ï¼Œå°†ä½¿ç”¨matplotlibåŸºç¡€å¯è§†åŒ–")
    VISUALIZATION_AVAILABLE = False


class TokenCodebookAnnotator:
    """Token Codebookæ ‡æ³¨å·¥å…·"""
    
    def __init__(self):
        """åˆå§‹åŒ–æ ‡æ³¨å·¥å…·"""
        self.project_root = project_root
        self.token_analysis_dir = self.project_root / "token_analysis"
        self.codebook_path = self.token_analysis_dir / "codebook_annotation_template.json"
        self.output_path = self.token_analysis_dir / "codebook_annotations.json"
        
        # NTU RGB+D 25å…³èŠ‚ç‚¹çš„è¯­ä¹‰åˆ†ç»„ï¼ˆæ¥è‡ªGCNSkeletonTokenizerï¼‰
        self.semantic_groups = {
            'head_spine': [0, 1, 2, 3, 20],               # å¤´éƒ¨+è„ŠæŸ± (5ä¸ªå…³èŠ‚)
            'left_arm': [4, 5, 6, 7, 21, 22],             # å·¦è‡‚+å·¦æ‰‹ (6ä¸ªå…³èŠ‚)
            'right_arm': [8, 9, 10, 11, 23, 24],          # å³è‡‚+å³æ‰‹ (6ä¸ªå…³èŠ‚)
            'left_leg': [12, 13, 14, 15],                 # å·¦è…¿ (4ä¸ªå…³èŠ‚)
            'right_leg': [16, 17, 18, 19]                 # å³è…¿ (4ä¸ªå…³èŠ‚)
        }
        
        # MARSåŠ¨ä½œæ¨¡æ¿ï¼ˆç”¨äºå¿«é€Ÿé€‰æ‹©ï¼‰
        self.mars_action_templates = {
            'head_spine': [
                "æ­£å¸¸å§¿æ€", "æŠ¬å¤´", "ä½å¤´çœ‹", "å·¦ä¾§è½¬", "å³ä¾§è½¬",
                "æŒºç›´ç«™ç«‹", "å‰å€¾", "åä»°", "å·¦å€¾æ–œ", "å³å€¾æ–œ"
            ],
            'left_arm': [
                "è‡ªç„¶å‚è½", "ä¸Šä¸¾", "å‰ä¼¸", "ä¾§ä¸¾", "å‰è…°",
                "å‘å†…å¼¯æ›²", "è‡ªç„¶å¼¯æ›²", "åä¼¸", "å·¦ä¾§æŠ¬èµ·", "å‘ä¸Šå¼¯æ›²"
            ],
            'right_arm': [
                "è‡ªç„¶å‚è½", "ä¸Šä¸¾", "å‰ä¼¸", "ä¾§ä¸¾", "å‰è…°",
                "å‘å†…å¼¯æ›²", "è‡ªç„¶å¼¯æ›²", "åä¼¸", "å³ä¾§æŠ¬èµ·", "å‘ä¸Šå¼¯æ›²"
            ],
            'left_leg': [
                "ç«™ç«‹", "å¼¯æ›²", "å‰æŠ¬", "ä¾§æŠ¬", "è¹²ä¸‹",
                "åé€€", "è¸¢å‡º", "å‘å‰è·¨æ­¥", "å‘å·¦è·¨æ­¥", "è·³è·ƒ"
            ],
            'right_leg': [
                "ç«™ç«‹", "å¼¯æ›²", "å‰æŠ¬", "ä¾§æŠ¬", "è¹²ä¸‹",
                "åé€€", "è¸¢å‡º", "å‘å‰è·¨æ­¥", "å‘å³è·¨æ­¥", "è·³è·ƒ"
            ]
        }
        
        self.part_names = ['head_spine', 'left_arm', 'right_arm', 'left_leg', 'right_leg']
        self.part_display_names = ['å¤´éƒ¨è„ŠæŸ±', 'å·¦è‡‚', 'å³è‡‚', 'å·¦è…¿', 'å³è…¿']
        
        # åŠ è½½MARS Tokenæ•°æ®é›†
        self.dataset = None
        self.load_mars_token_dataset()
        
        # åŠ è½½Tokenæ¨¡æ¿ï¼ˆéœ€è¦åœ¨æ•°æ®é›†åŠ è½½åï¼Œå› ä¸ºå¯èƒ½éœ€è¦ç”Ÿæˆæ¨¡æ¿ï¼‰
        self.load_token_template()
        
        # æ³¨æ„ï¼šä¸éœ€è¦åŠ è½½VQ-VAEæ¨¡å‹ï¼Œå› ä¸ºæˆ‘ä»¬ç›´æ¥ä½¿ç”¨æ•°æ®é›†ä¸­çš„çœŸå®éª¨æ¶æ ·æœ¬è¿›è¡Œå¯è§†åŒ–
        
        # å½“å‰æ ‡æ³¨çŠ¶æ€
        self.current_annotations = {}
        self.load_existing_annotations()
    
    def load_token_template(self):
        """åŠ è½½Tokenæ¨¡æ¿"""
        if not self.codebook_path.exists():
            print(f"âš ï¸ æ¨¡æ¿æ–‡ä»¶ä¸å­˜åœ¨ï¼Œæ­£åœ¨ç”Ÿæˆ...")
            self.generate_token_template()
        
        with open(self.codebook_path, 'r', encoding='utf-8') as f:
            data = json.load(f)
            self.token_template = data['codebook_annotation']
            self.metadata = data['metadata']
        
        print(f"âœ… åŠ è½½Tokenæ¨¡æ¿: {self.metadata['total_unique_tokens']} ä¸ªToken")
    
    def generate_token_template(self):
        """ç”ŸæˆTokenæ¨¡æ¿ï¼ˆä»æ•°æ®é›†ä¸­æå–å”¯ä¸€Tokenï¼‰"""
        if self.dataset is None:
            print("âŒ æ— æ³•ç”Ÿæˆæ¨¡æ¿ï¼šæ•°æ®é›†æœªåŠ è½½")
            sys.exit(1)
        
        # åˆ›å»ºç›®å½•
        self.token_analysis_dir.mkdir(parents=True, exist_ok=True)
        
        # ç»Ÿè®¡æ‰€æœ‰å”¯ä¸€TokenåŠå…¶å‡ºç°æ¬¡æ•°
        token_counts = {part: {} for part in self.part_names}
        total_samples = len(self.dataset)
        
        print(f"ğŸ“Š åˆ†æ {total_samples} ä¸ªæ ·æœ¬ï¼Œæå–å”¯ä¸€Token...")
        for idx, sample in enumerate(self.dataset):
            tokens = sample['tokens']
            for i, part in enumerate(self.part_names):
                token_id = int(tokens[i])
                token_counts[part][token_id] = token_counts[part].get(token_id, 0) + 1
            
            # æ˜¾ç¤ºè¿›åº¦
            if (idx + 1) % 5000 == 0:
                print(f"   å·²åˆ†æ: {idx + 1}/{total_samples} ({(idx+1)/total_samples*100:.1f}%)")
        
        # åˆ›å»ºæ¨¡æ¿å¹¶ç”Ÿæˆç»Ÿè®¡ä¿¡æ¯
        template = {}
        total_tokens = 0
        
        print(f"\nâœ¨ Tokenç»Ÿè®¡:")
        for part, display_name in zip(self.part_names, self.part_display_names):
            template[part] = {}
            sorted_tokens = sorted(token_counts[part].items(), key=lambda x: x[1], reverse=True)
            
            print(f"  {display_name}: {len(sorted_tokens)} ä¸ªå”¯ä¸€Token")
            for token_id, count in sorted_tokens[:3]:  # æ˜¾ç¤ºå‰3ä¸ªæœ€å¸¸è§çš„
                print(f"    - Token {token_id}: {count} æ¬¡ ({count/total_samples*100:.2f}%)")
            
            for token_id, count in sorted_tokens:
                template[part][str(token_id)] = f"[å¾…æ ‡æ³¨] Token{token_id}"
                total_tokens += 1
        
        # ä¿å­˜æ¨¡æ¿
        output_data = {
            'codebook_annotation': template,
            'metadata': {
                'total_samples': total_samples,
                'total_unique_tokens': total_tokens,
                'estimated_annotation_time_hours': total_tokens * 0.5 / 60,
                'token_counts': {part: dict(sorted(counts.items())) for part, counts in token_counts.items()},
                'created_at': datetime.now().isoformat()
            }
        }
        
        with open(self.codebook_path, 'w', encoding='utf-8') as f:
            json.dump(output_data, f, ensure_ascii=False, indent=2)
        
        print(f"\nâœ… ç”Ÿæˆæ¨¡æ¿: {self.codebook_path}")
        print(f"   æ€»è®¡ {total_tokens} ä¸ªå”¯ä¸€Token")
        print(f"   é¢„è®¡æ ‡æ³¨æ—¶é—´: {total_tokens * 0.5:.1f} åˆ†é’Ÿ")
        
    def load_existing_annotations(self):
        """åŠ è½½å·²æœ‰æ ‡æ³¨"""
        if self.output_path.exists():
            with open(self.output_path, 'r', encoding='utf-8') as f:
                data = json.load(f)
                self.current_annotations = data.get('codebook_annotation', {})
            print(f"âœ… åŠ è½½å·²æœ‰æ ‡æ³¨: {self.count_annotated()} / {self.metadata['total_unique_tokens']}")
        else:
            self.current_annotations = {part: {} for part in self.part_names}
            print("ğŸ“ åˆå§‹åŒ–æ–°çš„æ ‡æ³¨æ–‡ä»¶")
    
    def load_mars_token_dataset(self):
        """åŠ è½½MARS Tokenæ•°æ®é›†ï¼ˆæ‰€æœ‰æ ·æœ¬ï¼‰"""
        print("ğŸ“Š åŠ è½½MARS Tokenæ•°æ®é›†...")
        
        data_dir_paths = [
            "/home/uo/myProject/CRSkeleton/data/MARS_recon_tokens",
            "/home/uo/myProject/HumanPoint-BERT/data/MARS_recon_tokens",
            self.project_root / "data" / "MARS_recon_tokens"
        ]
        
        for dir_path in data_dir_paths:
            dir_path = Path(dir_path)
            if dir_path.exists() and dir_path.is_dir():
                try:
                    # åŠ è½½æ‰€æœ‰trainæ ·æœ¬ï¼ˆä¸é™åˆ¶æ•°é‡ï¼‰
                    npz_files = list(dir_path.glob("train_sample_*.npz"))
                    
                    if not npz_files:
                        # å¦‚æœæ²¡æœ‰trainæ ·æœ¬ï¼Œå°è¯•validateæ ·æœ¬
                        npz_files = list(dir_path.glob("validate_sample_*.npz"))
                    
                    if not npz_files:
                        continue
                    
                    npz_files = sorted(npz_files)
                    total_files = len(npz_files)
                    
                    print(f"   å‘ç° {total_files} ä¸ªæ ·æœ¬æ–‡ä»¶ï¼Œå¼€å§‹åŠ è½½...")
                    
                    self.dataset = []
                    for i, npz_file in enumerate(npz_files):
                        try:
                            data = np.load(npz_file, allow_pickle=True)
                            
                            # ä¼˜å…ˆä½¿ç”¨çº¯ç æœ¬é‡æ„ï¼ˆbase_reconstructedï¼‰ï¼Œå¦‚æœæ²¡æœ‰åˆ™ç”¨reconstructed
                            if 'base_reconstructed' in data:
                                skeleton_data = data['base_reconstructed']  # çº¯ç æœ¬é‡æ„ï¼ˆæ— æ®‹å·®ï¼‰
                            else:
                                skeleton_data = data['reconstructed']  # å«æ®‹å·®çš„é‡æ„
                            
                            self.dataset.append({
                                'index': len(self.dataset),
                                'tokens': data['tokens'],  # (5,) çš„tokenåºåˆ—
                                'skeleton': skeleton_data,  # (25, 3) çš„éª¨æ¶åæ ‡
                                'file': npz_file.name
                            })
                            
                            # æ¯åŠ è½½1000ä¸ªæ˜¾ç¤ºè¿›åº¦
                            if (i + 1) % 1000 == 0:
                                print(f"   å·²åŠ è½½: {i + 1}/{total_files} ({(i+1)/total_files*100:.1f}%)")
                                
                        except Exception as e:
                            continue
                    
                    print(f"âœ… æˆåŠŸåŠ è½½æ•°æ®é›†: {dir_path}")
                    print(f"   æ€»æ ·æœ¬æ•°: {len(self.dataset)}")
                    
                    if len(self.dataset) > 0:
                        sample = self.dataset[0]
                        print(f"   ç¤ºä¾‹Token: {sample['tokens']}")
                        print(f"   éª¨æ¶å½¢çŠ¶: {sample['skeleton'].shape}")
                    
                    return
                    
                except Exception as e:
                    print(f"âš ï¸ åŠ è½½å¤±è´¥ {dir_path}: {e}")
        
        print("âŒ æœªæ‰¾åˆ°MARS Tokenæ•°æ®é›†ï¼ŒæŸäº›åŠŸèƒ½å¯èƒ½ä¸å¯ç”¨")
        self.dataset = None
    
    def count_annotated(self) -> int:
        """ç»Ÿè®¡å·²æ ‡æ³¨Tokenæ•°é‡"""
        count = 0
        for part in self.part_names:
            if part in self.current_annotations:
                for token_id, annotation in self.current_annotations[part].items():
                    if not annotation.startswith("[å¾…æ ‡æ³¨]"):
                        count += 1
        return count
    
    def get_samples_with_token(self, body_part: str, token_id: int, max_samples: int = 5) -> List[Dict]:
        """è·å–åŒ…å«æŒ‡å®šTokençš„æ ·æœ¬"""
        if self.dataset is None or len(self.dataset) == 0:
            return []
        
        part_index = self.part_names.index(body_part)
        samples = []
        
        try:
            for sample in self.dataset:
                if len(samples) >= max_samples:
                    break
                
                tokens = sample['tokens']
                
                # æ£€æŸ¥æ˜¯å¦åŒ¹é…Token
                if len(tokens) > part_index and int(tokens[part_index]) == token_id:
                    samples.append(sample)
        
        except Exception as e:
            print(f"âš ï¸ æœç´¢æ ·æœ¬æ—¶å‡ºé”™: {e}")
        
        return samples
    
    def visualize_token_samples(self, body_part: str, token_id: int):
        """å¯è§†åŒ–åŒ…å«æŒ‡å®šTokençš„æ ·æœ¬"""
        samples = self.get_samples_with_token(body_part, token_id, max_samples=5)
        
        if not samples:
            print(f"âš ï¸ æœªæ‰¾åˆ°åŒ…å« Token {token_id} çš„æ ·æœ¬")
            print(f"   å°è¯•ä½¿ç”¨æ¨¡å‹è§£ç å¯è§†åŒ–...")
            
            # å¦‚æœæœ‰VQ-VAEæ¨¡å‹ï¼Œå°è¯•è§£ç å¯è§†åŒ–
            if self.vqvae_model is not None:
                self.visualize_token_from_model(body_part, token_id)
            else:
                print(f"   æç¤º: Token {token_id} ({self.part_display_names[self.part_names.index(body_part)]}) å­˜åœ¨ä½†æš‚æ— æ ·æœ¬")
                print(f"   è¯·æ ¹æ®èº«ä½“éƒ¨ä½æè¿°è¿›è¡Œæ ‡æ³¨")
            return
        
        part_name_en = {
            'head_spine': 'Head-Spine',
            'left_arm': 'Left Arm',
            'right_arm': 'Right Arm',
            'left_leg': 'Left Leg',
            'right_leg': 'Right Leg'
        }[body_part]
        
        print(f"\nğŸ“Š Found {len(samples)} samples with Token {token_id} ({part_name_en})")
        
        # ä½¿ç”¨matplotlibå¯è§†åŒ–ï¼ˆéé˜»å¡æ¨¡å¼ï¼‰
        num_samples = min(len(samples), 3)
        fig = plt.figure(figsize=(5 * num_samples, 5))
        
        for i, sample in enumerate(samples[:num_samples]):
            ax = fig.add_subplot(1, num_samples, i+1, projection='3d')
            
            skeleton = sample['skeleton']
            if skeleton is not None:
                # å¤„ç†ä¸åŒçš„skeletonæ ¼å¼
                if len(skeleton.shape) == 3:  # (C, T, V) æˆ– (T, V, C)
                    # å–ç¬¬ä¸€å¸§
                    if skeleton.shape[-1] == 25:  # (T, V, C)
                        skeleton = skeleton[0]  # å–ç¬¬ä¸€å¸§
                    elif skeleton.shape[0] == 3:  # (C, T, V)
                        skeleton = skeleton[:, 0, :].T  # è½¬æ¢ä¸º (V, C)
                    else:  # (T, V, C) å…¶ä»–æƒ…å†µ
                        skeleton = skeleton[0]
                
                elif len(skeleton.shape) == 2:  # (V, C)
                    pass  # å·²ç»æ˜¯æ­£ç¡®æ ¼å¼
                
                if skeleton.shape[0] == 25 and skeleton.shape[1] == 3:
                    self._plot_skeleton(ax, skeleton, body_part)
                    ax.set_title(f"Sample {sample['index']}\nTokens: {sample['tokens']}", fontsize=10)
                else:
                    ax.text(0.5, 0.5, 0.5, f"Invalid shape\n{skeleton.shape}", ha='center', va='center')
                    ax.set_title(f"Sample {sample['index']}")
            else:
                ax.text(0.5, 0.5, 0.5, "No skeleton data", ha='center', va='center')
                ax.set_title(f"Sample {sample['index']}")
        
        # ä¸è®¾ç½®çª—å£æ ‡é¢˜ï¼Œé¿å…ä¸­æ–‡å­—ä½“è­¦å‘Š
        # fig.canvas.manager.set_window_title(f'Token {token_id} - {part_name_en}')
        
        plt.tight_layout()
        plt.ion()  # å¼€å¯äº¤äº’æ¨¡å¼
        plt.show(block=False)  # éé˜»å¡æ˜¾ç¤º
        plt.pause(0.1)
        
        print("\nğŸ’¡ Window opened (non-blocking). You can close it manually or continue annotation.")
    
    def visualize_token_from_model(self, body_part: str, token_id: int):
        """ä½¿ç”¨VQ-VAEæ¨¡å‹è§£ç Tokenè¿›è¡Œå¯è§†åŒ–ï¼ˆå¼€å‘ä¸­ï¼‰"""
        print(f"   æ¨¡å‹è§£ç åŠŸèƒ½å¼€å‘ä¸­...")
        # TODO: å®ç°ä»å•ä¸ªTokenè§£ç çš„åŠŸèƒ½
        # è¿™éœ€è¦å¯¹VQ-VAEæ¨¡å‹è¿›è¡Œä¿®æ”¹ï¼Œæ”¯æŒéƒ¨åˆ†è§£ç 
    
    def _plot_skeleton(self, ax, skeleton, highlight_part: str = None):
        """ç»˜åˆ¶éª¨æ¶ï¼ˆæ”¹è¿›ç‰ˆï¼Œä½¿ç”¨GCNSkeletonTokenizerçš„è¿æ¥å’Œåˆ†ç»„ï¼‰"""
        # åæ ‡è½¬æ¢: (x, y, z) â†’ (x, y, -z)
        skeleton = skeleton.copy()
        skeleton[:, 2] = -skeleton[:, 2]  # ç¿»è½¬Zè½´
        
        # NTU RGB+Déª¨æ¶è¿æ¥ï¼ˆæ¥è‡ªGCNSkeletonTokenizerï¼‰
        connections = [
            # å¤´éƒ¨å’Œè„ŠæŸ±
            (3, 2), (2, 20), (20, 1), (1, 0),
            # å·¦è‡‚
            (20, 4), (4, 5), (5, 6), (6, 22), (6, 7), (7, 21),
            # å³è‡‚
            (20, 8), (8, 9), (9, 10), (10, 24), (10, 11), (11, 23),
            # å·¦è…¿
            (0, 12), (12, 13), (13, 14), (14, 15),
            # å³è…¿
            (0, 16), (16, 17), (17, 18), (18, 19)
        ]
        
        # è·å–å½“å‰éƒ¨ä½çš„å…³èŠ‚ç´¢å¼•
        highlight_joints = set(self.semantic_groups.get(highlight_part, [])) if highlight_part else set()
        
        # ç»˜åˆ¶è¿æ¥çº¿
        for conn in connections:
            i, j = conn[0], conn[1]
            if i < len(skeleton) and j < len(skeleton):
                points = skeleton[[i, j]]
                
                # åˆ¤æ–­è¿æ¥æ˜¯å¦å±äºé«˜äº®éƒ¨ä½
                if highlight_part and (i in highlight_joints or j in highlight_joints):
                    ax.plot3D(points[:, 0], points[:, 1], points[:, 2], 
                             'r-', linewidth=3, alpha=0.9)
                else:
                    ax.plot3D(points[:, 0], points[:, 1], points[:, 2], 
                             'b-', linewidth=1, alpha=0.3)
        
        # ç»˜åˆ¶å…³èŠ‚ç‚¹
        for i, point in enumerate(skeleton):
            if i in highlight_joints:
                # é«˜äº®å½“å‰éƒ¨ä½çš„å…³èŠ‚
                ax.scatter(point[0], point[1], point[2], 
                          c='red', s=100, marker='o', edgecolors='darkred', linewidths=2)
            else:
                # å…¶ä»–å…³èŠ‚
                ax.scatter(point[0], point[1], point[2], 
                          c='lightblue', s=30, marker='o', alpha=0.4)
        
        # æ·»åŠ å…³èŠ‚ç¼–å·ï¼ˆä»…é«˜äº®éƒ¨ä½ï¼‰
        if highlight_part:
            for i in highlight_joints:
                if i < len(skeleton):
                    point = skeleton[i]
                    ax.text(point[0], point[1], point[2], f' {i}', 
                           fontsize=8, color='darkred', weight='bold')
        
        ax.set_xlabel('X')
        ax.set_ylabel('Y (Up)')
        ax.set_zlabel('Z')
        
        # è®¾ç½®ç›¸åŒçš„åæ ‡è½´èŒƒå›´
        max_range = np.array([
            skeleton[:, 0].max() - skeleton[:, 0].min(),
            skeleton[:, 1].max() - skeleton[:, 1].min(),
            skeleton[:, 2].max() - skeleton[:, 2].min()
        ]).max() / 2.0
        
        mid_x = (skeleton[:, 0].max() + skeleton[:, 0].min()) * 0.5
        mid_y = (skeleton[:, 1].max() + skeleton[:, 1].min()) * 0.5
        mid_z = (skeleton[:, 2].max() + skeleton[:, 2].min()) * 0.5
        
        ax.set_xlim(mid_x - max_range, mid_x + max_range)
        ax.set_ylim(mid_y - max_range, mid_y + max_range)
        ax.set_zlim(mid_z - max_range, mid_z + max_range)
        
        # è®¾ç½®è§†è§’ï¼šYè½´å‘ä¸Šï¼ˆé‡è¦ï¼ï¼‰
        ax.view_init(elev=15, azim=45)
        
        # æ·»åŠ æ ‡æ³¨è¯´æ˜Yè½´æ˜¯å‚ç›´æ–¹å‘ï¼ˆå¤´åœ¨ä¸Šï¼‰
        ax.text2D(0.02, 0.98, f"Yâ†‘(Head up)", transform=ax.transAxes, 
                 fontsize=10, color='red', weight='bold', va='top')
    
    def annotate_token(self, body_part: str, token_id: int, auto_visualize: bool = False):
        """æ ‡æ³¨å•ä¸ªToken
        
        Args:
            body_part: èº«ä½“éƒ¨ä½
            token_id: Token ID
            auto_visualize: å¦‚æœä¸ºTrueï¼Œè·³è¿‡è¯¢é—®ç›´æ¥è¿›å…¥æ ‡æ³¨ï¼ˆå¯è§†åŒ–å·²åœ¨å¤–éƒ¨æ˜¾ç¤ºï¼‰
        """
        part_idx = self.part_names.index(body_part)
        part_display = self.part_display_names[part_idx]
        
        print(f"\n{'='*60}")
        print(f"ğŸ“ æ ‡æ³¨ Token {token_id} ({part_display})")
        print(f"{'='*60}")
        
        # æ£€æŸ¥æ˜¯å¦å·²æ ‡æ³¨
        if body_part in self.current_annotations and str(token_id) in self.current_annotations[body_part]:
            existing = self.current_annotations[body_part][str(token_id)]
            if not existing.startswith("[å¾…æ ‡æ³¨]"):
                print(f"å½“å‰æ ‡æ³¨: {existing}")
                overwrite = input("æ˜¯å¦è¦†ç›–ç°æœ‰æ ‡æ³¨? (y/n): ").strip().lower()
                if overwrite != 'y':
                    return
        
        # å¦‚æœä¸æ˜¯è‡ªåŠ¨æ¨¡å¼ï¼Œè¯¢é—®æ˜¯å¦æ˜¾ç¤ºå¯è§†åŒ–
        if not auto_visualize:
            show_samples = input("æ˜¯å¦æ˜¾ç¤ºåŒ…å«æ­¤Tokençš„æ ·æœ¬? (y/n/c=å…³é—­æ‰€æœ‰çª—å£, é»˜è®¤y): ").strip().lower()
            if show_samples == 'c':
                plt.close('all')
                print("âœ… å·²å…³é—­æ‰€æœ‰å¯è§†åŒ–çª—å£")
            elif show_samples == 'y':
                self.visualize_token_samples(body_part, token_id)
        
        # æ˜¾ç¤ºå¿«é€Ÿé€‰é¡¹
        print(f"\nå¿«é€Ÿé€‰æ‹© ({part_display}):")
        templates = self.mars_action_templates[body_part]
        for i, action in enumerate(templates, 1):
            print(f"  {i}. {action}")
        print("  0. è‡ªå®šä¹‰è¾“å…¥")
        
        choice = input(f"\né€‰æ‹© (1-{len(templates)}, 0=è‡ªå®šä¹‰): ").strip()
        
        if choice == '0':
            annotation = input("è¯·è¾“å…¥è‡ªå®šä¹‰æ ‡æ³¨: ").strip()
        elif choice.isdigit() and 1 <= int(choice) <= len(templates):
            annotation = templates[int(choice) - 1]
            
            # å…è®¸æ·»åŠ è¯¦ç»†æè¿°
            add_detail = input(f"æ˜¯å¦æ·»åŠ æ›´è¯¦ç»†çš„æè¿°? (y/n, é»˜è®¤n): ").strip().lower()
            if add_detail == 'y':
                detail = input("è¯¦ç»†æè¿°: ").strip()
                annotation = f"{annotation}ï¼ˆ{detail}ï¼‰"
        else:
            print("âŒ æ— æ•ˆé€‰æ‹©ï¼Œè·³è¿‡")
            return
        
        # ä¿å­˜æ ‡æ³¨
        if body_part not in self.current_annotations:
            self.current_annotations[body_part] = {}
        
        self.current_annotations[body_part][str(token_id)] = annotation
        print(f"âœ… å·²æ ‡æ³¨: Token {token_id} = {annotation}")
        
        # è‡ªåŠ¨ä¿å­˜
        self.save_annotations()
    
    def annotate_body_part(self, body_part: str):
        """æ ‡æ³¨æŸä¸ªèº«ä½“éƒ¨ä½çš„æ‰€æœ‰Token"""
        part_idx = self.part_names.index(body_part)
        part_display = self.part_display_names[part_idx]
        
        tokens = list(self.token_template[body_part].keys())
        tokens = [int(t) for t in tokens]
        tokens.sort()
        
        print(f"\n{'='*60}")
        print(f"ğŸ“ æ ‡æ³¨èº«ä½“éƒ¨ä½: {part_display}")
        print(f"   å…± {len(tokens)} ä¸ªToken")
        print(f"{'='*60}")
        
        for i, token_id in enumerate(tokens, 1):
            print(f"\nè¿›åº¦: {i}/{len(tokens)}")
            
            # è‡ªåŠ¨æ˜¾ç¤ºå¯è§†åŒ–
            self.visualize_token_samples(body_part, token_id)
            
            # æ ‡æ³¨Token
            self.annotate_token(body_part, token_id, auto_visualize=True)
            
            if i < len(tokens):
                continue_choice = input("\nç»§ç»­ä¸‹ä¸€ä¸ªToken? (y/n/q=é€€å‡º, é»˜è®¤y): ").strip().lower()
                if continue_choice == 'q':
                    print("é€€å‡ºæ ‡æ³¨")
                    break
                elif continue_choice == 'n':
                    return
    
    def save_annotations(self):
        """ä¿å­˜æ ‡æ³¨ç»“æœ"""
        output_data = {
            'codebook_annotation': self.current_annotations,
            'metadata': {
                'total_samples': self.metadata['total_samples'],
                'total_unique_tokens': self.metadata['total_unique_tokens'],
                'annotated_tokens': self.count_annotated(),
                'annotation_progress': f"{self.count_annotated()}/{self.metadata['total_unique_tokens']}",
                'last_updated': datetime.now().isoformat()
            }
        }
        
        with open(self.output_path, 'w', encoding='utf-8') as f:
            json.dump(output_data, f, ensure_ascii=False, indent=2)
        
        print(f"ğŸ’¾ å·²ä¿å­˜æ ‡æ³¨: {self.output_path}")
    
    def show_progress(self):
        """æ˜¾ç¤ºæ ‡æ³¨è¿›åº¦"""
        print(f"\n{'='*60}")
        print(f"ğŸ“Š æ ‡æ³¨è¿›åº¦ç»Ÿè®¡")
        print(f"{'='*60}")
        
        total = self.metadata['total_unique_tokens']
        annotated = self.count_annotated()
        percentage = (annotated / total * 100) if total > 0 else 0
        
        print(f"æ€»ä½“è¿›åº¦: {annotated}/{total} ({percentage:.1f}%)")
        print(f"å‰©ä½™Token: {total - annotated}")
        
        print(f"\nå„éƒ¨ä½è¿›åº¦:")
        for part, display_name in zip(self.part_names, self.part_display_names):
            part_tokens = list(self.token_template[part].keys())
            part_annotated = sum(
                1 for tid in part_tokens
                if part in self.current_annotations 
                and str(tid) in self.current_annotations[part]
                and not self.current_annotations[part][str(tid)].startswith("[å¾…æ ‡æ³¨]")
            )
            part_total = len(part_tokens)
            part_pct = (part_annotated / part_total * 100) if part_total > 0 else 0
            
            print(f"  {display_name:8s}: {part_annotated:2d}/{part_total:2d} ({part_pct:5.1f}%)")
        
        print(f"{'='*60}\n")
    
    def interactive_menu(self):
        """äº¤äº’å¼èœå•"""
        while True:
            print(f"\n{'='*60}")
            print("Token Codebook æ ‡æ³¨å·¥å…·")
            print(f"{'='*60}")
            print("1. æ ‡æ³¨å¤´éƒ¨è„ŠæŸ± Token")
            print("2. æ ‡æ³¨å·¦è‡‚ Token")
            print("3. æ ‡æ³¨å³è‡‚ Token")
            print("4. æ ‡æ³¨å·¦è…¿ Token")
            print("5. æ ‡æ³¨å³è…¿ Token")
            print("6. æ ‡æ³¨å•ä¸ªToken")
            print("7. æŸ¥çœ‹æ ‡æ³¨è¿›åº¦")
            print("8. å¯¼å‡ºæ ‡æ³¨ç»“æœ")
            print("9. æŸ¥çœ‹Tokenæ ·æœ¬")
            print("c. å…³é—­æ‰€æœ‰å¯è§†åŒ–çª—å£")
            print("0. é€€å‡º")
            print(f"{'='*60}")
            
            choice = input("è¯·é€‰æ‹© (0-9/c): ").strip().lower()
            
            if choice == '1':
                self.annotate_body_part('head_spine')
            elif choice == '2':
                self.annotate_body_part('left_arm')
            elif choice == '3':
                self.annotate_body_part('right_arm')
            elif choice == '4':
                self.annotate_body_part('left_leg')
            elif choice == '5':
                self.annotate_body_part('right_leg')
            elif choice == '6':
                self._annotate_single_token_menu()
            elif choice == '7':
                self.show_progress()
            elif choice == '8':
                self.save_annotations()
                print("âœ… æ ‡æ³¨ç»“æœå·²ä¿å­˜")
            elif choice == '9':
                self._view_token_samples_menu()
            elif choice == 'c':
                plt.close('all')
                print("âœ… å·²å…³é—­æ‰€æœ‰å¯è§†åŒ–çª—å£")
            elif choice == '0':
                print("ä¿å­˜å¹¶é€€å‡º...")
                self.save_annotations()
                plt.close('all')  # å…³é—­æ‰€æœ‰çª—å£
                print("ğŸ‘‹ å†è§ï¼")
                break
            else:
                print("âŒ æ— æ•ˆé€‰æ‹©")
    
    def _annotate_single_token_menu(self):
        """æ ‡æ³¨å•ä¸ªTokençš„èœå•"""
        print("\né€‰æ‹©èº«ä½“éƒ¨ä½:")
        for i, name in enumerate(self.part_display_names, 1):
            print(f"  {i}. {name}")
        
        part_choice = input("é€‰æ‹©éƒ¨ä½ (1-5): ").strip()
        if not part_choice.isdigit() or not (1 <= int(part_choice) <= 5):
            print("âŒ æ— æ•ˆé€‰æ‹©")
            return
        
        body_part = self.part_names[int(part_choice) - 1]
        tokens = list(self.token_template[body_part].keys())
        
        print(f"\n{self.part_display_names[int(part_choice) - 1]} çš„Token:")
        print(", ".join(tokens))
        
        token_id = input("\nè¾“å…¥Token ID: ").strip()
        if token_id not in tokens:
            print(f"âŒ Token {token_id} ä¸åœ¨è¯¥éƒ¨ä½ä¸­")
            return
        
        self.annotate_token(body_part, int(token_id))
    
    def _view_token_samples_menu(self):
        """æŸ¥çœ‹Tokenæ ·æœ¬çš„èœå•"""
        print("\né€‰æ‹©èº«ä½“éƒ¨ä½:")
        for i, name in enumerate(self.part_display_names, 1):
            print(f"  {i}. {name}")
        
        part_choice = input("é€‰æ‹©éƒ¨ä½ (1-5): ").strip()
        if not part_choice.isdigit() or not (1 <= int(part_choice) <= 5):
            print("âŒ æ— æ•ˆé€‰æ‹©")
            return
        
        body_part = self.part_names[int(part_choice) - 1]
        tokens = list(self.token_template[body_part].keys())
        
        print(f"\n{self.part_display_names[int(part_choice) - 1]} çš„Token:")
        print(", ".join(tokens))
        
        token_id = input("\nè¾“å…¥Token ID: ").strip()
        if token_id not in tokens:
            print(f"âŒ Token {token_id} ä¸åœ¨è¯¥éƒ¨ä½ä¸­")
            return
        
        self.visualize_token_samples(body_part, int(token_id))


def main():
    """ä¸»å‡½æ•°"""
    print("ğŸš€ å¯åŠ¨ Token Codebook æ ‡æ³¨å·¥å…·")
    
    annotator = TokenCodebookAnnotator()
    annotator.show_progress()
    annotator.interactive_menu()


if __name__ == "__main__":
    main()
