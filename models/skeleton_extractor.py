#!/usr/bin/env python3
"""
MARS + è½»é‡çº§Transformeréª¨æ¶æå–æ¨¡å‹ (PyTorchç‰ˆæœ¬)
ç»“åˆåŸå§‹MARS CNNæ¶æ„å’ŒTransformeræ³¨æ„åŠ›æœºåˆ¶
ä¿æŒåŸå§‹æ•°æ®æ ¼å¼(n,8,8,5)ä¸å˜
"""
import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
from sklearn.metrics import mean_absolute_error, mean_squared_error
import matplotlib.pyplot as plt
import os
import math

# è®¾ç½®matplotlibåç«¯
plt.switch_backend('Agg')

# GPUé…ç½®
def configure_gpu():
    """é…ç½®GPUä½¿ç”¨"""
    if torch.cuda.is_available():
        device = torch.device('cuda')
        print(f"âœ“ GPUå¯ç”¨: {torch.cuda.get_device_name(0)}")
        print(f"  GPUå†…å­˜: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB")
        return device
    else:
        print("âŒ æœªæ£€æµ‹åˆ°GPUï¼Œå°†ä½¿ç”¨CPU")
        return torch.device('cpu')

# SEæ³¨æ„åŠ›æ¨¡å—
class SEBlock(nn.Module):
    """Squeeze-and-Excitationæ³¨æ„åŠ›å—"""
    def __init__(self, channels, reduction=16):
        super(SEBlock, self).__init__()
        self.avg_pool = nn.AdaptiveAvgPool2d(1)
        self.fc1 = nn.Linear(channels, channels // reduction)
        self.fc2 = nn.Linear(channels // reduction, channels)
        self.sigmoid = nn.Sigmoid()
        
    def forward(self, x):
        b, c, _, _ = x.size()
        # Squeeze
        y = self.avg_pool(x).view(b, c)
        # Excitation
        y = F.relu(self.fc1(y))
        y = self.sigmoid(self.fc2(y))
        y = y.view(b, c, 1, 1)
        # Scale
        return x * y

# æ®‹å·®å— + SEæ³¨æ„åŠ›
class ResidualSEBlock(nn.Module):
    """æ®‹å·®å—ç»“åˆSEæ³¨æ„åŠ›"""
    def __init__(self, in_channels, out_channels, kernel_size=3):
        super(ResidualSEBlock, self).__init__()
        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size, padding=1)
        self.bn1 = nn.BatchNorm2d(out_channels)
        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size, padding=1)
        self.bn2 = nn.BatchNorm2d(out_channels)
        self.se_block = SEBlock(out_channels)
        self.relu = nn.ReLU(inplace=True)
        
        # æ®‹å·®è¿æ¥çš„æŠ•å½±å±‚
        if in_channels != out_channels:
            self.shortcut = nn.Sequential(
                nn.Conv2d(in_channels, out_channels, 1),
                nn.BatchNorm2d(out_channels)
            )
        else:
            self.shortcut = nn.Identity()
    
    def forward(self, x):
        identity = self.shortcut(x)
        
        out = self.relu(self.bn1(self.conv1(x)))
        out = self.bn2(self.conv2(out))
        out = self.se_block(out)
        
        out += identity
        out = self.relu(out)
        
        return out

# ç©ºé—´æ³¨æ„åŠ›æ¨¡å—
class SpatialAttention(nn.Module):
    """ç©ºé—´æ³¨æ„åŠ›æ¨¡å—"""
    def __init__(self, kernel_size=7):
        super(SpatialAttention, self).__init__()
        self.conv = nn.Conv2d(2, 1, kernel_size, padding=kernel_size//2)
        self.sigmoid = nn.Sigmoid()
    
    def forward(self, x):
        avg_out = torch.mean(x, dim=1, keepdim=True)
        max_out, _ = torch.max(x, dim=1, keepdim=True)
        attention = torch.cat([avg_out, max_out], dim=1)
        attention = self.sigmoid(self.conv(attention))
        return x * attention

# ä½ç½®ç¼–ç 
class PositionalEncoding(nn.Module):
    """ä½ç½®ç¼–ç æ¨¡å—"""
    def __init__(self, d_model, max_len=100):
        super(PositionalEncoding, self).__init__()
        pe = torch.zeros(max_len, d_model)
        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)
        div_term = torch.exp(torch.arange(0, d_model, 2).float() * 
                           (-math.log(10000.0) / d_model))
        pe[:, 0::2] = torch.sin(position * div_term)
        pe[:, 1::2] = torch.cos(position * div_term)
        pe = pe.unsqueeze(0).transpose(0, 1)
        self.register_buffer('pe', pe)
    
    def forward(self, x):
        seq_len = x.size(1)
        return x + self.pe[:seq_len, :].transpose(0, 1)

# è½»é‡çº§Transformerå—
class LightweightTransformerBlock(nn.Module):
    """è½»é‡çº§Transformeræ³¨æ„åŠ›å—"""
    def __init__(self, d_model, num_heads=4, dff=256, dropout=0.1):
        super(LightweightTransformerBlock, self).__init__()
        self.mha = nn.MultiheadAttention(d_model, num_heads, dropout=dropout, batch_first=True)
        self.ffn = nn.Sequential(
            nn.Linear(d_model, dff),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(dff, d_model)
        )
        self.layernorm1 = nn.LayerNorm(d_model)
        self.layernorm2 = nn.LayerNorm(d_model)
        self.dropout1 = nn.Dropout(dropout)
        self.dropout2 = nn.Dropout(dropout)
    
    def forward(self, x):
        # è‡ªæ³¨æ„åŠ›
        attn_output, _ = self.mha(x, x, x)
        attn_output = self.dropout1(attn_output)
        out1 = self.layernorm1(x + attn_output)
        
        # å‰é¦ˆç½‘ç»œ
        ffn_output = self.ffn(out1)
        ffn_output = self.dropout2(ffn_output)
        out2 = self.layernorm2(out1 + ffn_output)
        
        return out2

# å¢å¼ºçš„MARSä¸»å¹²ç½‘ç»œï¼ˆå¤šå°ºåº¦ç‰¹å¾èåˆç‰ˆæœ¬ï¼‰
class EnhancedMARSBackbone(nn.Module):
    """åˆ›å»ºå¢å¼ºçš„MARSä¸»å¹²ç½‘ç»œ - æ”¯æŒå¤šå°ºåº¦ç‰¹å¾èåˆ"""
    def __init__(self, input_channels=5, multi_scale=True):
        super(EnhancedMARSBackbone, self).__init__()
        
        self.multi_scale = multi_scale
        
        # åˆå§‹ç‰¹å¾æå–
        self.initial_conv1 = nn.Conv2d(input_channels, 32, 3, padding=1)
        self.initial_bn1 = nn.BatchNorm2d(32)
        self.initial_conv2 = nn.Conv2d(32, 32, 3, padding=1)
        self.initial_bn2 = nn.BatchNorm2d(32)
        
        # æ®‹å·®SEå—
        self.res_se_1 = ResidualSEBlock(32, 64)
        self.maxpool1 = nn.MaxPool2d(2, 2)  # 8x8 -> 4x4
        
        self.res_se_2 = ResidualSEBlock(64, 128)
        self.spatial_att_1 = SpatialAttention()
        
        self.res_se_3 = ResidualSEBlock(128, 256)
        self.spatial_att_2 = SpatialAttention()
        
        # å…¨å±€å¹³å‡æ± åŒ–
        self.global_avg_pool = nn.AdaptiveAvgPool2d(1)
        
        self.relu = nn.ReLU(inplace=True)
        
        # è¾“å‡ºç‰¹å¾ç»´åº¦: 64 + 128 + 256 = 448 (å¤šå°ºåº¦) æˆ– 256 (å•å°ºåº¦)
        self.output_dim = 448 if multi_scale else 256
    
    def forward(self, x):
        # åˆå§‹ç‰¹å¾æå–
        x = self.relu(self.initial_bn1(self.initial_conv1(x)))
        x = self.relu(self.initial_bn2(self.initial_conv2(x)))
        
        # ç¬¬ä¸€ä¸ªæ®‹å·®SEå— - ä¿å­˜ç‰¹å¾1
        feat1 = self.res_se_1(x)  # (batch, 64, 8, 8) æˆ– (batch, 64, 4, 4) after pooling
        x = self.maxpool1(feat1)  # (batch, 64, 4, 4)
        
        # ç¬¬äºŒä¸ªæ®‹å·®SEå— + ç©ºé—´æ³¨æ„åŠ› - ä¿å­˜ç‰¹å¾2
        feat2 = self.res_se_2(x)  # (batch, 128, 4, 4)
        feat2 = self.spatial_att_1(feat2)
        
        # ç¬¬ä¸‰ä¸ªæ®‹å·®SEå— + ç©ºé—´æ³¨æ„åŠ› - ä¿å­˜ç‰¹å¾3
        feat3 = self.res_se_3(feat2)  # (batch, 256, 4, 4)
        feat3 = self.spatial_att_2(feat3)
        
        if self.multi_scale:
            # å¤šå°ºåº¦ç‰¹å¾èåˆ
            feat1_pool = self.global_avg_pool(feat1).flatten(1)  # (batch, 64)
            feat2_pool = self.global_avg_pool(feat2).flatten(1)  # (batch, 128)
            feat3_pool = self.global_avg_pool(feat3).flatten(1)  # (batch, 256)
            
            # æ‹¼æ¥å¤šå°ºåº¦ç‰¹å¾: 64 + 128 + 256 = 448
            output = torch.cat([feat1_pool, feat2_pool, feat3_pool], dim=1)
        else:
            # å•å°ºåº¦ç‰¹å¾ï¼ˆä»…ä½¿ç”¨æœ€æ·±å±‚ç‰¹å¾ï¼‰
            output = self.global_avg_pool(feat3).flatten(1)  # (batch, 256)
        
        return output

# Transformerå¢å¼ºçš„å›å½’å¤´
class TransformerRegressionHead(nn.Module):
    """åˆ›å»ºTransformerå¢å¼ºçš„å›å½’å¤´ - ç®€æ´é«˜æ•ˆç‰ˆæœ¬"""
    def __init__(self, input_dim=256, output_dim=57):
        super(TransformerRegressionHead, self).__init__()
        
        # ç®€æ´çš„ç‰¹å¾æŠ•å½±ï¼ˆå•å±‚ï¼Œè¶³å¤Ÿé«˜æ•ˆï¼‰
        self.feature_projection = nn.Sequential(
            nn.Linear(input_dim, 512),
            nn.ReLU(),
            nn.Dropout(0.3)
        )
        
        # é‡å¡‘ä¸ºåºåˆ—æ ¼å¼å‚æ•°
        self.seq_len = 8
        self.d_model = 64
        
        # ä½ç½®ç¼–ç 
        self.pos_encoding = PositionalEncoding(self.d_model, max_len=self.seq_len)
        
        # ä¿æŒåŸå…ˆçš„2å±‚Transformerï¼ˆç®€æ´é«˜æ•ˆï¼‰
        self.transformer_1 = LightweightTransformerBlock(self.d_model, num_heads=4, dff=128)
        self.transformer_2 = LightweightTransformerBlock(self.d_model, num_heads=4, dff=128)
        
        # å•ä¸€å¹³å‡æ± åŒ–ï¼ˆç®€æ´æœ‰æ•ˆï¼‰
        self.global_avg_pool = nn.AdaptiveAvgPool1d(1)
        
        # ä¼˜åŒ–çš„å›å½’å¤´ - æ¸è¿›å¼é™ç»´è®¾è®¡
        self.regression_head = nn.Sequential(
            nn.Linear(self.d_model, 512),
            nn.ReLU(),
            nn.BatchNorm1d(512),
            nn.Dropout(0.4),
            
            nn.Linear(512, 256),
            nn.ReLU(),
            nn.BatchNorm1d(256),
            nn.Dropout(0.3),
            
            nn.Linear(256, 128),
            nn.ReLU(),
            nn.BatchNorm1d(128),
            nn.Dropout(0.2),
            
            nn.Linear(128, output_dim)
        )
        
        # åˆå§‹åŒ–æƒé‡
        self._init_weights()
    
    def _init_weights(self):
        """æ”¹è¿›çš„æƒé‡åˆå§‹åŒ–"""
        for m in self.modules():
            if isinstance(m, nn.Linear):
                nn.init.xavier_uniform_(m.weight)
                if m.bias is not None:
                    nn.init.constant_(m.bias, 0)
            elif isinstance(m, nn.BatchNorm1d):
                nn.init.constant_(m.weight, 1)
                nn.init.constant_(m.bias, 0)
    
    def forward(self, x):
        # ç‰¹å¾æŠ•å½±
        x = self.feature_projection(x)  # (batch, 512)
        
        # é‡å¡‘ä¸ºåºåˆ—æ ¼å¼
        batch_size = x.size(0)
        x = x.view(batch_size, self.seq_len, self.d_model)  # (batch, 8, 64)
        
        # ä½ç½®ç¼–ç 
        x = self.pos_encoding(x)
        
        # 2å±‚Transformerå¤„ç†ï¼ˆä¿æŒç®€æ´ï¼‰
        x = self.transformer_1(x)
        x = self.transformer_2(x)
        
        # å•ä¸€å¹³å‡æ± åŒ–
        x = x.transpose(1, 2)  # (batch, d_model, seq_len)
        x = self.global_avg_pool(x).squeeze(-1)  # (batch, d_model)
        
        # ä¼˜åŒ–çš„æ¸è¿›å¼å›å½’å¤´
        output = self.regression_head(x)
        
        return output

# å®Œæ•´çš„MARS+Transformeræ¨¡å‹
class MARSTransformerModel(nn.Module):
    """å®Œæ•´çš„MARS+Transformeréª¨æ¶æå–æ¨¡å‹ - æ”¯æŒå¤šå°ºåº¦ç‰¹å¾èåˆ"""
    def __init__(self, input_channels=5, output_dim=57, multi_scale=True):
        super(MARSTransformerModel, self).__init__()
        
        # MARS CNNä¸»å¹²ï¼ˆæ”¯æŒå¤šå°ºåº¦ç‰¹å¾èåˆï¼‰
        self.backbone = EnhancedMARSBackbone(input_channels, multi_scale=multi_scale)
        
        # æ ¹æ®æ˜¯å¦ä½¿ç”¨å¤šå°ºåº¦ç¡®å®šè¾“å…¥ç»´åº¦
        input_dim = self.backbone.output_dim  # 448 (å¤šå°ºåº¦) æˆ– 256 (å•å°ºåº¦)
        
        # Transformerå›å½’å¤´ï¼ˆè‡ªåŠ¨é€‚é…è¾“å…¥ç»´åº¦ï¼‰
        self.regression_head = TransformerRegressionHead(input_dim, output_dim)
        
        print(f"âœ“ æ¨¡å‹åˆå§‹åŒ–å®Œæˆ - å¤šå°ºåº¦èåˆ: {multi_scale}, è¾“å…¥ç»´åº¦: {input_dim}")
    
    def forward(self, x):
        # CNNç‰¹å¾æå–ï¼ˆå¤šå°ºåº¦æˆ–å•å°ºåº¦ï¼‰
        features = self.backbone(x)
        
        # Transformerå›å½’
        output = self.regression_head(features)
        
        return output

# æ•°æ®é›†ç±»
class RadarSkeletonDataset(Dataset):
    """é›·è¾¾éª¨æ¶æ•°æ®é›†"""
    def __init__(self, features, labels):
        self.features = torch.FloatTensor(features)
        self.labels = torch.FloatTensor(labels)
    
    def __len__(self):
        return len(self.features)
    
    def __getitem__(self, idx):
        return self.features[idx], self.labels[idx]

# æ•°æ®åŠ è½½å’Œé¢„å¤„ç†
def load_and_preprocess_data():
    """åŠ è½½å¹¶é¢„å¤„ç†æ•°æ®"""
    print("ğŸ”„ åŠ è½½æ•°æ®...")
    
    # åŠ è½½ç‰¹å¾æ•°æ®
    featuremap_train = np.load('/home/uo/myProject/HumanPoint-BERT/data/MARS/featuremap_train.npy')
    featuremap_validate = np.load('/home/uo/myProject/HumanPoint-BERT/data/MARS/featuremap_validate.npy')
    featuremap_test = np.load('/home/uo/myProject/HumanPoint-BERT/data/MARS/featuremap_test.npy')
    
    print(f"è®­ç»ƒæ•°æ®: {featuremap_train.shape}")
    print(f"éªŒè¯æ•°æ®: {featuremap_validate.shape}")
    print(f"æµ‹è¯•æ•°æ®: {featuremap_test.shape}")
    
    # åŠ è½½æ ‡ç­¾æ•°æ®
    labels_train = np.load('/home/uo/myProject/HumanPoint-BERT/data/MARS/labels_train.npy')
    labels_validate = np.load('/home/uo/myProject/HumanPoint-BERT/data/MARS/labels_validate.npy')
    labels_test = np.load('/home/uo/myProject/HumanPoint-BERT/data/MARS/labels_test.npy')
    
    print(f"æ ‡ç­¾æ•°æ®: {labels_train.shape}")
    
    # è½¬æ¢æ•°æ®æ ¼å¼ï¼š(N, H, W, C) -> (N, C, H, W)
    featuremap_train = np.transpose(featuremap_train, (0, 3, 1, 2))
    featuremap_validate = np.transpose(featuremap_validate, (0, 3, 1, 2))
    featuremap_test = np.transpose(featuremap_test, (0, 3, 1, 2))
    
    return (featuremap_train, featuremap_validate, featuremap_test, 
            labels_train, labels_validate, labels_test)

def create_data_loaders(train_features, train_labels, val_features, val_labels, 
                       test_features, test_labels, batch_size=32):
    """åˆ›å»ºæ•°æ®åŠ è½½å™¨"""
    train_dataset = RadarSkeletonDataset(train_features, train_labels)
    val_dataset = RadarSkeletonDataset(val_features, val_labels)
    test_dataset = RadarSkeletonDataset(test_features, test_labels)
    
    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=4)
    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=4)
    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=4)
    
    return train_loader, val_loader, test_loader

def train_model(model, train_loader, val_loader, device, num_epochs=250):
    """è®­ç»ƒæ¨¡å‹"""
    print("ğŸš€ å¼€å§‹è®­ç»ƒMARS+Transformeræ¨¡å‹...")
    
    # ä¼˜åŒ–å™¨å’ŒæŸå¤±å‡½æ•°
    optimizer = optim.Adam(model.parameters(), lr=0.0005, weight_decay=1e-4)
    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', patience=10, 
                                                    factor=0.5, min_lr=1e-6)
    criterion = nn.MSELoss()
    
    # æ—©åœå’Œæ¨¡å‹ä¿å­˜
    best_val_loss = float('inf')
    patience_counter = 0
    patience = 20
    
    train_losses = []
    val_losses = []
    
    for epoch in range(num_epochs):
        # è®­ç»ƒé˜¶æ®µ
        model.train()
        train_loss = 0.0
        train_mae = 0.0
        
        for batch_features, batch_labels in train_loader:
            batch_features = batch_features.to(device)
            batch_labels = batch_labels.to(device)
            
            optimizer.zero_grad()
            outputs = model(batch_features)
            loss = criterion(outputs, batch_labels)
            loss.backward()
            optimizer.step()
            
            train_loss += loss.item()
            train_mae += F.l1_loss(outputs, batch_labels).item()
        
        # éªŒè¯é˜¶æ®µ
        model.eval()
        val_loss = 0.0
        val_mae = 0.0
        
        with torch.no_grad():
            for batch_features, batch_labels in val_loader:
                batch_features = batch_features.to(device)
                batch_labels = batch_labels.to(device)
                
                outputs = model(batch_features)
                loss = criterion(outputs, batch_labels)
                
                val_loss += loss.item()
                val_mae += F.l1_loss(outputs, batch_labels).item()
        
        # è®¡ç®—å¹³å‡æŸå¤±
        train_loss /= len(train_loader)
        train_mae /= len(train_loader)
        val_loss /= len(val_loader)
        val_mae /= len(val_loader)
        
        train_losses.append(train_loss)
        val_losses.append(val_loss)
        
        print(f"Epoch {epoch+1:3d}/{num_epochs} - "
              f"Train Loss: {train_loss:.6f}, Train MAE: {train_mae:.6f} - "
              f"Val Loss: {val_loss:.6f}, Val MAE: {val_mae:.6f}")
        
        # å­¦ä¹ ç‡è°ƒåº¦
        scheduler.step(val_loss)
        
        # æ—©åœå’Œæ¨¡å‹ä¿å­˜
        if val_loss < best_val_loss:
            best_val_loss = val_loss
            patience_counter = 0
            try:
                # å…ˆä¿å­˜åˆ°ä¸´æ—¶æ–‡ä»¶ï¼Œç„¶åé‡å‘½åï¼Œé¿å…ä¿å­˜è¿‡ç¨‹ä¸­è¢«ä¸­æ–­
                torch.save(model.state_dict(), 'mars_transformer_best_tmp.pth')
                import os
                if os.path.exists('mars_transformer_best.pth'):
                    os.remove('mars_transformer_best.pth')
                os.rename('mars_transformer_best_tmp.pth', 'mars_transformer_best.pth')
                print(f"âœ“ ä¿å­˜æœ€ä½³æ¨¡å‹ (Val Loss: {val_loss:.6f})")
            except Exception as e:
                print(f"âš ï¸ æ¨¡å‹ä¿å­˜å¤±è´¥: {e}")
        else:
            patience_counter += 1
            
        if patience_counter >= patience:
            print(f"Early stopping at epoch {epoch+1}")
            break
    
    # ä¿å­˜æœ€ç»ˆæ¨¡å‹
    torch.save(model.state_dict(), 'mars_transformer_final.pth')
    print("âœ“ ä¿å­˜æœ€ç»ˆæ¨¡å‹")
    
    return train_losses, val_losses

def evaluate_model(model, test_loader, device):
    """è¯„ä¼°æ¨¡å‹"""
    print("ğŸ“Š è¯„ä¼°æ¨¡å‹æ€§èƒ½...")
    
    model.eval()
    all_predictions = []
    all_labels = []
    
    with torch.no_grad():
        for batch_features, batch_labels in test_loader:
            batch_features = batch_features.to(device)
            batch_labels = batch_labels.to(device)
            
            outputs = model(batch_features)
            
            all_predictions.append(outputs.cpu().numpy())
            all_labels.append(batch_labels.cpu().numpy())
    
    # åˆå¹¶æ‰€æœ‰ç»“æœ
    predictions = np.concatenate(all_predictions, axis=0)
    ground_truth = np.concatenate(all_labels, axis=0)
    
    # è®¡ç®—è¯„ä¼°æŒ‡æ ‡
    mae = mean_absolute_error(ground_truth, predictions)
    mse = mean_squared_error(ground_truth, predictions)
    rmse = np.sqrt(mse)
    
    print(f"æµ‹è¯•é›†æ€§èƒ½:")
    print(f"MAE:  {mae:.6f}")
    print(f"MSE:  {mse:.6f}")
    print(f"RMSE: {rmse:.6f}")
    
    # ä¿å­˜é¢„æµ‹ç»“æœ
    np.save('predictions_mars_transformer_torch.npy', predictions)
    print("âœ“ é¢„æµ‹ç»“æœå·²ä¿å­˜: predictions_mars_transformer_torch.npy")
    
    return predictions, ground_truth, mae, mse, rmse

def main():
    """ä¸»å‡½æ•°"""
    print("MARS+Transformeréª¨æ¶æå–æ¨¡å‹ (PyTorchç‰ˆæœ¬)")
    print("=" * 60)
    
    # é…ç½®è®¾å¤‡
    device = configure_gpu()
    
    # åŠ è½½æ•°æ®
    (train_features, val_features, test_features, 
     train_labels, val_labels, test_labels) = load_and_preprocess_data()
    
    # åˆ›å»ºæ•°æ®åŠ è½½å™¨
    train_loader, val_loader, test_loader = create_data_loaders(
        train_features, train_labels, val_features, val_labels, 
        test_features, test_labels, batch_size=32
    )
    
    # åˆ›å»ºæ¨¡å‹ï¼ˆå¯ç”¨å¤šå°ºåº¦ç‰¹å¾èåˆï¼‰
    print("\nğŸ”§ æ¨¡å‹é…ç½®:")
    print("  - å¤šå°ºåº¦ç‰¹å¾èåˆ: å¯ç”¨ (64 + 128 + 256 = 448ç»´)")
    print("  - Transformerå±‚æ•°: 2å±‚")
    print("  - å›å½’å¤´: æ¸è¿›å¼é™ç»´è®¾è®¡")
    model = MARSTransformerModel(input_channels=5, output_dim=57, multi_scale=True).to(device)
    
    # æ‰“å°æ¨¡å‹ä¿¡æ¯
    total_params = sum(p.numel() for p in model.parameters())
    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
    print(f"æ¨¡å‹å‚æ•°: æ€»è®¡ {total_params:,}, å¯è®­ç»ƒ {trainable_params:,}")
    
    # è®­ç»ƒæ¨¡å‹ (100è½®)
    train_losses, val_losses = train_model(model, train_loader, val_loader, device, num_epochs=250)
    
    # åŠ è½½æœ€ä½³æ¨¡å‹è¿›è¡Œè¯„ä¼°
    try:
        model.load_state_dict(torch.load('mars_transformer_best.pth', map_location=device))
        print("âœ“ æˆåŠŸåŠ è½½æœ€ä½³æ¨¡å‹")
    except Exception as e:
        print(f"âš ï¸ åŠ è½½æœ€ä½³æ¨¡å‹å¤±è´¥: {e}")
        print("ä½¿ç”¨å½“å‰è®­ç»ƒåçš„æ¨¡å‹è¿›è¡Œè¯„ä¼°")
    
    # è¯„ä¼°æ¨¡å‹
    predictions, ground_truth, mae, mse, rmse = evaluate_model(model, test_loader, device)
    
    # ç»˜åˆ¶è®­ç»ƒå†å²
    # plot_training_history(train_losses, val_losses)
    
    print("\nğŸ‰ MARS+Transformerè®­ç»ƒå®Œæˆ (PyTorchç‰ˆæœ¬)!")
    print(f"âœ“ æœ€ä½³æ¨¡å‹: mars_transformer_best.pth")
    print(f"âœ“ æœ€ç»ˆæ¨¡å‹: mars_transformer_final.pth")
    print(f"âœ“ é¢„æµ‹ç»“æœ: predictions_mars_transformer_torch.npy")
    print(f"âœ“ è®­ç»ƒå†å²: mars_transformer_training_history_torch.png")

if __name__ == "__main__":
    main()