# -*- coding: utf-8 -*-
"""
MARS+Transformeréª¨æ¶å¯è§†åŒ–è„šæœ¬ (PyTorchç‰ˆæœ¬)
ç”ŸæˆGround Truth vs é¢„æµ‹ç»“æœçš„3Déª¨æ¶å¯¹æ¯”å›¾
æ”¯æŒç›´æ¥ä½¿ç”¨PyTorchæ¨¡å‹è¿›è¡Œé¢„æµ‹å’Œå¯è§†åŒ–
"""

import numpy as np
import matplotlib
matplotlib.use('Agg')  # ä½¿ç”¨éäº¤äº’å¼åç«¯
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D
import os
import warnings
warnings.filterwarnings('ignore')

# å°è¯•å¯¼å…¥PyTorch (ç”¨äºç›´æ¥æ¨¡å‹æ¨ç†)
try:
    import torch
    import torch.nn as nn
    import torch.nn.functional as F
    HAS_TORCH = True
    print("âœ“ PyTorchå¯ç”¨ï¼Œæ”¯æŒç›´æ¥æ¨¡å‹æ¨ç†")
except ImportError:
    HAS_TORCH = False
    print("âš ï¸ PyTorchä¸å¯ç”¨ï¼Œä»…æ”¯æŒé¢„ä¿å­˜çš„é¢„æµ‹ç»“æœ")

# Microsoft Kinect 19å…³èŠ‚ç‚¹éª¨æ¶è¿æ¥å®šä¹‰ (è½¬æ¢ä¸º0-basedç´¢å¼•)
skeleton_connections = [
    (2, 3),   # head-neck
    (2, 18),  # neck-spineshoulder
    (18, 4),  # spineshoulder-leftshoulder
    (4, 5),   # leftshoulder-leftelbow
    (5, 6),   # leftelbow-leftwrist
    (18, 7),  # spineshoulder-rightshoulder
    (7, 8),   # rightshoulder-rightelbow
    (8, 9),   # rightelbow-rightwrist
    (18, 1),  # spineshoulder-spinemid
    (1, 0),   # spinemid-spinebase
    (0, 10),  # spinebase-hipleft
    (10, 11), # hipleft-kneeleft
    (11, 12), # kneeleft-ankleleft
    (12, 13), # ankleleft-footleft
    (0, 14),  # spinebase-hipright
    (14, 15), # hipright-kneeright
    (15, 16), # kneeright-ankleright
    (16, 17)  # ankleright-footright
]

# ç®€åŒ–çš„PyTorchæ¨¡å‹å®šä¹‰ï¼ˆä»…ç”¨äºæ¨ç†ï¼‰
class SEBlock(nn.Module):
    """Squeeze-and-Excitationæ³¨æ„åŠ›å—"""
    def __init__(self, channels, reduction=16):
        super(SEBlock, self).__init__()
        self.avg_pool = nn.AdaptiveAvgPool2d(1)
        self.fc1 = nn.Linear(channels, channels // reduction)
        self.fc2 = nn.Linear(channels // reduction, channels)
        self.sigmoid = nn.Sigmoid()
        
    def forward(self, x):
        b, c, _, _ = x.size()
        y = self.avg_pool(x).view(b, c)
        y = F.relu(self.fc1(y))
        y = self.sigmoid(self.fc2(y))
        y = y.view(b, c, 1, 1)
        return x * y

class ResidualSEBlock(nn.Module):
    """æ®‹å·®å—ç»“åˆSEæ³¨æ„åŠ›"""
    def __init__(self, in_channels, out_channels, kernel_size=3):
        super(ResidualSEBlock, self).__init__()
        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size, padding=1)
        self.bn1 = nn.BatchNorm2d(out_channels)
        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size, padding=1)
        self.bn2 = nn.BatchNorm2d(out_channels)
        self.se_block = SEBlock(out_channels)
        self.relu = nn.ReLU(inplace=True)
        
        if in_channels != out_channels:
            self.shortcut = nn.Sequential(
                nn.Conv2d(in_channels, out_channels, 1),
                nn.BatchNorm2d(out_channels)
            )
        else:
            self.shortcut = nn.Identity()
    
    def forward(self, x):
        identity = self.shortcut(x)
        out = self.relu(self.bn1(self.conv1(x)))
        out = self.bn2(self.conv2(out))
        out = self.se_block(out)
        out += identity
        out = self.relu(out)
        return out

class SpatialAttention(nn.Module):
    """ç©ºé—´æ³¨æ„åŠ›æ¨¡å—"""
    def __init__(self, kernel_size=7):
        super(SpatialAttention, self).__init__()
        self.conv = nn.Conv2d(2, 1, kernel_size, padding=kernel_size//2)
        self.sigmoid = nn.Sigmoid()
    
    def forward(self, x):
        avg_out = torch.mean(x, dim=1, keepdim=True)
        max_out, _ = torch.max(x, dim=1, keepdim=True)
        attention = torch.cat([avg_out, max_out], dim=1)
        attention = self.sigmoid(self.conv(attention))
        return x * attention

class PositionalEncoding(nn.Module):
    """ä½ç½®ç¼–ç æ¨¡å—"""
    def __init__(self, d_model, max_len=100):
        super(PositionalEncoding, self).__init__()
        import math
        pe = torch.zeros(max_len, d_model)
        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)
        div_term = torch.exp(torch.arange(0, d_model, 2).float() * 
                           (-math.log(10000.0) / d_model))
        pe[:, 0::2] = torch.sin(position * div_term)
        pe[:, 1::2] = torch.cos(position * div_term)
        pe = pe.unsqueeze(0).transpose(0, 1)
        self.register_buffer('pe', pe)
    
    def forward(self, x):
        seq_len = x.size(1)
        return x + self.pe[:seq_len, :].transpose(0, 1)

class LightweightTransformerBlock(nn.Module):
    """è½»é‡çº§Transformeræ³¨æ„åŠ›å—"""
    def __init__(self, d_model, num_heads=4, dff=256, dropout=0.1):
        super(LightweightTransformerBlock, self).__init__()
        self.mha = nn.MultiheadAttention(d_model, num_heads, dropout=dropout, batch_first=True)
        self.ffn = nn.Sequential(
            nn.Linear(d_model, dff),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(dff, d_model)
        )
        self.layernorm1 = nn.LayerNorm(d_model)
        self.layernorm2 = nn.LayerNorm(d_model)
        self.dropout1 = nn.Dropout(dropout)
        self.dropout2 = nn.Dropout(dropout)
    
    def forward(self, x):
        attn_output, _ = self.mha(x, x, x)
        attn_output = self.dropout1(attn_output)
        out1 = self.layernorm1(x + attn_output)
        ffn_output = self.ffn(out1)
        ffn_output = self.dropout2(ffn_output)
        out2 = self.layernorm2(out1 + ffn_output)
        return out2

class EnhancedMARSBackbone(nn.Module):
    """åˆ›å»ºå¢å¼ºçš„MARSä¸»å¹²ç½‘ç»œ"""
    def __init__(self, input_channels=5):
        super(EnhancedMARSBackbone, self).__init__()
        
        self.initial_conv1 = nn.Conv2d(input_channels, 32, 3, padding=1)
        self.initial_bn1 = nn.BatchNorm2d(32)
        self.initial_conv2 = nn.Conv2d(32, 32, 3, padding=1)
        self.initial_bn2 = nn.BatchNorm2d(32)
        
        self.res_se_1 = ResidualSEBlock(32, 64)
        self.maxpool1 = nn.MaxPool2d(2, 2)
        
        self.res_se_2 = ResidualSEBlock(64, 128)
        self.spatial_att_1 = SpatialAttention()
        
        self.res_se_3 = ResidualSEBlock(128, 256)
        self.spatial_att_2 = SpatialAttention()
        
        self.global_avg_pool = nn.AdaptiveAvgPool2d(1)
        self.relu = nn.ReLU(inplace=True)
    
    def forward(self, x):
        x = self.relu(self.initial_bn1(self.initial_conv1(x)))
        x = self.relu(self.initial_bn2(self.initial_conv2(x)))
        
        x = self.res_se_1(x)
        x = self.maxpool1(x)
        
        x = self.res_se_2(x)
        x = self.spatial_att_1(x)
        
        x = self.res_se_3(x)
        x = self.spatial_att_2(x)
        
        x = self.global_avg_pool(x)
        x = x.flatten(1)
        
        return x

class TransformerRegressionHead(nn.Module):
    """åˆ›å»ºTransformerå¢å¼ºçš„å›å½’å¤´"""
    def __init__(self, input_dim=256, output_dim=57):
        super(TransformerRegressionHead, self).__init__()
        
        self.feature_projection = nn.Sequential(
            nn.Linear(input_dim, 512),
            nn.ReLU(),
            nn.Dropout(0.3)
        )
        
        self.seq_len = 8
        self.d_model = 64
        
        self.pos_encoding = PositionalEncoding(self.d_model, max_len=self.seq_len)
        self.transformer_1 = LightweightTransformerBlock(self.d_model, num_heads=4, dff=128)
        self.transformer_2 = LightweightTransformerBlock(self.d_model, num_heads=4, dff=128)
        self.global_avg_pool = nn.AdaptiveAvgPool1d(1)
        
        self.final_layers = nn.Sequential(
            nn.Linear(self.d_model, 512),
            nn.ReLU(),
            nn.BatchNorm1d(512),
            nn.Dropout(0.4),
            nn.Linear(512, 256),
            nn.ReLU(),
            nn.BatchNorm1d(256),
            nn.Dropout(0.3),
            nn.Linear(256, output_dim)
        )
    
    def forward(self, x):
        x = self.feature_projection(x)
        batch_size = x.size(0)
        x = x.view(batch_size, self.seq_len, self.d_model)
        x = self.pos_encoding(x)
        x = self.transformer_1(x)
        x = self.transformer_2(x)
        x = x.transpose(1, 2)
        x = self.global_avg_pool(x).squeeze(-1)
        output = self.final_layers(x)
        return output

class MARSTransformerModel(nn.Module):
    """å®Œæ•´çš„MARS+Transformeréª¨æ¶æå–æ¨¡å‹"""
    def __init__(self, input_channels=5, output_dim=57):
        super(MARSTransformerModel, self).__init__()
        self.backbone = EnhancedMARSBackbone(input_channels)
        self.regression_head = TransformerRegressionHead(256, output_dim)
    
    def forward(self, x):
        features = self.backbone(x)
        output = self.regression_head(features)
        return output

def load_data():
    """åŠ è½½æ•°æ®"""
    print("åŠ è½½æµ‹è¯•æ•°æ®å’Œé¢„æµ‹ç»“æœ...")
    
    # åŠ è½½Ground Truthæ ‡ç­¾
    labels_test = np.load('/home/uo/myProject/HumanPoint-BERT/data/MARS/labels_test.npy')
    print(f"âœ“ Ground Truthæ•°æ®: {labels_test.shape}")
    
    # å°è¯•åŠ è½½PyTorché¢„æµ‹ç»“æœ
    pred_files = [
        'predictions_mars_transformer_torch.npy',  # PyTorché¢„æµ‹ç»“æœ
        'predictions_mars_transformer.npy',  # TensorFlowé¢„æµ‹ç»“æœ
        'predictions_skeleton_extraction.npy',  # å¤‡ç”¨æ–‡ä»¶
        'Pred_test_transformer_100.npy',  # å…¶ä»–å¤‡ç”¨æ–‡ä»¶
        'Pred_test_100.npy'
    ]
    
    predictions = None
    used_file = None
    
    for pred_file in pred_files:
        if os.path.exists(pred_file):
            try:
                predictions = np.load(pred_file)
                used_file = pred_file
                print(f"âœ“ é¢„æµ‹ç»“æœæ•°æ®: {predictions.shape} (æ¥æº: {pred_file})")
                break
            except Exception as e:
                print(f"âš ï¸ åŠ è½½ {pred_file} å¤±è´¥: {e}")
                continue
    
    if predictions is None:
        print(f"âŒ æœªæ‰¾åˆ°æœ‰æ•ˆçš„é¢„æµ‹æ–‡ä»¶")
        print(f"   å°è¯•çš„æ–‡ä»¶: {pred_files}")
        return None, None
    
    # éªŒè¯æ•°æ®å½¢çŠ¶åŒ¹é…
    if len(labels_test) != len(predictions):
        print(f"âš ï¸ æ•°æ®é•¿åº¦ä¸åŒ¹é…: GT({len(labels_test)}) vs Pred({len(predictions)})")
        min_len = min(len(labels_test), len(predictions))
        labels_test = labels_test[:min_len]
        predictions = predictions[:min_len]
        print(f"âœ“ å·²æˆªæ–­åˆ°ç›¸åŒé•¿åº¦: {min_len}")
    
    return labels_test, predictions

def predict_with_torch_model(model_path='mars_transformer_best.pth', feature_path='/home/uo/myProject/HumanPoint-BERTdata/MARS/featuremap_test.npy'):
    """ä½¿ç”¨PyTorchæ¨¡å‹ç›´æ¥è¿›è¡Œé¢„æµ‹"""
    if not HAS_TORCH:
        print("âŒ PyTorchä¸å¯ç”¨ï¼Œæ— æ³•è¿›è¡Œæ¨¡å‹æ¨ç†")
        return None
        
    if not os.path.exists(model_path):
        print(f"âŒ æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨: {model_path}")
        return None
        
    if not os.path.exists(feature_path):
        print(f"âŒ ç‰¹å¾æ–‡ä»¶ä¸å­˜åœ¨: {feature_path}")
        return None
    
    try:
        # é…ç½®è®¾å¤‡
        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        print(f"ğŸ”„ ä½¿ç”¨è®¾å¤‡: {device}")
        
        # åŠ è½½æ¨¡å‹
        print(f"ğŸ”„ åŠ è½½PyTorchæ¨¡å‹: {model_path}")
        model = MARSTransformerModel(input_channels=5, output_dim=57)
        model.load_state_dict(torch.load(model_path, map_location=device))
        model = model.to(device)
        model.eval()
        print("âœ“ æ¨¡å‹åŠ è½½æˆåŠŸ")
        
        # åŠ è½½ç‰¹å¾æ•°æ®
        print(f"ğŸ”„ åŠ è½½æµ‹è¯•ç‰¹å¾: {feature_path}")
        features = np.load(feature_path)
        print(f"âœ“ ç‰¹å¾æ•°æ®: {features.shape}")
        
        # è½¬æ¢æ•°æ®æ ¼å¼ï¼š(N, H, W, C) -> (N, C, H, W)
        features = np.transpose(features, (0, 3, 1, 2))
        features_tensor = torch.FloatTensor(features).to(device)
        
        print("ğŸ”„ å¼€å§‹é¢„æµ‹...")
        predictions = []
        batch_size = 32
        
        with torch.no_grad():
            for i in range(0, len(features_tensor), batch_size):
                batch = features_tensor[i:i+batch_size]
                outputs = model(batch)
                predictions.append(outputs.cpu().numpy())
        
        predictions = np.concatenate(predictions, axis=0)
        print(f"âœ“ é¢„æµ‹å®Œæˆ: {predictions.shape}")
        
        # ä¿å­˜é¢„æµ‹ç»“æœ
        output_file = 'predictions_mars_transformer_torch_live.npy'
        np.save(output_file, predictions)
        print(f"âœ“ é¢„æµ‹ç»“æœå·²ä¿å­˜: {output_file}")
        
        return predictions
        
    except Exception as e:
        print(f"âŒ æ¨¡å‹æ¨ç†å¤±è´¥: {e}")
        return None

def plot_3d_skeleton(joints, ax, title, color='blue', linewidth=2, alpha=1.0):
    """ç»˜åˆ¶3Déª¨æ¶"""
    # æ•°æ®æ ¼å¼: (x1,x2,...,x19, y1,y2,...,y19, z1,z2,...,z19)
    if joints.shape == (57,):
        # é‡æ–°ç»„ç»‡æ•°æ®ä¸º (19, 3) æ ¼å¼
        x_coords = joints[0:19]    # xåæ ‡: 0-18
        y_coords = joints[19:38]   # yåæ ‡: 19-37  
        z_coords = joints[38:57]   # zåæ ‡: 38-56
        joints = np.column_stack((x_coords, y_coords, z_coords))
    
    # ç»˜åˆ¶å…³èŠ‚ç‚¹
    ax.scatter(joints[:, 0], joints[:, 1], joints[:, 2], 
              c=color, s=80, alpha=alpha, edgecolors='black', linewidths=0.5)
    
    # ç»˜åˆ¶éª¨æ¶è¿æ¥çº¿
    for connection in skeleton_connections:
        if connection[0] < len(joints) and connection[1] < len(joints):
            joint1 = joints[connection[0]]
            joint2 = joints[connection[1]]
            ax.plot([joint1[0], joint2[0]], 
                   [joint1[1], joint2[1]], 
                   [joint1[2], joint2[2]], 
                   color=color, alpha=alpha, linewidth=linewidth)
    
    # è®¾ç½®å›¾å½¢å±æ€§
    ax.set_title(title, fontsize=14, fontweight='bold')
    ax.set_xlabel('X', fontsize=12)
    ax.set_ylabel('Y', fontsize=12)
    ax.set_zlabel('Z', fontsize=12)
    
    # æ ¹æ®å®é™…æ•°æ®èŒƒå›´è®¾ç½®åæ ‡è½´ï¼Œä¿æŒxyzæ¯”ä¾‹ä¸€è‡´
    x_min, x_max = joints[:, 0].min(), joints[:, 0].max()
    y_min, y_max = joints[:, 1].min(), joints[:, 1].max()
    z_min, z_max = joints[:, 2].min(), joints[:, 2].max()
    
    # è®¡ç®—å„è½´çš„èŒƒå›´
    x_range = x_max - x_min
    y_range = y_max - y_min
    z_range = z_max - z_min
    
    # ä½¿ç”¨æœ€å¤§èŒƒå›´ä½œä¸ºç»Ÿä¸€æ¯”ä¾‹å°º
    max_range = max(x_range, y_range, z_range)
    margin = max_range * 0.2  # ç»Ÿä¸€çš„è¾¹è·
    
    # è®¡ç®—æ¯ä¸ªè½´çš„ä¸­å¿ƒç‚¹
    x_center = (x_min + x_max) / 2
    y_center = (y_min + y_max) / 2
    z_center = (z_min + z_max) / 2
    
    # è®¾ç½®ç›¸åŒçš„èŒƒå›´ï¼Œä»¥å„è½´ä¸­å¿ƒä¸ºåŸºå‡†
    half_range = max_range / 2 + margin
    ax.set_xlim(x_center - half_range, x_center + half_range)
    ax.set_ylim(y_center - half_range, y_center + half_range)
    ax.set_zlim(z_center - half_range, z_center + half_range)
    
    # è®¾ç½®è§†è§’
    ax.view_init(elev=20, azim=45)

def create_skeleton_comparison(ground_truth, prediction, sample_idx, output_dir):
    """åˆ›å»ºå•ä¸ªæ ·æœ¬çš„éª¨æ¶å¯¹æ¯”å›¾"""
    
    # è§£ææ•°æ®æ ¼å¼: (x1...x19, y1...y19, z1...z19)
    def parse_joints(joints_data):
        x_coords = joints_data[0:19]
        y_coords = joints_data[19:38]  
        z_coords = joints_data[38:57]
        return np.column_stack((x_coords, y_coords, z_coords))
    
    # è®¡ç®—3Dè¯¯å·®
    gt_joints = parse_joints(ground_truth)
    pred_joints = parse_joints(prediction)
    
    # è®¡ç®—æ¯ä¸ªå…³èŠ‚ç‚¹çš„3Dæ¬§å‡ é‡Œå¾—è·ç¦»è¯¯å·®
    joint_errors = np.sqrt(np.sum((gt_joints - pred_joints) ** 2, axis=1))
    mean_error = np.mean(joint_errors)
    
    # åˆ›å»ºå›¾å½¢
    fig = plt.figure(figsize=(16, 8))
    fig.suptitle(f'Sample {sample_idx+1:02d} - Skeleton Comparison (PyTorch) (Mean 3D Error: {mean_error:.4f})', 
                fontsize=16, fontweight='bold')
    
    # Ground Truthéª¨æ¶
    ax1 = fig.add_subplot(121, projection='3d')
    plot_3d_skeleton(ground_truth, ax1, 'Ground Truth', color='blue', linewidth=3)
    
    # é¢„æµ‹éª¨æ¶
    ax2 = fig.add_subplot(122, projection='3d')
    plot_3d_skeleton(prediction, ax2, 'PyTorch Prediction', color='red', linewidth=3)
    
    # æ·»åŠ è¯¯å·®ä¿¡æ¯
    error_text = f"3D Error Statistics (PyTorch):\n"
    error_text += f"Mean: {mean_error:.4f}\n"
    error_text += f"Max: {np.max(joint_errors):.4f}\n"
    error_text += f"Min: {np.min(joint_errors):.4f}\n"
    error_text += f"Std: {np.std(joint_errors):.4f}"
    
    fig.text(0.02, 0.02, error_text, fontsize=10, 
             bbox=dict(boxstyle="round,pad=0.5", facecolor="lightgray", alpha=0.8))
    
    # ä¿å­˜å›¾ç‰‡
    filename = os.path.join(output_dir, f'skeleton_torch_sample_{sample_idx+1:02d}.png')
    plt.savefig(filename, dpi=300, bbox_inches='tight', facecolor='white')
    plt.close()
    
    print(f"âœ“ å·²ç”ŸæˆPyTorchç‰ˆæœ¬: {filename} (3D Error: {mean_error:.4f})")
    
    return mean_error

def create_overlay_comparison(ground_truth, prediction, sample_idx, output_dir):
    """åˆ›å»ºé‡å å¯¹æ¯”å›¾"""
    
    # è§£ææ•°æ®æ ¼å¼
    def parse_joints(joints_data):
        x_coords = joints_data[0:19]
        y_coords = joints_data[19:38]  
        z_coords = joints_data[38:57]
        return np.column_stack((x_coords, y_coords, z_coords))
    
    # è®¡ç®—è¯¯å·®
    gt_joints = parse_joints(ground_truth)
    pred_joints = parse_joints(prediction)
    joint_errors = np.sqrt(np.sum((gt_joints - pred_joints) ** 2, axis=1))
    mean_error = np.mean(joint_errors)
    
    # åˆ›å»ºé‡å å›¾
    fig = plt.figure(figsize=(12, 10))
    fig.suptitle(f'Sample {sample_idx+1:02d} - PyTorch Overlay Comparison', fontsize=16, fontweight='bold')
    
    ax = fig.add_subplot(111, projection='3d')
    
    # ç»˜åˆ¶Ground Truth (è“è‰²)
    plot_3d_skeleton(ground_truth, ax, '', color='blue', linewidth=3, alpha=0.8)
    
    # ç»˜åˆ¶é¢„æµ‹ç»“æœ (çº¢è‰²ï¼Œé€æ˜)
    plot_3d_skeleton(prediction, ax, '', color='red', linewidth=2, alpha=0.6)
    
    # è®¾ç½®æ ‡é¢˜å’Œå›¾ä¾‹
    ax.set_title(f'Ground Truth (Blue) vs PyTorch Prediction (Red)\nMean 3D Error: {mean_error:.4f}', 
                fontsize=14, fontweight='bold')
    
    # æ·»åŠ å›¾ä¾‹ (ä½¿ç”¨æ–‡æœ¬æ ‡æ³¨ä»£æ›¿)
    ax.text2D(0.02, 0.95, "â— Ground Truth (Blue)", transform=ax.transAxes, 
              color='blue', fontsize=12, fontweight='bold')
    ax.text2D(0.02, 0.90, "â— PyTorch Prediction (Red)", transform=ax.transAxes, 
              color='red', fontsize=12, fontweight='bold')
    
    # ä¿å­˜å›¾ç‰‡
    filename = os.path.join(output_dir, f'overlay_torch_sample_{sample_idx+1:02d}.png')
    plt.savefig(filename, dpi=300, bbox_inches='tight', facecolor='white')
    plt.close()
    
    print(f"âœ“ å·²ç”ŸæˆPyTorché‡å å›¾: {filename}")
    
    return mean_error

def main(use_live_prediction=True):
    """ä¸»å‡½æ•°"""
    print("MARS+Transformeréª¨æ¶å¯è§†åŒ–å·¥å…· (PyTorchç‰ˆæœ¬)")
    print("=" * 60)
    
    # åˆ›å»ºè¾“å‡ºç›®å½• (ä½¿ç”¨torchç‰¹å®šåç§°)
    output_dir = 'visualizations/skeleton_extractor'
    if not os.path.exists(output_dir):
        os.makedirs(output_dir)
        print(f"âœ“ å·²åˆ›å»ºè¾“å‡ºç›®å½•: {output_dir}")
    
    # åŠ è½½Ground Truth  /home/uo/myProject/HumanPoint-BERT/data/MARS/labels_test.npy
    ground_truth = np.load('/home/uo/myProject/HumanPoint-BERT/data/MARS/labels_test.npy')
    print(f"âœ“ Ground Truthæ•°æ®: {ground_truth.shape}")
    
    predictions = None
    
    # å°è¯•ä½¿ç”¨å®æ—¶PyTorchæ¨¡å‹é¢„æµ‹
    if use_live_prediction and HAS_TORCH:
        print("\nğŸš€ å°è¯•ä½¿ç”¨mars_transformer_best.pthè¿›è¡Œå®æ—¶é¢„æµ‹...")
        predictions = predict_with_torch_model('mars_transformer_best.pth')
        
        if predictions is not None:
            print("âœ… ä½¿ç”¨PyTorchå®æ—¶æ¨¡å‹é¢„æµ‹æˆåŠŸ!")
        else:
            print("âš ï¸ PyTorchå®æ—¶é¢„æµ‹å¤±è´¥ï¼Œå°è¯•åŠ è½½é¢„ä¿å­˜çš„ç»“æœ...")
    
    # å¦‚æœå®æ—¶é¢„æµ‹å¤±è´¥ï¼Œä½¿ç”¨é¢„ä¿å­˜çš„ç»“æœ
    if predictions is None:
        print("\nğŸ“ åŠ è½½é¢„ä¿å­˜çš„é¢„æµ‹ç»“æœ...")
        ground_truth, predictions = load_data()
        if ground_truth is None or predictions is None:
            print("âŒ æ•°æ®åŠ è½½å¤±è´¥ï¼Œé€€å‡ºç¨‹åº")
            return
    
    # ç”Ÿæˆå‰10ä¸ªæ ·æœ¬çš„å¯è§†åŒ–
    print(f"\nå¼€å§‹ç”Ÿæˆå‰10ä¸ªæ ·æœ¬çš„éª¨æ¶å¯è§†åŒ– (PyTorchç‰ˆæœ¬)...")
    print("-" * 50)
    
    total_error = 0
    sample_errors = []
    
    for i in range(min(10, len(ground_truth))):
        # ç”Ÿæˆå¯¹æ¯”å›¾
        error = create_skeleton_comparison(ground_truth[i], predictions[i], i, output_dir)
        
        # ç”Ÿæˆé‡å å›¾
        create_overlay_comparison(ground_truth[i], predictions[i], i, output_dir)
        
        sample_errors.append(error)
        total_error += error
    
    # ç”Ÿæˆæ±‡æ€»ä¿¡æ¯
    print("-" * 50)
    print(f"âœ… PyTorchå¯è§†åŒ–å®Œæˆ!")
    print(f"ç”Ÿæˆæ–‡ä»¶æ•°é‡: {len(os.listdir(output_dir))} å¼ å›¾ç‰‡")
    print(f"å¹³å‡3Dè¯¯å·®: {total_error/10:.4f}")
    print(f"æœ€ä½³æ ·æœ¬: Sample {np.argmin(sample_errors)+1:02d} (è¯¯å·®: {min(sample_errors):.4f})")
    print(f"æœ€å·®æ ·æœ¬: Sample {np.argmax(sample_errors)+1:02d} (è¯¯å·®: {max(sample_errors):.4f})")
    
    # åˆ›å»ºè¯¯å·®æ€»ç»“æ–‡ä»¶
    summary_file = os.path.join(output_dir, 'error_summary_torch.txt')
    with open(summary_file, 'w', encoding='utf-8') as f:
        f.write("MARS+Transformeréª¨æ¶é¢„æµ‹è¯¯å·®æ€»ç»“ (PyTorchç‰ˆæœ¬)\n")
        f.write("=" * 50 + "\n\n")
        f.write(f"æ¨¡å‹: MARS+Transformer PyTorch (mars_transformer_best.pth)\n")
        f.write(f"æµ‹è¯•æ—¶é—´: {np.datetime64('now')}\n\n")
        
        for i, error in enumerate(sample_errors):
            f.write(f"Sample {i+1:02d}: {error:.6f}\n")
        
        f.write(f"\nç»Ÿè®¡ä¿¡æ¯:\n")
        f.write(f"å¹³å‡è¯¯å·®: {total_error/10:.6f}\n")
        f.write(f"æ ‡å‡†å·®: {np.std(sample_errors):.6f}\n")
        f.write(f"æœ€å°è¯¯å·®: {min(sample_errors):.6f}\n")
        f.write(f"æœ€å¤§è¯¯å·®: {max(sample_errors):.6f}\n")
        
        # æ·»åŠ æ€§èƒ½å¯¹æ¯”ä¿¡æ¯
        f.write(f"\næ¨¡å‹ä¿¡æ¯:\n")
        f.write(f"æ¡†æ¶: PyTorch\n")
        f.write(f"æ¶æ„: MARS CNN + Transformer Attention\n")
        f.write(f"è¾“å…¥æ ¼å¼: (N, 5, 8, 8) - PyTorchæ ¼å¼\n")
        f.write(f"è¾“å‡ºç»´åº¦: 57 (19ä¸ªå…³èŠ‚ç‚¹çš„3Dåæ ‡)\n")
    
    print(f"âœ“ PyTorchè¯¯å·®æ€»ç»“å·²ä¿å­˜: {summary_file}")
    print(f"\nğŸ“ æ‰€æœ‰æ–‡ä»¶å·²ä¿å­˜åˆ°: {output_dir}/")
    print(f"ğŸš€ MARS+Transformer PyTorchå¯è§†åŒ–å®Œæˆ!")

if __name__ == "__main__":
    import sys
    
    # æ”¯æŒå‘½ä»¤è¡Œå‚æ•°
    use_live = True
    if len(sys.argv) > 1:
        if sys.argv[1].lower() in ['--no-live', '-n']:
            use_live = False
            print("ğŸ“ å¼ºåˆ¶ä½¿ç”¨é¢„ä¿å­˜çš„é¢„æµ‹ç»“æœ")
        elif sys.argv[1].lower() in ['--help', '-h']:
            print("MARS+Transformer PyTorchå¯è§†åŒ–å·¥å…·")
            print("ç”¨æ³•:")
            print("  python skeleton_visualization_torch.py           # ä¼˜å…ˆä½¿ç”¨å®æ—¶æ¨¡å‹é¢„æµ‹")
            print("  python skeleton_visualization_torch.py --no-live # åªä½¿ç”¨é¢„ä¿å­˜ç»“æœ")
            print("  python skeleton_visualization_torch.py --help    # æ˜¾ç¤ºå¸®åŠ©")
            exit(0)
    
    main(use_live_prediction=use_live)