# -*- coding: utf-8 -*-
"""
MARS+Transformeréª¨æ¶GIFåŠ¨ç”»å¯è§†åŒ–è„šæœ¬ (PyTorchç‰ˆæœ¬)
ç”Ÿæˆå¤šä¸ªæ ·æœ¬ç›¸é‚»å‡ å¸§çš„3Déª¨æ¶åŠ¨ç”»GIF
æ”¯æŒGround Truth vs é¢„æµ‹ç»“æœçš„åŠ¨æ€å¯¹æ¯”
"""

import numpy as np
import matplotlib
matplotlib.use('Agg')  # ä½¿ç”¨éäº¤äº’å¼åç«¯
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D
import matplotlib.animation as animation
import os
import warnings
warnings.filterwarnings('ignore')

# å°è¯•å¯¼å…¥PyTorch (ç”¨äºç›´æ¥æ¨¡å‹æ¨ç†)
try:
    import torch
    import torch.nn as nn
    import torch.nn.functional as F
    HAS_TORCH = True
    print("âœ“ PyTorchå¯ç”¨ï¼Œæ”¯æŒç›´æ¥æ¨¡å‹æ¨ç†")
except ImportError:
    HAS_TORCH = False
    print("âš ï¸ PyTorchä¸å¯ç”¨ï¼Œä»…æ”¯æŒé¢„ä¿å­˜çš„é¢„æµ‹ç»“æœ")

# Microsoft Kinect 19å…³èŠ‚ç‚¹éª¨æ¶è¿æ¥å®šä¹‰ (è½¬æ¢ä¸º0-basedç´¢å¼•)
skeleton_connections = [
    (2, 3),   # head-neck
    (2, 18),  # neck-spineshoulder
    (18, 4),  # spineshoulder-leftshoulder
    (4, 5),   # leftshoulder-leftelbow
    (5, 6),   # leftelbow-leftwrist
    (18, 7),  # spineshoulder-rightshoulder
    (7, 8),   # rightshoulder-rightelbow
    (8, 9),   # rightelbow-rightwrist
    (18, 1),  # spineshoulder-spinemid
    (1, 0),   # spinemid-spinebase
    (0, 10),  # spinebase-hipleft
    (10, 11), # hipleft-kneeleft
    (11, 12), # kneeleft-ankleleft
    (12, 13), # ankleleft-footleft
    (0, 14),  # spinebase-hipright
    (14, 15), # hipright-kneeright
    (15, 16), # kneeright-ankleright
    (16, 17)  # ankleright-footright
]

# PyTorchæ¨¡å‹å®šä¹‰ï¼ˆç®€åŒ–ç‰ˆï¼Œä»…ç”¨äºæ¨ç†ï¼‰
class SEBlock(nn.Module):
    """Squeeze-and-Excitationæ³¨æ„åŠ›å—"""
    def __init__(self, channels, reduction=16):
        super(SEBlock, self).__init__()
        self.avg_pool = nn.AdaptiveAvgPool2d(1)
        self.fc1 = nn.Linear(channels, channels // reduction)
        self.fc2 = nn.Linear(channels // reduction, channels)
        self.sigmoid = nn.Sigmoid()
        
    def forward(self, x):
        b, c, _, _ = x.size()
        y = self.avg_pool(x).view(b, c)
        y = F.relu(self.fc1(y))
        y = self.sigmoid(self.fc2(y))
        y = y.view(b, c, 1, 1)
        return x * y

class ResidualSEBlock(nn.Module):
    """æ®‹å·®å—ç»“åˆSEæ³¨æ„åŠ›"""
    def __init__(self, in_channels, out_channels, kernel_size=3):
        super(ResidualSEBlock, self).__init__()
        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size, padding=1)
        self.bn1 = nn.BatchNorm2d(out_channels)
        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size, padding=1)
        self.bn2 = nn.BatchNorm2d(out_channels)
        self.se_block = SEBlock(out_channels)
        self.relu = nn.ReLU(inplace=True)
        
        if in_channels != out_channels:
            self.shortcut = nn.Sequential(
                nn.Conv2d(in_channels, out_channels, 1),
                nn.BatchNorm2d(out_channels)
            )
        else:
            self.shortcut = nn.Identity()
    
    def forward(self, x):
        identity = self.shortcut(x)
        out = self.relu(self.bn1(self.conv1(x)))
        out = self.bn2(self.conv2(out))
        out = self.se_block(out)
        out += identity
        out = self.relu(out)
        return out

class SpatialAttention(nn.Module):
    """ç©ºé—´æ³¨æ„åŠ›æ¨¡å—"""
    def __init__(self, kernel_size=7):
        super(SpatialAttention, self).__init__()
        self.conv = nn.Conv2d(2, 1, kernel_size, padding=kernel_size//2)
        self.sigmoid = nn.Sigmoid()
    
    def forward(self, x):
        avg_out = torch.mean(x, dim=1, keepdim=True)
        max_out, _ = torch.max(x, dim=1, keepdim=True)
        attention = torch.cat([avg_out, max_out], dim=1)
        attention = self.sigmoid(self.conv(attention))
        return x * attention

class PositionalEncoding(nn.Module):
    """ä½ç½®ç¼–ç æ¨¡å—"""
    def __init__(self, d_model, max_len=100):
        super(PositionalEncoding, self).__init__()
        import math
        pe = torch.zeros(max_len, d_model)
        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)
        div_term = torch.exp(torch.arange(0, d_model, 2).float() * 
                           (-math.log(10000.0) / d_model))
        pe[:, 0::2] = torch.sin(position * div_term)
        pe[:, 1::2] = torch.cos(position * div_term)
        pe = pe.unsqueeze(0).transpose(0, 1)
        self.register_buffer('pe', pe)
    
    def forward(self, x):
        seq_len = x.size(1)
        return x + self.pe[:seq_len, :].transpose(0, 1)

class LightweightTransformerBlock(nn.Module):
    """è½»é‡çº§Transformeræ³¨æ„åŠ›å—"""
    def __init__(self, d_model, num_heads=4, dff=256, dropout=0.1):
        super(LightweightTransformerBlock, self).__init__()
        self.mha = nn.MultiheadAttention(d_model, num_heads, dropout=dropout, batch_first=True)
        self.ffn = nn.Sequential(
            nn.Linear(d_model, dff),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(dff, d_model)
        )
        self.layernorm1 = nn.LayerNorm(d_model)
        self.layernorm2 = nn.LayerNorm(d_model)
        self.dropout1 = nn.Dropout(dropout)
        self.dropout2 = nn.Dropout(dropout)
    
    def forward(self, x):
        attn_output, _ = self.mha(x, x, x)
        attn_output = self.dropout1(attn_output)
        out1 = self.layernorm1(x + attn_output)
        ffn_output = self.ffn(out1)
        ffn_output = self.dropout2(ffn_output)
        out2 = self.layernorm2(out1 + ffn_output)
        return out2

class EnhancedMARSBackbone(nn.Module):
    """åˆ›å»ºå¢å¼ºçš„MARSä¸»å¹²ç½‘ç»œ - æ”¯æŒå¤šå°ºåº¦ç‰¹å¾èåˆ"""
    def __init__(self, input_channels=5, multi_scale=True):
        super(EnhancedMARSBackbone, self).__init__()
        
        self.multi_scale = multi_scale
        
        self.initial_conv1 = nn.Conv2d(input_channels, 32, 3, padding=1)
        self.initial_bn1 = nn.BatchNorm2d(32)
        self.initial_conv2 = nn.Conv2d(32, 32, 3, padding=1)
        self.initial_bn2 = nn.BatchNorm2d(32)
        
        self.res_se_1 = ResidualSEBlock(32, 64)
        self.maxpool1 = nn.MaxPool2d(2, 2)
        
        self.res_se_2 = ResidualSEBlock(64, 128)
        self.spatial_att_1 = SpatialAttention()
        
        self.res_se_3 = ResidualSEBlock(128, 256)
        self.spatial_att_2 = SpatialAttention()
        
        self.global_avg_pool = nn.AdaptiveAvgPool2d(1)
        self.relu = nn.ReLU(inplace=True)
        
        # è¾“å‡ºç‰¹å¾ç»´åº¦: 64 + 128 + 256 = 448 (å¤šå°ºåº¦) æˆ– 256 (å•å°ºåº¦)
        self.output_dim = 448 if multi_scale else 256
    
    def forward(self, x):
        x = self.relu(self.initial_bn1(self.initial_conv1(x)))
        x = self.relu(self.initial_bn2(self.initial_conv2(x)))
        
        # ç¬¬ä¸€ä¸ªæ®‹å·®SEå— - ä¿å­˜ç‰¹å¾1
        feat1 = self.res_se_1(x)
        x = self.maxpool1(feat1)
        
        # ç¬¬äºŒä¸ªæ®‹å·®SEå— + ç©ºé—´æ³¨æ„åŠ› - ä¿å­˜ç‰¹å¾2
        feat2 = self.res_se_2(x)
        feat2 = self.spatial_att_1(feat2)
        
        # ç¬¬ä¸‰ä¸ªæ®‹å·®SEå— + ç©ºé—´æ³¨æ„åŠ› - ä¿å­˜ç‰¹å¾3
        feat3 = self.res_se_3(feat2)
        feat3 = self.spatial_att_2(feat3)
        
        if self.multi_scale:
            # å¤šå°ºåº¦ç‰¹å¾èåˆ
            feat1_pool = self.global_avg_pool(feat1).flatten(1)  # (batch, 64)
            feat2_pool = self.global_avg_pool(feat2).flatten(1)  # (batch, 128)
            feat3_pool = self.global_avg_pool(feat3).flatten(1)  # (batch, 256)
            
            # æ‹¼æ¥å¤šå°ºåº¦ç‰¹å¾: 64 + 128 + 256 = 448
            output = torch.cat([feat1_pool, feat2_pool, feat3_pool], dim=1)
        else:
            # å•å°ºåº¦ç‰¹å¾ï¼ˆä»…ä½¿ç”¨æœ€æ·±å±‚ç‰¹å¾ï¼‰
            output = self.global_avg_pool(feat3).flatten(1)  # (batch, 256)
        
        return output

class TransformerRegressionHead(nn.Module):
    """åˆ›å»ºTransformerå¢å¼ºçš„å›å½’å¤´ - ç®€æ´é«˜æ•ˆç‰ˆæœ¬ï¼ˆä¸models/skeleton_extractor.pyä¿æŒä¸€è‡´ï¼‰"""
    def __init__(self, input_dim=256, output_dim=57):
        super(TransformerRegressionHead, self).__init__()
        
        # ç®€æ´çš„ç‰¹å¾æŠ•å½±ï¼ˆå•å±‚ï¼Œè¶³å¤Ÿé«˜æ•ˆï¼‰
        self.feature_projection = nn.Sequential(
            nn.Linear(input_dim, 512),
            nn.ReLU(),
            nn.Dropout(0.3)
        )
        
        # é‡å¡‘ä¸ºåºåˆ—æ ¼å¼å‚æ•°
        self.seq_len = 8
        self.d_model = 64
        
        # ä½ç½®ç¼–ç 
        self.pos_encoding = PositionalEncoding(self.d_model, max_len=self.seq_len)
        
        # ä¿æŒåŸå…ˆçš„2å±‚Transformerï¼ˆç®€æ´é«˜æ•ˆï¼‰
        self.transformer_1 = LightweightTransformerBlock(self.d_model, num_heads=4, dff=128)
        self.transformer_2 = LightweightTransformerBlock(self.d_model, num_heads=4, dff=128)
        
        # å•ä¸€å¹³å‡æ± åŒ–ï¼ˆç®€æ´æœ‰æ•ˆï¼‰
        self.global_avg_pool = nn.AdaptiveAvgPool1d(1)
        
        # ä¼˜åŒ–çš„å›å½’å¤´ - æ¸è¿›å¼é™ç»´è®¾è®¡
        self.regression_head = nn.Sequential(
            nn.Linear(self.d_model, 512),
            nn.ReLU(),
            nn.BatchNorm1d(512),
            nn.Dropout(0.4),
            
            nn.Linear(512, 256),
            nn.ReLU(),
            nn.BatchNorm1d(256),
            nn.Dropout(0.3),
            
            nn.Linear(256, 128),
            nn.ReLU(),
            nn.BatchNorm1d(128),
            nn.Dropout(0.2),
            
            nn.Linear(128, output_dim)
        )
        
        # åˆå§‹åŒ–æƒé‡
        self._init_weights()
    
    def _init_weights(self):
        """æ”¹è¿›çš„æƒé‡åˆå§‹åŒ–"""
        for m in self.modules():
            if isinstance(m, nn.Linear):
                nn.init.xavier_uniform_(m.weight)
                if m.bias is not None:
                    nn.init.constant_(m.bias, 0)
            elif isinstance(m, nn.BatchNorm1d):
                nn.init.constant_(m.weight, 1)
                nn.init.constant_(m.bias, 0)
    
    def forward(self, x):
        # ç‰¹å¾æŠ•å½±
        x = self.feature_projection(x)  # (batch, 512)
        
        # é‡å¡‘ä¸ºåºåˆ—æ ¼å¼
        batch_size = x.size(0)
        x = x.view(batch_size, self.seq_len, self.d_model)  # (batch, 8, 64)
        
        # ä½ç½®ç¼–ç 
        x = self.pos_encoding(x)
        
        # 2å±‚Transformerå¤„ç†ï¼ˆä¿æŒç®€æ´ï¼‰
        x = self.transformer_1(x)
        x = self.transformer_2(x)
        
        # å•ä¸€å¹³å‡æ± åŒ–
        x = x.transpose(1, 2)  # (batch, d_model, seq_len)
        x = self.global_avg_pool(x).squeeze(-1)  # (batch, d_model)
        
        # ä¼˜åŒ–çš„æ¸è¿›å¼å›å½’å¤´
        output = self.regression_head(x)
        
        return output

class MARSTransformerModel(nn.Module):
    """å®Œæ•´çš„MARS+Transformeréª¨æ¶æå–æ¨¡å‹ - æ”¯æŒå¤šå°ºåº¦ç‰¹å¾èåˆ"""
    def __init__(self, input_channels=5, output_dim=57, multi_scale=True):
        super(MARSTransformerModel, self).__init__()
        
        # MARS CNNä¸»å¹²ï¼ˆæ”¯æŒå¤šå°ºåº¦ç‰¹å¾èåˆï¼‰
        self.backbone = EnhancedMARSBackbone(input_channels, multi_scale=multi_scale)
        
        # æ ¹æ®æ˜¯å¦ä½¿ç”¨å¤šå°ºåº¦ç¡®å®šè¾“å…¥ç»´åº¦
        input_dim = self.backbone.output_dim  # 448 (å¤šå°ºåº¦) æˆ– 256 (å•å°ºåº¦)
        
        # Transformerå›å½’å¤´ï¼ˆè‡ªåŠ¨é€‚é…è¾“å…¥ç»´åº¦ï¼‰
        self.regression_head = TransformerRegressionHead(input_dim, output_dim)
    
    def forward(self, x):
        # CNNç‰¹å¾æå–ï¼ˆå¤šå°ºåº¦æˆ–å•å°ºåº¦ï¼‰
        features = self.backbone(x)
        
        # Transformerå›å½’
        output = self.regression_head(features)
        
        return output

def load_data():
    """åŠ è½½æ•°æ®"""
    print("åŠ è½½æµ‹è¯•æ•°æ®å’Œé¢„æµ‹ç»“æœ...")
    
    # åŠ è½½Ground Truthæ ‡ç­¾
    labels_test = np.load('/home/uo/myProject/HumanPoint-BERT/data/MARS/labels_test.npy')
    print(f"âœ“ Ground Truthæ•°æ®: {labels_test.shape}")
    
    # å°è¯•åŠ è½½PyTorché¢„æµ‹ç»“æœ
    pred_files = [
        'predictions_mars_transformer_torch.npy',  # PyTorché¢„æµ‹ç»“æœ
        'predictions_mars_transformer_torch_live.npy',  # PyTorchå®æ—¶é¢„æµ‹ç»“æœ
        'predictions_mars_transformer.npy',  # TensorFlowé¢„æµ‹ç»“æœ
        'predictions_mars_transformer_live.npy',  # TensorFlowå®æ—¶é¢„æµ‹ç»“æœ
        'predictions_skeleton_extraction.npy',  # å¤‡ç”¨æ–‡ä»¶
        'Pred_test_transformer_100.npy',  # å…¶ä»–å¤‡ç”¨æ–‡ä»¶
        'Pred_test_100.npy'
    ]
    
    predictions = None
    used_file = None
    
    for pred_file in pred_files:
        if os.path.exists(pred_file):
            try:
                predictions = np.load(pred_file)
                used_file = pred_file
                print(f"âœ“ é¢„æµ‹ç»“æœæ•°æ®: {predictions.shape} (æ¥æº: {pred_file})")
                break
            except Exception as e:
                print(f"âš ï¸ åŠ è½½ {pred_file} å¤±è´¥: {e}")
                continue
    
    if predictions is None:
        print(f"âŒ æœªæ‰¾åˆ°æœ‰æ•ˆçš„é¢„æµ‹æ–‡ä»¶")
        print(f"   å°è¯•çš„æ–‡ä»¶: {pred_files}")
        return None, None
    
    # éªŒè¯æ•°æ®å½¢çŠ¶åŒ¹é…
    if len(labels_test) != len(predictions):
        print(f"âš ï¸ æ•°æ®é•¿åº¦ä¸åŒ¹é…: GT({len(labels_test)}) vs Pred({len(predictions)})")
        min_len = min(len(labels_test), len(predictions))
        labels_test = labels_test[:min_len]
        predictions = predictions[:min_len]
        print(f"âœ“ å·²æˆªæ–­åˆ°ç›¸åŒé•¿åº¦: {min_len}")
    
    return labels_test, predictions

def predict_with_torch_model(model_path='mars_transformer_best.pth', feature_path='/home/uo/myProject/HumanPoint-BERT/data/MARS/featuremap_test.npy'):
    """ä½¿ç”¨PyTorchæ¨¡å‹ç›´æ¥è¿›è¡Œé¢„æµ‹"""
    if not HAS_TORCH:
        print("âŒ PyTorchä¸å¯ç”¨ï¼Œæ— æ³•è¿›è¡Œæ¨¡å‹æ¨ç†")
        return None
        
    if not os.path.exists(model_path):
        print(f"âŒ æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨: {model_path}")
        return None
        
    if not os.path.exists(feature_path):
        print(f"âŒ ç‰¹å¾æ–‡ä»¶ä¸å­˜åœ¨: {feature_path}")
        return None
    
    try:
        # é…ç½®è®¾å¤‡
        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        print(f"ğŸ”„ ä½¿ç”¨è®¾å¤‡: {device}")
        
        # åŠ è½½æ¨¡å‹ï¼ˆé»˜è®¤å¯ç”¨å¤šå°ºåº¦ç‰¹å¾èåˆï¼‰
        print(f"ğŸ”„ åŠ è½½PyTorchæ¨¡å‹: {model_path}")
        print("   é…ç½®: å¤šå°ºåº¦ç‰¹å¾èåˆ (64+128+256=448ç»´)")
        model = MARSTransformerModel(input_channels=5, output_dim=57, multi_scale=True)
        model.load_state_dict(torch.load(model_path, map_location=device))
        model = model.to(device)
        model.eval()
        print("âœ“ æ¨¡å‹åŠ è½½æˆåŠŸ")
        
        # åŠ è½½ç‰¹å¾æ•°æ®
        print(f"ğŸ”„ åŠ è½½æµ‹è¯•ç‰¹å¾: {feature_path}")
        features = np.load(feature_path)
        print(f"âœ“ ç‰¹å¾æ•°æ®: {features.shape}")
        
        # è½¬æ¢æ•°æ®æ ¼å¼ï¼š(N, H, W, C) -> (N, C, H, W)
        features = np.transpose(features, (0, 3, 1, 2))
        features_tensor = torch.FloatTensor(features).to(device)
        
        print("ğŸ”„ å¼€å§‹é¢„æµ‹...")
        predictions = []
        batch_size = 32
        
        with torch.no_grad():
            for i in range(0, len(features_tensor), batch_size):
                batch = features_tensor[i:i+batch_size]
                outputs = model(batch)
                predictions.append(outputs.cpu().numpy())
                
                # æ˜¾ç¤ºè¿›åº¦
                if (i // batch_size + 1) % 50 == 0:
                    print(f"  å¤„ç†æ‰¹æ¬¡: {i//batch_size + 1}/{(len(features_tensor) + batch_size - 1)//batch_size}")
        
        predictions = np.concatenate(predictions, axis=0)
        print(f"âœ“ é¢„æµ‹å®Œæˆ: {predictions.shape}")
        
        # ä¿å­˜é¢„æµ‹ç»“æœ
        output_file = 'predictions_mars_transformer_torch_gif_live.npy'
        np.save(output_file, predictions)
        print(f"âœ“ é¢„æµ‹ç»“æœå·²ä¿å­˜: {output_file}")
        
        return predictions
        
    except Exception as e:
        print(f"âŒ æ¨¡å‹æ¨ç†å¤±è´¥: {e}")
        return None

def parse_joints(joints_data):
    """è§£æå…³èŠ‚æ•°æ®æ ¼å¼: (x1...x19, y1...y19, z1...z19)"""
    if joints_data.shape == (57,):
        x_coords = joints_data[0:19]
        y_coords = joints_data[19:38]  
        z_coords = joints_data[38:57]
        return np.column_stack((x_coords, y_coords, z_coords))
    else:
        raise ValueError(f"æ— æ•ˆçš„å…³èŠ‚æ•°æ®å½¢çŠ¶: {joints_data.shape}")

def plot_skeleton_frame(joints, ax, color='blue', linewidth=2, alpha=1.0, marker_size=80):
    """ç»˜åˆ¶å•å¸§éª¨æ¶"""
    ax.clear()
    
    # ç»˜åˆ¶å…³èŠ‚ç‚¹
    ax.scatter(joints[:, 0], joints[:, 1], joints[:, 2], 
              c=color, s=marker_size, alpha=alpha, edgecolors='black', linewidths=0.5)
    
    # ç»˜åˆ¶éª¨æ¶è¿æ¥çº¿
    for connection in skeleton_connections:
        if connection[0] < len(joints) and connection[1] < len(joints):
            joint1 = joints[connection[0]]
            joint2 = joints[connection[1]]
            ax.plot([joint1[0], joint2[0]], 
                   [joint1[1], joint2[1]], 
                   [joint1[2], joint2[2]], 
                   color=color, alpha=alpha, linewidth=linewidth)
    
    # è®¾ç½®å›ºå®šçš„åæ ‡è½´èŒƒå›´ï¼ˆåŸºäºæ‰€æœ‰å¸§çš„æ•°æ®èŒƒå›´ï¼‰
    return ax

def get_data_bounds(ground_truth_frames, prediction_frames):
    """è®¡ç®—æ‰€æœ‰å¸§çš„æ•°æ®è¾¹ç•Œ"""
    all_joints = []
    
    for gt, pred in zip(ground_truth_frames, prediction_frames):
        all_joints.append(parse_joints(gt))
        all_joints.append(parse_joints(pred))
    
    all_joints = np.vstack(all_joints)
    
    x_min, x_max = all_joints[:, 0].min(), all_joints[:, 0].max()
    y_min, y_max = all_joints[:, 1].min(), all_joints[:, 1].max()
    z_min, z_max = all_joints[:, 2].min(), all_joints[:, 2].max()
    
    # è®¡ç®—ç»Ÿä¸€çš„èŒƒå›´
    x_range = x_max - x_min
    y_range = y_max - y_min
    z_range = z_max - z_min
    max_range = max(x_range, y_range, z_range)
    
    # æ·»åŠ è¾¹è·
    margin = max_range * 0.2
    
    # è®¡ç®—ä¸­å¿ƒç‚¹
    x_center = (x_min + x_max) / 2
    y_center = (y_min + y_max) / 2
    z_center = (z_min + z_max) / 2
    
    # è¿”å›ç»Ÿä¸€çš„è¾¹ç•Œ
    half_range = max_range / 2 + margin
    bounds = {
        'xlim': (x_center - half_range, x_center + half_range),
        'ylim': (y_center - half_range, y_center + half_range),
        'zlim': (z_center - half_range, z_center + half_range)
    }
    
    return bounds

def create_comparison_gif(ground_truth_frames, prediction_frames, sample_indices, output_path, 
                         fps=2, duration_per_frame=0.8):
    """åˆ›å»ºå¯¹æ¯”GIFåŠ¨ç”»"""
    
    print(f"ğŸ¬ å¼€å§‹åˆ›å»ºPyTorchå¯¹æ¯”GIF: {len(ground_truth_frames)} å¸§")
    
    # è®¡ç®—æ•°æ®è¾¹ç•Œ
    bounds = get_data_bounds(ground_truth_frames, prediction_frames)
    
    # åˆ›å»ºå›¾å½¢
    fig = plt.figure(figsize=(16, 8))
    fig.suptitle('MARS+Transformer Skeleton Animation (PyTorch): Ground Truth vs Prediction', 
                fontsize=16, fontweight='bold')
    
    # å·¦ä¾§: Ground Truth
    ax1 = fig.add_subplot(121, projection='3d')
    ax1.set_title('Ground Truth', fontsize=14, fontweight='bold', color='blue')
    
    # å³ä¾§: PyTorch Prediction
    ax2 = fig.add_subplot(122, projection='3d')
    ax2.set_title('PyTorch Prediction', fontsize=14, fontweight='bold', color='red')
    
    # è®¾ç½®è½´å±æ€§
    for ax in [ax1, ax2]:
        ax.set_xlabel('X', fontsize=12)
        ax.set_ylabel('Y', fontsize=12)
        ax.set_zlabel('Z', fontsize=12)
        ax.set_xlim(bounds['xlim'])
        ax.set_ylim(bounds['ylim'])
        ax.set_zlim(bounds['zlim'])
        ax.view_init(elev=20, azim=45)
    
    # æ·»åŠ å¸§ä¿¡æ¯æ–‡æœ¬
    frame_text = fig.text(0.5, 0.02, '', ha='center', fontsize=12, fontweight='bold')
    
    def animate(frame_idx):
        """åŠ¨ç”»æ›´æ–°å‡½æ•°"""
        # è·å–å½“å‰å¸§æ•°æ®
        gt_joints = parse_joints(ground_truth_frames[frame_idx])
        pred_joints = parse_joints(prediction_frames[frame_idx])
        
        # è®¡ç®—è¯¯å·®
        joint_errors = np.sqrt(np.sum((gt_joints - pred_joints) ** 2, axis=1))
        mean_error = np.mean(joint_errors)
        
        # ç»˜åˆ¶Ground Truth
        plot_skeleton_frame(gt_joints, ax1, color='blue', linewidth=3, alpha=0.9)
        ax1.set_xlim(bounds['xlim'])
        ax1.set_ylim(bounds['ylim'])
        ax1.set_zlim(bounds['zlim'])
        ax1.view_init(elev=20, azim=45)
        
        # ç»˜åˆ¶PyTorch Prediction
        plot_skeleton_frame(pred_joints, ax2, color='red', linewidth=3, alpha=0.9)
        ax2.set_xlim(bounds['xlim'])
        ax2.set_ylim(bounds['ylim'])
        ax2.set_zlim(bounds['zlim'])
        ax2.view_init(elev=20, azim=45)
        
        # æ›´æ–°å¸§ä¿¡æ¯
        sample_idx = sample_indices[frame_idx]
        frame_text.set_text(f'Sample {sample_idx+1:03d} | Frame {frame_idx+1:02d}/{len(ground_truth_frames):02d} | 3D Error: {mean_error:.4f} | PyTorch')
        
        return []
    
    # åˆ›å»ºåŠ¨ç”»
    print("ğŸ”„ æ¸²æŸ“PyTorchåŠ¨ç”»å¸§...")
    anim = animation.FuncAnimation(fig, animate, frames=len(ground_truth_frames), 
                                 interval=int(duration_per_frame * 1000), blit=False, repeat=True)
    
    # ä¿å­˜GIF
    print(f"ğŸ’¾ ä¿å­˜PyTorch GIFåŠ¨ç”»: {output_path}")
    try:
        anim.save(output_path, writer='pillow', fps=fps, dpi=100)
        print(f"âœ… PyTorch GIFä¿å­˜æˆåŠŸ: {output_path}")
    except Exception as e:
        print(f"âŒ PyTorch GIFä¿å­˜å¤±è´¥: {e}")
        # å°è¯•é™ä½è´¨é‡ä¿å­˜
        try:
            print("ğŸ”„ å°è¯•é™ä½è´¨é‡é‡æ–°ä¿å­˜...")
            anim.save(output_path, writer='pillow', fps=fps, dpi=80)
            print(f"âœ… PyTorch GIFä¿å­˜æˆåŠŸ (é™ä½è´¨é‡): {output_path}")
        except Exception as e2:
            print(f"âŒ é™è´¨é‡ä¿å­˜ä¹Ÿå¤±è´¥: {e2}")
    
    plt.close()

def create_overlay_gif(ground_truth_frames, prediction_frames, sample_indices, output_path, 
                      fps=2, duration_per_frame=0.8):
    """åˆ›å»ºé‡å GIFåŠ¨ç”»"""
    
    print(f"ğŸ¬ å¼€å§‹åˆ›å»ºPyTorché‡å GIF: {len(ground_truth_frames)} å¸§")
    
    # è®¡ç®—æ•°æ®è¾¹ç•Œ
    bounds = get_data_bounds(ground_truth_frames, prediction_frames)
    
    # åˆ›å»ºå›¾å½¢
    fig = plt.figure(figsize=(12, 10))
    fig.suptitle('MARS+Transformer Skeleton Animation (PyTorch): Overlay Comparison', 
                fontsize=16, fontweight='bold')
    
    ax = fig.add_subplot(111, projection='3d')
    ax.set_xlabel('X', fontsize=12)
    ax.set_ylabel('Y', fontsize=12)
    ax.set_zlabel('Z', fontsize=12)
    ax.set_xlim(bounds['xlim'])
    ax.set_ylim(bounds['ylim'])
    ax.set_zlim(bounds['zlim'])
    ax.view_init(elev=20, azim=45)
    
    # æ·»åŠ å›¾ä¾‹å’Œå¸§ä¿¡æ¯
    legend_text = fig.text(0.02, 0.95, "â— Ground Truth (Blue)\nâ— PyTorch Prediction (Red)", 
                          fontsize=12, fontweight='bold', verticalalignment='top')
    frame_text = fig.text(0.5, 0.02, '', ha='center', fontsize=12, fontweight='bold')
    
    def animate(frame_idx):
        """åŠ¨ç”»æ›´æ–°å‡½æ•°"""
        ax.clear()
        
        # è·å–å½“å‰å¸§æ•°æ®
        gt_joints = parse_joints(ground_truth_frames[frame_idx])
        pred_joints = parse_joints(prediction_frames[frame_idx])
        
        # è®¡ç®—è¯¯å·®
        joint_errors = np.sqrt(np.sum((gt_joints - pred_joints) ** 2, axis=1))
        mean_error = np.mean(joint_errors)
        
        # ç»˜åˆ¶Ground Truth (è“è‰²)
        ax.scatter(gt_joints[:, 0], gt_joints[:, 1], gt_joints[:, 2], 
                  c='blue', s=100, alpha=0.8, edgecolors='black', linewidths=0.5, label='Ground Truth')
        
        for connection in skeleton_connections:
            if connection[0] < len(gt_joints) and connection[1] < len(gt_joints):
                joint1 = gt_joints[connection[0]]
                joint2 = gt_joints[connection[1]]
                ax.plot([joint1[0], joint2[0]], [joint1[1], joint2[1]], [joint1[2], joint2[2]], 
                       color='blue', alpha=0.8, linewidth=3)
        
        # ç»˜åˆ¶PyTorch Prediction (çº¢è‰²ï¼Œé€æ˜)
        ax.scatter(pred_joints[:, 0], pred_joints[:, 1], pred_joints[:, 2], 
                  c='red', s=80, alpha=0.6, edgecolors='darkred', linewidths=0.5, label='PyTorch Prediction')
        
        for connection in skeleton_connections:
            if connection[0] < len(pred_joints) and connection[1] < len(pred_joints):
                joint1 = pred_joints[connection[0]]
                joint2 = pred_joints[connection[1]]
                ax.plot([joint1[0], joint2[0]], [joint1[1], joint2[1]], [joint1[2], joint2[2]], 
                       color='red', alpha=0.6, linewidth=2)
        
        # è®¾ç½®è½´å±æ€§
        ax.set_xlabel('X', fontsize=12)
        ax.set_ylabel('Y', fontsize=12)
        ax.set_zlabel('Z', fontsize=12)
        ax.set_xlim(bounds['xlim'])
        ax.set_ylim(bounds['ylim'])
        ax.set_zlim(bounds['zlim'])
        ax.view_init(elev=20, azim=45)
        
        # æ›´æ–°å¸§ä¿¡æ¯
        sample_idx = sample_indices[frame_idx]
        frame_text.set_text(f'Sample {sample_idx+1:03d} | Frame {frame_idx+1:02d}/{len(ground_truth_frames):02d} | 3D Error: {mean_error:.4f} | PyTorch')
        
        return []
    
    # åˆ›å»ºåŠ¨ç”»
    print("ğŸ”„ æ¸²æŸ“PyTorché‡å åŠ¨ç”»å¸§...")
    anim = animation.FuncAnimation(fig, animate, frames=len(ground_truth_frames), 
                                 interval=int(duration_per_frame * 1000), blit=False, repeat=True)
    
    # ä¿å­˜GIF
    print(f"ğŸ’¾ ä¿å­˜PyTorché‡å GIFåŠ¨ç”»: {output_path}")
    try:
        anim.save(output_path, writer='pillow', fps=fps, dpi=100)
        print(f"âœ… PyTorché‡å GIFä¿å­˜æˆåŠŸ: {output_path}")
    except Exception as e:
        print(f"âŒ PyTorché‡å GIFä¿å­˜å¤±è´¥: {e}")
        # å°è¯•é™ä½è´¨é‡ä¿å­˜
        try:
            print("ğŸ”„ å°è¯•é™ä½è´¨é‡é‡æ–°ä¿å­˜...")
            anim.save(output_path, writer='pillow', fps=fps, dpi=80)
            print(f"âœ… PyTorché‡å GIFä¿å­˜æˆåŠŸ (é™ä½è´¨é‡): {output_path}")
        except Exception as e2:
            print(f"âŒ é™è´¨é‡ä¿å­˜ä¹Ÿå¤±è´¥: {e2}")
    
    plt.close()

def main(use_live_prediction=True, frames_per_gif=8, num_gifs=5, fps=2):
    """ä¸»å‡½æ•°"""
    print("MARS+Transformeréª¨æ¶GIFåŠ¨ç”»å¯è§†åŒ–å·¥å…· (PyTorchç‰ˆæœ¬)")
    print("=" * 70)
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    output_dir = 'visualizations/skeleton_extractor_gif_new'
    if not os.path.exists(output_dir):
        os.makedirs(output_dir)
        print(f"âœ“ å·²åˆ›å»ºè¾“å‡ºç›®å½•: {output_dir}")
    
    # åŠ è½½Ground Truth
    ground_truth = np.load('/home/uo/myProject/HumanPoint-BERT/data/MARS/labels_test.npy')
    print(f"âœ“ Ground Truthæ•°æ®: {ground_truth.shape}")
    
    predictions = None
    
    # å°è¯•ä½¿ç”¨å®æ—¶PyTorchæ¨¡å‹é¢„æµ‹
    if use_live_prediction and HAS_TORCH:
        print("\nğŸš€ å°è¯•ä½¿ç”¨mars_transformer_best.pthè¿›è¡Œå®æ—¶é¢„æµ‹...")
        predictions = predict_with_torch_model('mars_transformer_best.pth')
        
        if predictions is not None:
            print("âœ… ä½¿ç”¨PyTorchå®æ—¶æ¨¡å‹é¢„æµ‹æˆåŠŸ!")
        else:
            print("âš ï¸ PyTorchå®æ—¶é¢„æµ‹å¤±è´¥ï¼Œå°è¯•åŠ è½½é¢„ä¿å­˜çš„ç»“æœ...")
    
    # å¦‚æœå®æ—¶é¢„æµ‹å¤±è´¥ï¼Œä½¿ç”¨é¢„ä¿å­˜çš„ç»“æœ
    if predictions is None:
        print("\nğŸ“ åŠ è½½é¢„ä¿å­˜çš„é¢„æµ‹ç»“æœ...")
        ground_truth, predictions = load_data()
        if ground_truth is None or predictions is None:
            print("âŒ æ•°æ®åŠ è½½å¤±è´¥ï¼Œé€€å‡ºç¨‹åº")
            return
    
    print(f"\nğŸ¬ å¼€å§‹ç”Ÿæˆ {num_gifs} ä¸ªPyTorch GIFåŠ¨ç”»ï¼Œæ¯ä¸ªåŒ…å« {frames_per_gif} å¸§")
    print(f"åŠ¨ç”»å‚æ•°: FPS={fps}, æ¯å¸§æ—¶é•¿={1000/fps:.0f}ms")
    print("-" * 60)
    
    total_samples = len(ground_truth)
    step = max(1, total_samples // (num_gifs * frames_per_gif))
    
    for gif_idx in range(num_gifs):
        print(f"\nğŸ¥ ç”Ÿæˆç¬¬ {gif_idx+1}/{num_gifs} ä¸ªPyTorch GIFåŠ¨ç”»...")
        
        # è®¡ç®—èµ·å§‹ç´¢å¼•
        start_idx = gif_idx * frames_per_gif * step
        
        # é€‰æ‹©è¿ç»­çš„å¸§
        frame_indices = []
        gt_frames = []
        pred_frames = []
        
        for frame_idx in range(frames_per_gif):
            sample_idx = start_idx + frame_idx * step
            if sample_idx < total_samples:
                frame_indices.append(sample_idx)
                gt_frames.append(ground_truth[sample_idx])
                pred_frames.append(predictions[sample_idx])
        
        if len(frame_indices) < frames_per_gif:
            print(f"âš ï¸ æ ·æœ¬ä¸è¶³ï¼Œåªèƒ½ç”Ÿæˆ {len(frame_indices)} å¸§")
        
        print(f"   æ ·æœ¬ç´¢å¼•: {[idx+1 for idx in frame_indices]}")
        
        # ç”ŸæˆPyTorchå¯¹æ¯”GIF
        comparison_path = os.path.join(output_dir, f'skeleton_torch_comparison_{gif_idx+1:02d}.gif')
        create_comparison_gif(gt_frames, pred_frames, frame_indices, comparison_path, fps=fps)
        
        # ç”ŸæˆPyTorché‡å GIF
        overlay_path = os.path.join(output_dir, f'skeleton_torch_overlay_{gif_idx+1:02d}.gif')
        create_overlay_gif(gt_frames, pred_frames, frame_indices, overlay_path, fps=fps)
    
    # ç”Ÿæˆæ±‡æ€»ä¿¡æ¯
    print("-" * 60)
    print(f"âœ… PyTorch GIFåŠ¨ç”»ç”Ÿæˆå®Œæˆ!")
    print(f"è¾“å‡ºç›®å½•: {output_dir}/")
    print(f"ç”Ÿæˆæ–‡ä»¶:")
    
    gif_files = [f for f in os.listdir(output_dir) if f.endswith('.gif')]
    for i, gif_file in enumerate(sorted(gif_files), 1):
        file_path = os.path.join(output_dir, gif_file)
        file_size = os.path.getsize(file_path) / (1024 * 1024)  # MB
        print(f"  {i:2d}. {gif_file} ({file_size:.2f} MB)")
    
    # åˆ›å»ºREADMEæ–‡ä»¶
    readme_path = os.path.join(output_dir, 'README_torch.txt')
    with open(readme_path, 'w', encoding='utf-8') as f:
        f.write("MARS+Transformeréª¨æ¶GIFåŠ¨ç”»è¯´æ˜ (PyTorchç‰ˆæœ¬)\n")
        f.write("=" * 50 + "\n\n")
        f.write("æ–‡ä»¶ç±»å‹:\n")
        f.write("- skeleton_torch_comparison_XX.gif: å·¦å³å¯¹æ¯”åŠ¨ç”» (Ground Truth vs PyTorch Prediction)\n")
        f.write("- skeleton_torch_overlay_XX.gif: é‡å å¯¹æ¯”åŠ¨ç”» (è“è‰²GT + çº¢è‰²PyTorch Prediction)\n\n")
        f.write(f"åŠ¨ç”»å‚æ•°:\n")
        f.write(f"- å¸§æ•°: {frames_per_gif} å¸§/GIF\n")
        f.write(f"- å¸§ç‡: {fps} FPS\n")
        f.write(f"- æ€»GIFæ•°é‡: {num_gifs}\n")
        f.write(f"- ç”Ÿæˆæ—¶é—´: {np.datetime64('now')}\n\n")
        f.write("æ¨¡å‹ä¿¡æ¯:\n")
        f.write("- æ¶æ„: MARS+Transformer (PyTorchç‰ˆæœ¬)\n")
        f.write("- æƒé‡æ–‡ä»¶: mars_transformer_best.pth\n")
        f.write("- æ¡†æ¶: PyTorch\n")
        f.write("- è¾“å…¥æ ¼å¼: (N, C, H, W) - PyTorchæ ‡å‡†æ ¼å¼\n")
        f.write("- è¾“å‡ºç»´åº¦: 57 (19ä¸ªå…³èŠ‚ç‚¹çš„3Dåæ ‡)\n")
    
    print(f"âœ“ PyTorchè¯´æ˜æ–‡ä»¶å·²ä¿å­˜: {readme_path}")
    print(f"\nğŸš€ MARS+Transformer PyTorch GIFå¯è§†åŒ–å®Œæˆ!")

if __name__ == "__main__":
    import sys
    
    # é»˜è®¤å‚æ•°
    use_live = True
    frames_per_gif = 8
    num_gifs = 8
    fps = 2
    
    # è§£æå‘½ä»¤è¡Œå‚æ•°
    if len(sys.argv) > 1:
        for arg in sys.argv[1:]:
            if arg.lower() in ['--no-live', '-n']:
                use_live = False
                print("ğŸ“ å¼ºåˆ¶ä½¿ç”¨é¢„ä¿å­˜çš„é¢„æµ‹ç»“æœ")
            elif arg.startswith('--frames='):
                frames_per_gif = int(arg.split('=')[1])
                print(f"ğŸ¬ è®¾ç½®æ¯ä¸ªGIFå¸§æ•°: {frames_per_gif}")
            elif arg.startswith('--gifs='):
                num_gifs = int(arg.split('=')[1])
                print(f"ğŸ¥ è®¾ç½®GIFæ•°é‡: {num_gifs}")
            elif arg.startswith('--fps='):
                fps = int(arg.split('=')[1])
                print(f"â±ï¸ è®¾ç½®å¸§ç‡: {fps} FPS")
            elif arg.lower() in ['--help', '-h']:
                print("MARS+Transformer PyTorch GIFåŠ¨ç”»å¯è§†åŒ–å·¥å…·")
                print("ç”¨æ³•:")
                print("  python skeleton_gif_visualization_torch.py                    # ä½¿ç”¨é»˜è®¤å‚æ•°")
                print("  python skeleton_gif_visualization_torch.py --no-live         # åªä½¿ç”¨é¢„ä¿å­˜ç»“æœ")
                print("  python skeleton_gif_visualization_torch.py --frames=10       # è®¾ç½®æ¯ä¸ªGIFå¸§æ•°")
                print("  python skeleton_gif_visualization_torch.py --gifs=3          # è®¾ç½®GIFæ•°é‡")
                print("  python skeleton_gif_visualization_torch.py --fps=3           # è®¾ç½®å¸§ç‡")
                print("  python skeleton_gif_visualization_torch.py --help            # æ˜¾ç¤ºå¸®åŠ©")
                print("\né»˜è®¤å‚æ•°:")
                print(f"  frames_per_gif={frames_per_gif}, num_gifs={num_gifs}, fps={fps}")
                exit(0)
    
    print(f"ğŸ¬ PyTorchåŠ¨ç”»å‚æ•°: {frames_per_gif}å¸§/GIF, {num_gifs}ä¸ªGIF, {fps}FPS")
    main(use_live_prediction=use_live, frames_per_gif=frames_per_gif, num_gifs=num_gifs, fps=fps)